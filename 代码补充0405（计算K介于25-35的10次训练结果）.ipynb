{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e12be2d4-7e6c-437b-af3f-c9bf516aa2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 13:26:26.659480: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-05 13:26:29,536 - INFO - 创建输出目录: model_results_20250405_132629\n",
      "2025-04-05 13:26:31,521 - INFO - Successfully loaded data from /Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/DL_Data8.csv\n",
      "2025-04-05 13:26:34,236 - INFO - Expanded features from 33 to 561\n",
      "2025-04-05 13:26:45,333 - INFO - Filled NaN values with mean\n",
      "2025-04-05 13:26:58.372699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-05 13:27:40,466 - INFO - Epoch [1/200], Train Loss: 0.7395, Val Loss: 0.0058\n",
      "2025-04-05 13:28:25,465 - INFO - Epoch [2/200], Train Loss: 0.0109, Val Loss: 0.0069\n",
      "2025-04-05 13:29:10,598 - INFO - Epoch [3/200], Train Loss: 0.0034, Val Loss: 0.0052\n",
      "2025-04-05 13:29:55,854 - INFO - Epoch [4/200], Train Loss: 0.0034, Val Loss: 0.0051\n",
      "2025-04-05 13:30:41,075 - INFO - Epoch [5/200], Train Loss: 0.0035, Val Loss: 0.0046\n",
      "2025-04-05 13:31:26,270 - INFO - Epoch [6/200], Train Loss: 0.0035, Val Loss: 0.0050\n",
      "2025-04-05 13:32:11,449 - INFO - Epoch [7/200], Train Loss: 0.0035, Val Loss: 0.0061\n",
      "2025-04-05 13:32:56,598 - INFO - Epoch [8/200], Train Loss: 0.0036, Val Loss: 0.0054\n",
      "2025-04-05 13:33:41,846 - INFO - Epoch [9/200], Train Loss: 0.0035, Val Loss: 0.0030\n",
      "2025-04-05 13:34:26,984 - INFO - Epoch [10/200], Train Loss: 0.0035, Val Loss: 0.0062\n",
      "2025-04-05 13:35:12,204 - INFO - Epoch [11/200], Train Loss: 0.0035, Val Loss: 0.0042\n",
      "2025-04-05 13:35:57,416 - INFO - Epoch [12/200], Train Loss: 0.0035, Val Loss: 0.0046\n",
      "2025-04-05 13:36:42,732 - INFO - Epoch [13/200], Train Loss: 0.0035, Val Loss: 0.0047\n",
      "2025-04-05 13:37:27,745 - INFO - Epoch [14/200], Train Loss: 0.0035, Val Loss: 0.0172\n",
      "2025-04-05 13:38:12,772 - INFO - Epoch [15/200], Train Loss: 0.0035, Val Loss: 0.0053\n",
      "2025-04-05 13:38:57,771 - INFO - Epoch [16/200], Train Loss: 0.0035, Val Loss: 0.0057\n",
      "2025-04-05 13:39:42,838 - INFO - Epoch [17/200], Train Loss: 0.0035, Val Loss: 0.0042\n",
      "2025-04-05 13:40:27,836 - INFO - Epoch [18/200], Train Loss: 0.0035, Val Loss: 0.0041\n",
      "2025-04-05 13:41:12,820 - INFO - Epoch [19/200], Train Loss: 0.0035, Val Loss: 0.0057\n",
      "2025-04-05 13:41:57,839 - INFO - Epoch [20/200], Train Loss: 0.0036, Val Loss: 0.0051\n",
      "2025-04-05 13:42:43,007 - INFO - Epoch [21/200], Train Loss: 0.0035, Val Loss: 0.0061\n",
      "2025-04-05 13:43:28,007 - INFO - Epoch [22/200], Train Loss: 0.0034, Val Loss: 0.0138\n",
      "2025-04-05 13:44:13,005 - INFO - Epoch [23/200], Train Loss: 0.0035, Val Loss: 0.0043\n",
      "2025-04-05 13:44:58,166 - INFO - Epoch [24/200], Train Loss: 0.0034, Val Loss: 0.0051\n",
      "2025-04-05 13:45:43,180 - INFO - Epoch [25/200], Train Loss: 0.0034, Val Loss: 0.0048\n",
      "2025-04-05 13:46:28,125 - INFO - Epoch [26/200], Train Loss: 0.0034, Val Loss: 0.0047\n",
      "2025-04-05 13:47:13,117 - INFO - Epoch [27/200], Train Loss: 0.0034, Val Loss: 0.0057\n",
      "2025-04-05 13:47:58,273 - INFO - Epoch [28/200], Train Loss: 0.0034, Val Loss: 0.0047\n",
      "2025-04-05 13:48:43,224 - INFO - Epoch [29/200], Train Loss: 0.0034, Val Loss: 0.0037\n",
      "2025-04-05 13:48:43,225 - INFO - Early stopping triggered at epoch 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.847\n",
      "Model:                            OLS   Adj. R-squared:                  0.847\n",
      "Method:                 Least Squares   F-statistic:                 3.340e+04\n",
      "Date:                Sat, 05 Apr 2025   Prob (F-statistic):               0.00\n",
      "Time:                        13:48:44   Log-Likelihood:             2.8285e+05\n",
      "No. Observations:              180354   AIC:                        -5.656e+05\n",
      "Df Residuals:                  180323   BIC:                        -5.653e+05\n",
      "Df Model:                          30                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4544      0.000   3626.419      0.000       0.454       0.455\n",
      "x1          2465.7724    944.811      2.610      0.009     613.965    4317.579\n",
      "x2          3722.5555    905.923      4.109      0.000    1946.967    5498.144\n",
      "x3          3987.1728    943.796      4.225      0.000    2137.353    5836.992\n",
      "x4         -1120.3312    869.671     -1.288      0.198   -2824.867     584.204\n",
      "x5          3350.9720    873.443      3.837      0.000    1639.044    5062.900\n",
      "x6         -8414.5831    936.556     -8.985      0.000   -1.03e+04   -6578.954\n",
      "x7         -6004.0021    924.703     -6.493      0.000   -7816.398   -4191.606\n",
      "x8         -5002.0154    882.722     -5.667      0.000   -6732.130   -3271.901\n",
      "x9           461.4932    902.931      0.511      0.609   -1308.230    2231.217\n",
      "x10         4580.7417    877.215      5.222      0.000    2861.421    6300.062\n",
      "x11         4670.5085    864.255      5.404      0.000    2976.589    6364.428\n",
      "x12         -396.3271    888.829     -0.446      0.656   -2138.412    1345.758\n",
      "x13        -2060.7077    915.239     -2.252      0.024   -3854.555    -266.861\n",
      "x14         2396.7872    928.288      2.582      0.010     577.365    4216.210\n",
      "x15         8981.9849    884.509     10.155      0.000    7248.368    1.07e+04\n",
      "x16         1911.0424    897.958      2.128      0.033     151.065    3671.020\n",
      "x17        -4851.6810    895.185     -5.420      0.000   -6606.223   -3097.139\n",
      "x18         4678.5878    895.839      5.223      0.000    2922.764    6434.411\n",
      "x19         8541.5618    886.861      9.631      0.000    6803.335    1.03e+04\n",
      "x20         3086.8772    927.007      3.330      0.001    1269.964    4903.791\n",
      "x21         2339.7952    891.452      2.625      0.009     592.569    4087.021\n",
      "x22         5219.1645    891.709      5.853      0.000    3471.435    6966.894\n",
      "x23        -7548.0762    890.133     -8.480      0.000   -9292.717   -5803.435\n",
      "x24        -3359.0228    858.435     -3.913      0.000   -5041.537   -1676.509\n",
      "x25         1167.8273    936.626      1.247      0.212    -667.938    3003.592\n",
      "x26          684.1829    858.223      0.797      0.425    -997.915    2366.280\n",
      "x27         2128.0250    887.164      2.399      0.016     389.204    3866.846\n",
      "x28        -4912.1169    929.330     -5.286      0.000   -6733.582   -3090.652\n",
      "x29        -4343.1685    872.401     -4.978      0.000   -6053.055   -2633.282\n",
      "x30        -6370.5182    897.144     -7.101      0.000   -8128.900   -4612.136\n",
      "==============================================================================\n",
      "Omnibus:                   185864.320   Durbin-Watson:                   2.003\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         89527145.279\n",
      "Skew:                           4.418   Prob(JB):                         0.00\n",
      "Kurtosis:                     111.791   Cond. No.                     4.46e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.46e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "共同因子数据已保存到 'common_factors_and_returns.csv'\n",
      "调整后的R方: 0.8475\n",
      "损失图表已保存为 'loss_plot.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 13:49:06,233 - INFO - Epoch [1/200], Train Loss: 3.6834, Val Loss: 0.0271\n",
      "2025-04-05 13:49:23,971 - INFO - Epoch [2/200], Train Loss: 0.0334, Val Loss: 0.2870\n",
      "2025-04-05 13:49:41,830 - INFO - Epoch [3/200], Train Loss: 0.0780, Val Loss: 0.8829\n",
      "2025-04-05 13:49:59,536 - INFO - Epoch [4/200], Train Loss: 0.0722, Val Loss: 0.1466\n",
      "2025-04-05 13:50:17,403 - INFO - Epoch [5/200], Train Loss: 0.0537, Val Loss: 0.1652\n",
      "2025-04-05 13:50:35,154 - INFO - Epoch [6/200], Train Loss: 0.0311, Val Loss: 0.0633\n",
      "2025-04-05 13:50:52,997 - INFO - Epoch [7/200], Train Loss: 0.0218, Val Loss: 0.0234\n",
      "2025-04-05 13:51:10,670 - INFO - Epoch [8/200], Train Loss: 0.0194, Val Loss: 0.0271\n",
      "2025-04-05 13:51:28,467 - INFO - Epoch [9/200], Train Loss: 0.0184, Val Loss: 0.0105\n",
      "2025-04-05 13:51:46,166 - INFO - Epoch [10/200], Train Loss: 0.0185, Val Loss: 0.0124\n",
      "2025-04-05 13:52:03,987 - INFO - Epoch [11/200], Train Loss: 0.0186, Val Loss: 0.0160\n",
      "2025-04-05 13:52:21,692 - INFO - Epoch [12/200], Train Loss: 0.0184, Val Loss: 0.0125\n",
      "2025-04-05 13:52:39,552 - INFO - Epoch [13/200], Train Loss: 0.0185, Val Loss: 0.0130\n",
      "2025-04-05 13:52:57,267 - INFO - Epoch [14/200], Train Loss: 0.0185, Val Loss: 0.0132\n",
      "2025-04-05 13:53:15,125 - INFO - Epoch [15/200], Train Loss: 0.0185, Val Loss: 0.0124\n",
      "2025-04-05 13:53:32,832 - INFO - Epoch [16/200], Train Loss: 0.0185, Val Loss: 0.0130\n",
      "2025-04-05 13:53:50,570 - INFO - Epoch [17/200], Train Loss: 0.0185, Val Loss: 0.0124\n",
      "2025-04-05 13:54:08,348 - INFO - Epoch [18/200], Train Loss: 0.0185, Val Loss: 0.0132\n",
      "2025-04-05 13:54:26,040 - INFO - Epoch [19/200], Train Loss: 0.0185, Val Loss: 0.0131\n",
      "2025-04-05 13:54:43,876 - INFO - Epoch [20/200], Train Loss: 0.0185, Val Loss: 0.0125\n",
      "2025-04-05 13:55:01,600 - INFO - Epoch [21/200], Train Loss: 0.0185, Val Loss: 0.0114\n",
      "2025-04-05 13:55:19,477 - INFO - Epoch [22/200], Train Loss: 0.0184, Val Loss: 0.0113\n",
      "2025-04-05 13:55:37,177 - INFO - Epoch [23/200], Train Loss: 0.0183, Val Loss: 0.0114\n",
      "2025-04-05 13:55:55,079 - INFO - Epoch [24/200], Train Loss: 0.0184, Val Loss: 0.0115\n",
      "2025-04-05 13:56:12,759 - INFO - Epoch [25/200], Train Loss: 0.0184, Val Loss: 0.0119\n",
      "2025-04-05 13:56:30,763 - INFO - Epoch [26/200], Train Loss: 0.0183, Val Loss: 0.0130\n",
      "2025-04-05 13:56:48,468 - INFO - Epoch [27/200], Train Loss: 0.0183, Val Loss: 0.0152\n",
      "2025-04-05 13:57:06,292 - INFO - Epoch [28/200], Train Loss: 0.0183, Val Loss: 0.0106\n",
      "2025-04-05 13:57:23,275 - INFO - Epoch [29/200], Train Loss: 0.0183, Val Loss: 0.0109\n",
      "2025-04-05 13:57:23,275 - INFO - Early stopping triggered at epoch 29\n",
      "2025-04-05 13:58:01,312 - INFO - Epoch [1/200], Train Loss: 2.1305, Val Loss: 0.0619\n",
      "2025-04-05 13:58:36,716 - INFO - Epoch [2/200], Train Loss: 0.0853, Val Loss: 0.0507\n",
      "2025-04-05 13:59:12,032 - INFO - Epoch [3/200], Train Loss: 0.0440, Val Loss: 0.8437\n",
      "2025-04-05 13:59:47,512 - INFO - Epoch [4/200], Train Loss: 0.0186, Val Loss: 0.0138\n",
      "2025-04-05 14:00:23,001 - INFO - Epoch [5/200], Train Loss: 0.0161, Val Loss: 0.0110\n",
      "2025-04-05 14:00:58,257 - INFO - Epoch [6/200], Train Loss: 0.0161, Val Loss: 0.0112\n",
      "2025-04-05 14:01:33,528 - INFO - Epoch [7/200], Train Loss: 0.0162, Val Loss: 0.0116\n",
      "2025-04-05 14:02:08,756 - INFO - Epoch [8/200], Train Loss: 0.0162, Val Loss: 0.0112\n",
      "2025-04-05 14:02:44,016 - INFO - Epoch [9/200], Train Loss: 0.0162, Val Loss: 0.0119\n",
      "2025-04-05 14:03:19,233 - INFO - Epoch [10/200], Train Loss: 0.0161, Val Loss: 0.0116\n",
      "2025-04-05 14:03:54,511 - INFO - Epoch [11/200], Train Loss: 0.0161, Val Loss: 0.0109\n",
      "2025-04-05 14:04:29,804 - INFO - Epoch [12/200], Train Loss: 0.0160, Val Loss: 0.0114\n",
      "2025-04-05 14:05:05,106 - INFO - Epoch [13/200], Train Loss: 0.0160, Val Loss: 0.0110\n",
      "2025-04-05 14:05:40,382 - INFO - Epoch [14/200], Train Loss: 0.0160, Val Loss: 0.0118\n",
      "2025-04-05 14:06:15,637 - INFO - Epoch [15/200], Train Loss: 0.0160, Val Loss: 0.0107\n",
      "2025-04-05 14:06:51,061 - INFO - Epoch [16/200], Train Loss: 0.0160, Val Loss: 0.0112\n",
      "2025-04-05 14:07:26,377 - INFO - Epoch [17/200], Train Loss: 0.0160, Val Loss: 0.0100\n",
      "2025-04-05 14:08:01,737 - INFO - Epoch [18/200], Train Loss: 0.0159, Val Loss: 0.0102\n",
      "2025-04-05 14:08:36,989 - INFO - Epoch [19/200], Train Loss: 0.0159, Val Loss: 0.0112\n",
      "2025-04-05 14:09:12,254 - INFO - Epoch [20/200], Train Loss: 0.0159, Val Loss: 0.0106\n",
      "2025-04-05 14:09:47,482 - INFO - Epoch [21/200], Train Loss: 0.0158, Val Loss: 0.0115\n",
      "2025-04-05 14:10:22,697 - INFO - Epoch [22/200], Train Loss: 0.0157, Val Loss: 0.0113\n",
      "2025-04-05 14:10:57,901 - INFO - Epoch [23/200], Train Loss: 0.0157, Val Loss: 0.0102\n",
      "2025-04-05 14:11:33,080 - INFO - Epoch [24/200], Train Loss: 0.0157, Val Loss: 0.0105\n",
      "2025-04-05 14:12:08,278 - INFO - Epoch [25/200], Train Loss: 0.0156, Val Loss: 0.0095\n",
      "2025-04-05 14:12:43,528 - INFO - Epoch [26/200], Train Loss: 0.0156, Val Loss: 0.0099\n",
      "2025-04-05 14:13:18,724 - INFO - Epoch [27/200], Train Loss: 0.0156, Val Loss: 0.0091\n",
      "2025-04-05 14:13:53,947 - INFO - Epoch [28/200], Train Loss: 0.0156, Val Loss: 0.0093\n",
      "2025-04-05 14:14:29,172 - INFO - Epoch [29/200], Train Loss: 0.0156, Val Loss: 0.0099\n",
      "2025-04-05 14:15:04,363 - INFO - Epoch [30/200], Train Loss: 0.0156, Val Loss: 0.0103\n",
      "2025-04-05 14:15:39,615 - INFO - Epoch [31/200], Train Loss: 0.0156, Val Loss: 0.0106\n",
      "2025-04-05 14:16:14,855 - INFO - Epoch [32/200], Train Loss: 0.0155, Val Loss: 0.0106\n",
      "2025-04-05 14:16:50,162 - INFO - Epoch [33/200], Train Loss: 0.0155, Val Loss: 0.0107\n",
      "2025-04-05 14:17:25,425 - INFO - Epoch [34/200], Train Loss: 0.0154, Val Loss: 0.0101\n",
      "2025-04-05 14:18:00,759 - INFO - Epoch [35/200], Train Loss: 0.0155, Val Loss: 0.0094\n",
      "2025-04-05 14:18:36,037 - INFO - Epoch [36/200], Train Loss: 0.0154, Val Loss: 0.0103\n",
      "2025-04-05 14:19:11,297 - INFO - Epoch [37/200], Train Loss: 0.0154, Val Loss: 0.0096\n",
      "2025-04-05 14:19:46,575 - INFO - Epoch [38/200], Train Loss: 0.0155, Val Loss: 0.0100\n",
      "2025-04-05 14:20:21,783 - INFO - Epoch [39/200], Train Loss: 0.0154, Val Loss: 0.0101\n",
      "2025-04-05 14:20:57,024 - INFO - Epoch [40/200], Train Loss: 0.0155, Val Loss: 0.0108\n",
      "2025-04-05 14:21:32,345 - INFO - Epoch [41/200], Train Loss: 0.0154, Val Loss: 0.0094\n",
      "2025-04-05 14:22:07,581 - INFO - Epoch [42/200], Train Loss: 0.0154, Val Loss: 0.0092\n",
      "2025-04-05 14:22:42,879 - INFO - Epoch [43/200], Train Loss: 0.0153, Val Loss: 0.0091\n",
      "2025-04-05 14:23:18,130 - INFO - Epoch [44/200], Train Loss: 0.0154, Val Loss: 0.0103\n",
      "2025-04-05 14:23:53,423 - INFO - Epoch [45/200], Train Loss: 0.0153, Val Loss: 0.0091\n",
      "2025-04-05 14:24:28,659 - INFO - Epoch [46/200], Train Loss: 0.0153, Val Loss: 0.0098\n",
      "2025-04-05 14:25:03,969 - INFO - Epoch [47/200], Train Loss: 0.0153, Val Loss: 0.0090\n",
      "2025-04-05 14:25:39,249 - INFO - Epoch [48/200], Train Loss: 0.0153, Val Loss: 0.0102\n",
      "2025-04-05 14:26:14,505 - INFO - Epoch [49/200], Train Loss: 0.0153, Val Loss: 0.0093\n",
      "2025-04-05 14:26:48,573 - INFO - Epoch [50/200], Train Loss: 0.0154, Val Loss: 0.0086\n",
      "2025-04-05 14:27:24,067 - INFO - Epoch [51/200], Train Loss: 0.0153, Val Loss: 0.0092\n",
      "2025-04-05 14:27:59,433 - INFO - Epoch [52/200], Train Loss: 0.0153, Val Loss: 0.0098\n",
      "2025-04-05 14:28:34,810 - INFO - Epoch [53/200], Train Loss: 0.0153, Val Loss: 0.0088\n",
      "2025-04-05 14:29:09,993 - INFO - Epoch [54/200], Train Loss: 0.0153, Val Loss: 0.0096\n",
      "2025-04-05 14:30:26,013 - INFO - Epoch [55/200], Train Loss: 0.0153, Val Loss: 0.0097\n",
      "2025-04-05 14:31:01,712 - INFO - Epoch [56/200], Train Loss: 0.0153, Val Loss: 0.0099\n",
      "2025-04-05 14:31:37,105 - INFO - Epoch [57/200], Train Loss: 0.0153, Val Loss: 0.0100\n",
      "2025-04-05 14:32:13,452 - INFO - Epoch [58/200], Train Loss: 0.0153, Val Loss: 0.0099\n",
      "2025-04-05 14:32:52,051 - INFO - Epoch [59/200], Train Loss: 0.0152, Val Loss: 0.0089\n",
      "2025-04-05 14:33:29,404 - INFO - Epoch [60/200], Train Loss: 0.0152, Val Loss: 0.0103\n",
      "2025-04-05 14:34:05,952 - INFO - Epoch [61/200], Train Loss: 0.0152, Val Loss: 0.0095\n",
      "2025-04-05 14:34:41,819 - INFO - Epoch [62/200], Train Loss: 0.0152, Val Loss: 0.0088\n",
      "2025-04-05 14:35:16,960 - INFO - Epoch [63/200], Train Loss: 0.0152, Val Loss: 0.0076\n",
      "2025-04-05 14:35:52,202 - INFO - Epoch [64/200], Train Loss: 0.0152, Val Loss: 0.0099\n",
      "2025-04-05 14:36:27,533 - INFO - Epoch [65/200], Train Loss: 0.0152, Val Loss: 0.0080\n",
      "2025-04-05 14:37:02,662 - INFO - Epoch [66/200], Train Loss: 0.0152, Val Loss: 0.0112\n",
      "2025-04-05 14:37:37,984 - INFO - Epoch [67/200], Train Loss: 0.0152, Val Loss: 0.0095\n",
      "2025-04-05 14:38:13,189 - INFO - Epoch [68/200], Train Loss: 0.0152, Val Loss: 0.0078\n",
      "2025-04-05 14:38:48,306 - INFO - Epoch [69/200], Train Loss: 0.0152, Val Loss: 0.0105\n",
      "2025-04-05 14:39:23,573 - INFO - Epoch [70/200], Train Loss: 0.0152, Val Loss: 0.0090\n",
      "2025-04-05 14:39:59,780 - INFO - Epoch [71/200], Train Loss: 0.0152, Val Loss: 0.0088\n",
      "2025-04-05 14:40:35,335 - INFO - Epoch [72/200], Train Loss: 0.0151, Val Loss: 0.0108\n",
      "2025-04-05 14:41:10,648 - INFO - Epoch [73/200], Train Loss: 0.0151, Val Loss: 0.0087\n",
      "2025-04-05 14:41:46,121 - INFO - Epoch [74/200], Train Loss: 0.0151, Val Loss: 0.0080\n",
      "2025-04-05 14:42:21,880 - INFO - Epoch [75/200], Train Loss: 0.0151, Val Loss: 0.0096\n",
      "2025-04-05 14:42:57,112 - INFO - Epoch [76/200], Train Loss: 0.0151, Val Loss: 0.0084\n",
      "2025-04-05 14:43:32,502 - INFO - Epoch [77/200], Train Loss: 0.0151, Val Loss: 0.0089\n",
      "2025-04-05 14:44:07,714 - INFO - Epoch [78/200], Train Loss: 0.0151, Val Loss: 0.0083\n",
      "2025-04-05 14:44:42,995 - INFO - Epoch [79/200], Train Loss: 0.0151, Val Loss: 0.0101\n",
      "2025-04-05 14:45:18,296 - INFO - Epoch [80/200], Train Loss: 0.0151, Val Loss: 0.0083\n",
      "2025-04-05 14:45:54,154 - INFO - Epoch [81/200], Train Loss: 0.0151, Val Loss: 0.0104\n",
      "2025-04-05 14:46:29,594 - INFO - Epoch [82/200], Train Loss: 0.0151, Val Loss: 0.0077\n",
      "2025-04-05 14:47:04,813 - INFO - Epoch [83/200], Train Loss: 0.0151, Val Loss: 0.0088\n",
      "2025-04-05 14:47:04,814 - INFO - Early stopping triggered at epoch 83\n",
      "2025-04-05 14:48:01,977 - INFO - Epoch [1/200], Train Loss: 1.3815, Val Loss: 0.2666\n",
      "2025-04-05 14:48:54,721 - INFO - Epoch [2/200], Train Loss: 0.0520, Val Loss: 0.0616\n",
      "2025-04-05 14:49:46,667 - INFO - Epoch [3/200], Train Loss: 0.0162, Val Loss: 0.0178\n",
      "2025-04-05 14:50:35,012 - INFO - Epoch [4/200], Train Loss: 0.0151, Val Loss: 0.0183\n",
      "2025-04-05 14:51:27,544 - INFO - Epoch [5/200], Train Loss: 0.0152, Val Loss: 0.0178\n",
      "2025-04-05 14:52:18,001 - INFO - Epoch [6/200], Train Loss: 0.0152, Val Loss: 0.0174\n",
      "2025-04-05 14:53:07,443 - INFO - Epoch [7/200], Train Loss: 0.0152, Val Loss: 0.0163\n",
      "2025-04-05 14:53:57,112 - INFO - Epoch [8/200], Train Loss: 0.0151, Val Loss: 0.0180\n",
      "2025-04-05 14:54:46,721 - INFO - Epoch [9/200], Train Loss: 0.0151, Val Loss: 0.0167\n",
      "2025-04-05 14:55:35,610 - INFO - Epoch [10/200], Train Loss: 0.0150, Val Loss: 0.0171\n",
      "2025-04-05 14:56:24,877 - INFO - Epoch [11/200], Train Loss: 0.0150, Val Loss: 0.0155\n",
      "2025-04-05 14:57:15,187 - INFO - Epoch [12/200], Train Loss: 0.0149, Val Loss: 0.0184\n",
      "2025-04-05 14:58:06,055 - INFO - Epoch [13/200], Train Loss: 0.0149, Val Loss: 0.0138\n",
      "2025-04-05 14:58:55,691 - INFO - Epoch [14/200], Train Loss: 0.0148, Val Loss: 0.0134\n",
      "2025-04-05 14:59:47,430 - INFO - Epoch [15/200], Train Loss: 0.0148, Val Loss: 0.0146\n",
      "2025-04-05 15:00:40,352 - INFO - Epoch [16/200], Train Loss: 0.0148, Val Loss: 0.0136\n",
      "2025-04-05 15:01:34,055 - INFO - Epoch [17/200], Train Loss: 0.0147, Val Loss: 0.0162\n",
      "2025-04-05 15:02:23,054 - INFO - Epoch [18/200], Train Loss: 0.0147, Val Loss: 0.0129\n",
      "2025-04-05 15:03:11,902 - INFO - Epoch [19/200], Train Loss: 0.0147, Val Loss: 0.0146\n",
      "2025-04-05 15:04:00,978 - INFO - Epoch [20/200], Train Loss: 0.0147, Val Loss: 0.0153\n",
      "2025-04-05 15:04:49,719 - INFO - Epoch [21/200], Train Loss: 0.0146, Val Loss: 0.0138\n",
      "2025-04-05 15:05:38,713 - INFO - Epoch [22/200], Train Loss: 0.0145, Val Loss: 0.0133\n",
      "2025-04-05 15:06:25,748 - INFO - Epoch [23/200], Train Loss: 0.0145, Val Loss: 0.0140\n",
      "2025-04-05 15:07:15,031 - INFO - Epoch [24/200], Train Loss: 0.0145, Val Loss: 0.0124\n",
      "2025-04-05 15:08:04,864 - INFO - Epoch [25/200], Train Loss: 0.0145, Val Loss: 0.0135\n",
      "2025-04-05 15:08:58,888 - INFO - Epoch [26/200], Train Loss: 0.0145, Val Loss: 0.0142\n",
      "2025-04-05 15:09:49,782 - INFO - Epoch [27/200], Train Loss: 0.0145, Val Loss: 0.0130\n",
      "2025-04-05 15:10:40,417 - INFO - Epoch [28/200], Train Loss: 0.0145, Val Loss: 0.0152\n",
      "2025-04-05 15:11:30,027 - INFO - Epoch [29/200], Train Loss: 0.0145, Val Loss: 0.0145\n",
      "2025-04-05 15:12:18,790 - INFO - Epoch [30/200], Train Loss: 0.0145, Val Loss: 0.0135\n",
      "2025-04-05 15:13:07,299 - INFO - Epoch [31/200], Train Loss: 0.0145, Val Loss: 0.0157\n",
      "2025-04-05 15:13:58,078 - INFO - Epoch [32/200], Train Loss: 0.0144, Val Loss: 0.0266\n",
      "2025-04-05 15:14:48,170 - INFO - Epoch [33/200], Train Loss: 0.0144, Val Loss: 0.0125\n",
      "2025-04-05 15:15:39,656 - INFO - Epoch [34/200], Train Loss: 0.0144, Val Loss: 0.0124\n",
      "2025-04-05 15:16:30,029 - INFO - Epoch [35/200], Train Loss: 0.0144, Val Loss: 0.0112\n",
      "2025-04-05 15:17:18,820 - INFO - Epoch [36/200], Train Loss: 0.0144, Val Loss: 0.0131\n",
      "2025-04-05 15:18:06,809 - INFO - Epoch [37/200], Train Loss: 0.0144, Val Loss: 0.0119\n",
      "2025-04-05 15:18:54,634 - INFO - Epoch [38/200], Train Loss: 0.0144, Val Loss: 0.0134\n",
      "2025-04-05 15:19:42,491 - INFO - Epoch [39/200], Train Loss: 0.0144, Val Loss: 0.0167\n",
      "2025-04-05 15:20:30,674 - INFO - Epoch [40/200], Train Loss: 0.0144, Val Loss: 0.0124\n",
      "2025-04-05 15:21:19,514 - INFO - Epoch [41/200], Train Loss: 0.0144, Val Loss: 0.0122\n",
      "2025-04-05 15:22:07,731 - INFO - Epoch [42/200], Train Loss: 0.0143, Val Loss: 0.0127\n",
      "2025-04-05 15:22:56,336 - INFO - Epoch [43/200], Train Loss: 0.0143, Val Loss: 0.0155\n",
      "2025-04-05 15:23:44,838 - INFO - Epoch [44/200], Train Loss: 0.0143, Val Loss: 0.0119\n",
      "2025-04-05 15:24:34,746 - INFO - Epoch [45/200], Train Loss: 0.0143, Val Loss: 0.0143\n",
      "2025-04-05 15:25:26,928 - INFO - Epoch [46/200], Train Loss: 0.0143, Val Loss: 0.0123\n",
      "2025-04-05 15:26:16,052 - INFO - Epoch [47/200], Train Loss: 0.0143, Val Loss: 0.0312\n",
      "2025-04-05 15:27:06,534 - INFO - Epoch [48/200], Train Loss: 0.0143, Val Loss: 0.0114\n",
      "2025-04-05 15:27:53,927 - INFO - Epoch [49/200], Train Loss: 0.0143, Val Loss: 0.0113\n",
      "2025-04-05 15:28:44,198 - INFO - Epoch [50/200], Train Loss: 0.0143, Val Loss: 0.0109\n",
      "2025-04-05 15:29:34,705 - INFO - Epoch [51/200], Train Loss: 0.0143, Val Loss: 0.0116\n",
      "2025-04-05 15:30:25,189 - INFO - Epoch [52/200], Train Loss: 0.0142, Val Loss: 0.0102\n",
      "2025-04-05 15:31:16,232 - INFO - Epoch [53/200], Train Loss: 0.0143, Val Loss: 0.0131\n",
      "2025-04-05 15:32:07,476 - INFO - Epoch [54/200], Train Loss: 0.0143, Val Loss: 0.0120\n",
      "2025-04-05 15:32:57,630 - INFO - Epoch [55/200], Train Loss: 0.0142, Val Loss: 0.0104\n",
      "2025-04-05 15:33:46,164 - INFO - Epoch [56/200], Train Loss: 0.0143, Val Loss: 0.0130\n",
      "2025-04-05 15:34:36,957 - INFO - Epoch [57/200], Train Loss: 0.0143, Val Loss: 0.0114\n",
      "2025-04-05 15:35:27,784 - INFO - Epoch [58/200], Train Loss: 0.0143, Val Loss: 0.0117\n",
      "2025-04-05 15:36:18,975 - INFO - Epoch [59/200], Train Loss: 0.0143, Val Loss: 0.0119\n",
      "2025-04-05 15:37:09,129 - INFO - Epoch [60/200], Train Loss: 0.0143, Val Loss: 0.0122\n",
      "2025-04-05 15:37:58,736 - INFO - Epoch [61/200], Train Loss: 0.0142, Val Loss: 0.0118\n",
      "2025-04-05 15:38:49,182 - INFO - Epoch [62/200], Train Loss: 0.0142, Val Loss: 0.0099\n",
      "2025-04-05 15:39:42,984 - INFO - Epoch [63/200], Train Loss: 0.0142, Val Loss: 0.0112\n",
      "2025-04-05 15:40:35,151 - INFO - Epoch [64/200], Train Loss: 0.0142, Val Loss: 0.0106\n",
      "2025-04-05 15:41:24,527 - INFO - Epoch [65/200], Train Loss: 0.0142, Val Loss: 0.0142\n",
      "2025-04-05 15:42:13,297 - INFO - Epoch [66/200], Train Loss: 0.0142, Val Loss: 0.0122\n",
      "2025-04-05 15:43:02,038 - INFO - Epoch [67/200], Train Loss: 0.0142, Val Loss: 0.0120\n",
      "2025-04-05 15:43:50,581 - INFO - Epoch [68/200], Train Loss: 0.0142, Val Loss: 0.0095\n",
      "2025-04-05 15:44:39,286 - INFO - Epoch [69/200], Train Loss: 0.0142, Val Loss: 0.0100\n",
      "2025-04-05 15:45:27,702 - INFO - Epoch [70/200], Train Loss: 0.0142, Val Loss: 0.0111\n",
      "2025-04-05 15:46:16,055 - INFO - Epoch [71/200], Train Loss: 0.0142, Val Loss: 0.0110\n",
      "2025-04-05 15:47:04,238 - INFO - Epoch [72/200], Train Loss: 0.0141, Val Loss: 0.0107\n",
      "2025-04-05 15:47:52,595 - INFO - Epoch [73/200], Train Loss: 0.0141, Val Loss: 0.0102\n",
      "2025-04-05 15:48:40,900 - INFO - Epoch [74/200], Train Loss: 0.0141, Val Loss: 0.0095\n",
      "2025-04-05 15:49:29,178 - INFO - Epoch [75/200], Train Loss: 0.0142, Val Loss: 0.0098\n",
      "2025-04-05 15:50:17,372 - INFO - Epoch [76/200], Train Loss: 0.0141, Val Loss: 0.0092\n",
      "2025-04-05 15:51:05,387 - INFO - Epoch [77/200], Train Loss: 0.0142, Val Loss: 0.0102\n",
      "2025-04-05 15:51:53,469 - INFO - Epoch [78/200], Train Loss: 0.0141, Val Loss: 0.0108\n",
      "2025-04-05 15:52:41,639 - INFO - Epoch [79/200], Train Loss: 0.0142, Val Loss: 0.0104\n",
      "2025-04-05 15:53:29,707 - INFO - Epoch [80/200], Train Loss: 0.0141, Val Loss: 0.0100\n",
      "2025-04-05 15:54:17,790 - INFO - Epoch [81/200], Train Loss: 0.0141, Val Loss: 0.0109\n",
      "2025-04-05 15:55:07,316 - INFO - Epoch [82/200], Train Loss: 0.0141, Val Loss: 0.0091\n",
      "2025-04-05 15:55:58,450 - INFO - Epoch [83/200], Train Loss: 0.0141, Val Loss: 0.0100\n",
      "2025-04-05 15:56:47,959 - INFO - Epoch [84/200], Train Loss: 0.0141, Val Loss: 0.0097\n",
      "2025-04-05 15:57:37,126 - INFO - Epoch [85/200], Train Loss: 0.0141, Val Loss: 0.0087\n",
      "2025-04-05 15:58:25,962 - INFO - Epoch [86/200], Train Loss: 0.0141, Val Loss: 0.0081\n",
      "2025-04-05 15:59:14,530 - INFO - Epoch [87/200], Train Loss: 0.0141, Val Loss: 0.0111\n",
      "2025-04-05 16:00:03,307 - INFO - Epoch [88/200], Train Loss: 0.0141, Val Loss: 0.0090\n",
      "2025-04-05 16:00:52,708 - INFO - Epoch [89/200], Train Loss: 0.0141, Val Loss: 0.0106\n",
      "2025-04-05 16:01:43,815 - INFO - Epoch [90/200], Train Loss: 0.0140, Val Loss: 0.0290\n",
      "2025-04-05 16:02:34,055 - INFO - Epoch [91/200], Train Loss: 0.0141, Val Loss: 0.0181\n",
      "2025-04-05 16:03:22,676 - INFO - Epoch [92/200], Train Loss: 0.0140, Val Loss: 0.0101\n",
      "2025-04-05 16:04:11,551 - INFO - Epoch [93/200], Train Loss: 0.0140, Val Loss: 0.0080\n",
      "2025-04-05 16:04:59,904 - INFO - Epoch [94/200], Train Loss: 0.0140, Val Loss: 0.0102\n",
      "2025-04-05 16:05:47,912 - INFO - Epoch [95/200], Train Loss: 0.0140, Val Loss: 0.0086\n",
      "2025-04-05 16:06:36,768 - INFO - Epoch [96/200], Train Loss: 0.0140, Val Loss: 0.0090\n",
      "2025-04-05 16:07:24,320 - INFO - Epoch [97/200], Train Loss: 0.0140, Val Loss: 0.0106\n",
      "2025-04-05 16:08:11,859 - INFO - Epoch [98/200], Train Loss: 0.0140, Val Loss: 0.0093\n",
      "2025-04-05 16:08:59,278 - INFO - Epoch [99/200], Train Loss: 0.0140, Val Loss: 0.0098\n",
      "2025-04-05 16:09:46,684 - INFO - Epoch [100/200], Train Loss: 0.0140, Val Loss: 0.0094\n",
      "2025-04-05 16:10:34,096 - INFO - Epoch [101/200], Train Loss: 0.0140, Val Loss: 0.0090\n",
      "2025-04-05 16:11:21,565 - INFO - Epoch [102/200], Train Loss: 0.0139, Val Loss: 0.0106\n",
      "2025-04-05 16:12:08,801 - INFO - Epoch [103/200], Train Loss: 0.0140, Val Loss: 0.0107\n",
      "2025-04-05 16:12:56,559 - INFO - Epoch [104/200], Train Loss: 0.0140, Val Loss: 0.0104\n",
      "2025-04-05 16:13:43,981 - INFO - Epoch [105/200], Train Loss: 0.0140, Val Loss: 0.0108\n",
      "2025-04-05 16:14:31,257 - INFO - Epoch [106/200], Train Loss: 0.0139, Val Loss: 0.0094\n",
      "2025-04-05 16:15:18,534 - INFO - Epoch [107/200], Train Loss: 0.0140, Val Loss: 0.0130\n",
      "2025-04-05 16:16:05,837 - INFO - Epoch [108/200], Train Loss: 0.0140, Val Loss: 0.0088\n",
      "2025-04-05 16:16:53,171 - INFO - Epoch [109/200], Train Loss: 0.0140, Val Loss: 0.0087\n",
      "2025-04-05 16:17:40,578 - INFO - Epoch [110/200], Train Loss: 0.0140, Val Loss: 0.0075\n",
      "2025-04-05 16:18:28,593 - INFO - Epoch [111/200], Train Loss: 0.0140, Val Loss: 0.0081\n",
      "2025-04-05 16:19:16,758 - INFO - Epoch [112/200], Train Loss: 0.0139, Val Loss: 0.0076\n",
      "2025-04-05 16:20:05,278 - INFO - Epoch [113/200], Train Loss: 0.0139, Val Loss: 0.0090\n",
      "2025-04-05 16:20:53,930 - INFO - Epoch [114/200], Train Loss: 0.0139, Val Loss: 0.0069\n",
      "2025-04-05 16:21:42,652 - INFO - Epoch [115/200], Train Loss: 0.0139, Val Loss: 0.0097\n",
      "2025-04-05 16:22:31,395 - INFO - Epoch [116/200], Train Loss: 0.0139, Val Loss: 0.0084\n",
      "2025-04-05 16:23:20,167 - INFO - Epoch [117/200], Train Loss: 0.0139, Val Loss: 0.0083\n",
      "2025-04-05 16:24:08,929 - INFO - Epoch [118/200], Train Loss: 0.0139, Val Loss: 0.0076\n",
      "2025-04-05 16:24:57,927 - INFO - Epoch [119/200], Train Loss: 0.0139, Val Loss: 0.0109\n",
      "2025-04-05 16:25:47,314 - INFO - Epoch [120/200], Train Loss: 0.0140, Val Loss: 0.0069\n",
      "2025-04-05 16:26:37,136 - INFO - Epoch [121/200], Train Loss: 0.0139, Val Loss: 0.0079\n",
      "2025-04-05 16:27:27,393 - INFO - Epoch [122/200], Train Loss: 0.0139, Val Loss: 0.0097\n",
      "2025-04-05 16:28:17,959 - INFO - Epoch [123/200], Train Loss: 0.0139, Val Loss: 0.0085\n",
      "2025-04-05 16:29:08,697 - INFO - Epoch [124/200], Train Loss: 0.0139, Val Loss: 0.0074\n",
      "2025-04-05 16:29:59,479 - INFO - Epoch [125/200], Train Loss: 0.0139, Val Loss: 0.0077\n",
      "2025-04-05 16:30:50,130 - INFO - Epoch [126/200], Train Loss: 0.0139, Val Loss: 0.0096\n",
      "2025-04-05 16:31:38,990 - INFO - Epoch [127/200], Train Loss: 0.0138, Val Loss: 0.0068\n",
      "2025-04-05 16:32:27,755 - INFO - Epoch [128/200], Train Loss: 0.0139, Val Loss: 0.0978\n",
      "2025-04-05 16:33:16,351 - INFO - Epoch [129/200], Train Loss: 0.0139, Val Loss: 0.0067\n",
      "2025-04-05 16:34:04,851 - INFO - Epoch [130/200], Train Loss: 0.0139, Val Loss: 0.0087\n",
      "2025-04-05 16:34:53,259 - INFO - Epoch [131/200], Train Loss: 0.0139, Val Loss: 0.0076\n",
      "2025-04-05 16:35:41,422 - INFO - Epoch [132/200], Train Loss: 0.0139, Val Loss: 0.0061\n",
      "2025-04-05 16:36:29,556 - INFO - Epoch [133/200], Train Loss: 0.0138, Val Loss: 0.0069\n",
      "2025-04-05 16:37:17,688 - INFO - Epoch [134/200], Train Loss: 0.0138, Val Loss: 0.0117\n",
      "2025-04-05 16:38:07,591 - INFO - Epoch [135/200], Train Loss: 0.0138, Val Loss: 0.0085\n",
      "2025-04-05 16:38:57,643 - INFO - Epoch [136/200], Train Loss: 0.0138, Val Loss: 0.0092\n",
      "2025-04-05 16:39:45,719 - INFO - Epoch [137/200], Train Loss: 0.0138, Val Loss: 0.0073\n",
      "2025-04-05 16:40:33,457 - INFO - Epoch [138/200], Train Loss: 0.0138, Val Loss: 0.0070\n",
      "2025-04-05 16:41:21,441 - INFO - Epoch [139/200], Train Loss: 0.0138, Val Loss: 0.0078\n",
      "2025-04-05 16:42:09,718 - INFO - Epoch [140/200], Train Loss: 0.0138, Val Loss: 0.0072\n",
      "2025-04-05 16:42:58,196 - INFO - Epoch [141/200], Train Loss: 0.0138, Val Loss: 0.0064\n",
      "2025-04-05 16:43:48,784 - INFO - Epoch [142/200], Train Loss: 0.0138, Val Loss: 0.0067\n",
      "2025-04-05 16:44:36,483 - INFO - Epoch [143/200], Train Loss: 0.0138, Val Loss: 0.0162\n",
      "2025-04-05 16:45:24,196 - INFO - Epoch [144/200], Train Loss: 0.0138, Val Loss: 0.0074\n",
      "2025-04-05 16:46:12,265 - INFO - Epoch [145/200], Train Loss: 0.0138, Val Loss: 0.0066\n",
      "2025-04-05 16:47:00,587 - INFO - Epoch [146/200], Train Loss: 0.0138, Val Loss: 0.0114\n",
      "2025-04-05 16:47:48,491 - INFO - Epoch [147/200], Train Loss: 0.0138, Val Loss: 0.0081\n",
      "2025-04-05 16:48:37,159 - INFO - Epoch [148/200], Train Loss: 0.0138, Val Loss: 0.0092\n",
      "2025-04-05 16:49:26,162 - INFO - Epoch [149/200], Train Loss: 0.0138, Val Loss: 0.0066\n",
      "2025-04-05 16:50:14,509 - INFO - Epoch [150/200], Train Loss: 0.0138, Val Loss: 0.0105\n",
      "2025-04-05 16:51:02,944 - INFO - Epoch [151/200], Train Loss: 0.0138, Val Loss: 0.0256\n",
      "2025-04-05 16:51:51,755 - INFO - Epoch [152/200], Train Loss: 0.0138, Val Loss: 0.0099\n",
      "2025-04-05 16:51:51,756 - INFO - Early stopping triggered at epoch 152\n",
      "2025-04-05 16:51:52,684 - INFO - 时间序列交叉验证结果已保存到 'time_series_cv_results_20250405_165152.csv'\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉验证 R² 分数: 0.4722 (±0.1040)\n",
      "交叉验证结果图表已保存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 16:52:32,151 - INFO - Epoch [1/200], Train Loss: 1.8142, Val Loss: 0.7792\n",
      "2025-04-05 16:53:10,802 - INFO - Epoch [2/200], Train Loss: 0.0602, Val Loss: 0.0857\n",
      "2025-04-05 16:53:49,190 - INFO - Epoch [3/200], Train Loss: 0.0225, Val Loss: 0.0408\n",
      "2025-04-05 16:54:27,793 - INFO - Epoch [4/200], Train Loss: 0.0154, Val Loss: 0.0206\n",
      "2025-04-05 16:55:06,373 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0189\n",
      "2025-04-05 16:55:46,179 - INFO - Epoch [6/200], Train Loss: 0.0155, Val Loss: 0.0209\n",
      "2025-04-05 16:56:24,727 - INFO - Epoch [7/200], Train Loss: 0.0155, Val Loss: 0.0202\n",
      "2025-04-05 16:57:03,078 - INFO - Epoch [8/200], Train Loss: 0.0155, Val Loss: 0.0186\n",
      "2025-04-05 16:57:41,623 - INFO - Epoch [9/200], Train Loss: 0.0155, Val Loss: 0.0188\n",
      "2025-04-05 16:58:20,703 - INFO - Epoch [10/200], Train Loss: 0.0155, Val Loss: 0.0171\n",
      "2025-04-05 16:59:00,023 - INFO - Epoch [11/200], Train Loss: 0.0154, Val Loss: 0.0188\n",
      "2025-04-05 16:59:39,022 - INFO - Epoch [12/200], Train Loss: 0.0153, Val Loss: 0.0189\n",
      "2025-04-05 17:00:17,351 - INFO - Epoch [13/200], Train Loss: 0.0153, Val Loss: 0.0192\n",
      "2025-04-05 17:00:55,696 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0165\n",
      "2025-04-05 17:01:33,947 - INFO - Epoch [15/200], Train Loss: 0.0152, Val Loss: 0.0172\n",
      "2025-04-05 17:02:12,183 - INFO - Epoch [16/200], Train Loss: 0.0152, Val Loss: 0.0176\n",
      "2025-04-05 17:02:50,359 - INFO - Epoch [17/200], Train Loss: 0.0151, Val Loss: 0.0186\n",
      "2025-04-05 17:03:28,562 - INFO - Epoch [18/200], Train Loss: 0.0151, Val Loss: 0.0146\n",
      "2025-04-05 17:04:07,833 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0163\n",
      "2025-04-05 17:04:46,838 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0156\n",
      "2025-04-05 17:05:25,985 - INFO - Epoch [21/200], Train Loss: 0.0150, Val Loss: 0.0180\n",
      "2025-04-05 17:06:05,898 - INFO - Epoch [22/200], Train Loss: 0.0149, Val Loss: 0.0136\n",
      "2025-04-05 17:06:44,460 - INFO - Epoch [23/200], Train Loss: 0.0149, Val Loss: 0.0149\n",
      "2025-04-05 17:07:24,013 - INFO - Epoch [24/200], Train Loss: 0.0149, Val Loss: 0.0152\n",
      "2025-04-05 17:08:04,045 - INFO - Epoch [25/200], Train Loss: 0.0149, Val Loss: 0.0155\n",
      "2025-04-05 17:08:42,777 - INFO - Epoch [26/200], Train Loss: 0.0149, Val Loss: 0.0142\n",
      "2025-04-05 17:09:21,519 - INFO - Epoch [27/200], Train Loss: 0.0149, Val Loss: 0.0162\n",
      "2025-04-05 17:10:00,438 - INFO - Epoch [28/200], Train Loss: 0.0148, Val Loss: 0.0162\n",
      "2025-04-05 17:10:40,207 - INFO - Epoch [29/200], Train Loss: 0.0148, Val Loss: 0.0177\n",
      "2025-04-05 17:11:21,250 - INFO - Epoch [30/200], Train Loss: 0.0148, Val Loss: 0.0142\n",
      "2025-04-05 17:11:58,677 - INFO - Epoch [31/200], Train Loss: 0.0148, Val Loss: 0.0153\n",
      "2025-04-05 17:12:38,403 - INFO - Epoch [32/200], Train Loss: 0.0147, Val Loss: 0.0155\n",
      "2025-04-05 17:13:17,685 - INFO - Epoch [33/200], Train Loss: 0.0147, Val Loss: 0.0130\n",
      "2025-04-05 17:13:56,871 - INFO - Epoch [34/200], Train Loss: 0.0147, Val Loss: 0.0147\n",
      "2025-04-05 17:14:35,942 - INFO - Epoch [35/200], Train Loss: 0.0147, Val Loss: 0.0150\n",
      "2025-04-05 17:15:14,546 - INFO - Epoch [36/200], Train Loss: 0.0147, Val Loss: 0.0129\n",
      "2025-04-05 17:15:53,655 - INFO - Epoch [37/200], Train Loss: 0.0147, Val Loss: 0.0139\n",
      "2025-04-05 17:16:33,026 - INFO - Epoch [38/200], Train Loss: 0.0147, Val Loss: 0.0126\n",
      "2025-04-05 17:17:12,143 - INFO - Epoch [39/200], Train Loss: 0.0147, Val Loss: 0.0127\n",
      "2025-04-05 17:17:51,881 - INFO - Epoch [40/200], Train Loss: 0.0147, Val Loss: 0.0113\n",
      "2025-04-05 17:18:31,286 - INFO - Epoch [41/200], Train Loss: 0.0147, Val Loss: 0.0136\n",
      "2025-04-05 17:19:09,884 - INFO - Epoch [42/200], Train Loss: 0.0146, Val Loss: 0.0130\n",
      "2025-04-05 17:19:48,908 - INFO - Epoch [43/200], Train Loss: 0.0147, Val Loss: 0.0135\n",
      "2025-04-05 17:20:28,383 - INFO - Epoch [44/200], Train Loss: 0.0146, Val Loss: 0.0116\n",
      "2025-04-05 17:21:07,617 - INFO - Epoch [45/200], Train Loss: 0.0146, Val Loss: 0.0108\n",
      "2025-04-05 17:21:46,634 - INFO - Epoch [46/200], Train Loss: 0.0146, Val Loss: 0.0131\n",
      "2025-04-05 17:22:22,977 - INFO - Epoch [47/200], Train Loss: 0.0146, Val Loss: 0.0121\n",
      "2025-04-05 17:23:01,981 - INFO - Epoch [48/200], Train Loss: 0.0146, Val Loss: 0.0129\n",
      "2025-04-05 17:23:41,163 - INFO - Epoch [49/200], Train Loss: 0.0146, Val Loss: 0.0113\n",
      "2025-04-05 17:24:20,752 - INFO - Epoch [50/200], Train Loss: 0.0146, Val Loss: 0.0147\n",
      "2025-04-05 17:24:59,565 - INFO - Epoch [51/200], Train Loss: 0.0146, Val Loss: 0.0122\n",
      "2025-04-05 17:25:38,250 - INFO - Epoch [52/200], Train Loss: 0.0145, Val Loss: 0.0115\n",
      "2025-04-05 17:26:16,873 - INFO - Epoch [53/200], Train Loss: 0.0145, Val Loss: 0.0113\n",
      "2025-04-05 17:26:56,603 - INFO - Epoch [54/200], Train Loss: 0.0145, Val Loss: 0.0115\n",
      "2025-04-05 17:27:36,598 - INFO - Epoch [55/200], Train Loss: 0.0145, Val Loss: 0.0119\n",
      "2025-04-05 17:28:17,234 - INFO - Epoch [56/200], Train Loss: 0.0145, Val Loss: 0.0099\n",
      "2025-04-05 17:28:58,227 - INFO - Epoch [57/200], Train Loss: 0.0145, Val Loss: 0.0144\n",
      "2025-04-05 17:29:38,150 - INFO - Epoch [58/200], Train Loss: 0.0145, Val Loss: 0.0117\n",
      "2025-04-05 17:30:17,838 - INFO - Epoch [59/200], Train Loss: 0.0145, Val Loss: 0.0116\n",
      "2025-04-05 17:30:57,739 - INFO - Epoch [60/200], Train Loss: 0.0145, Val Loss: 0.0118\n",
      "2025-04-05 17:31:37,266 - INFO - Epoch [61/200], Train Loss: 0.0145, Val Loss: 0.0107\n",
      "2025-04-05 17:32:17,190 - INFO - Epoch [62/200], Train Loss: 0.0145, Val Loss: 0.0105\n",
      "2025-04-05 17:32:56,588 - INFO - Epoch [63/200], Train Loss: 0.0145, Val Loss: 0.0093\n",
      "2025-04-05 17:33:35,946 - INFO - Epoch [64/200], Train Loss: 0.0145, Val Loss: 0.0139\n",
      "2025-04-05 17:34:15,546 - INFO - Epoch [65/200], Train Loss: 0.0145, Val Loss: 0.0089\n",
      "2025-04-05 17:34:55,336 - INFO - Epoch [66/200], Train Loss: 0.0145, Val Loss: 0.0111\n",
      "2025-04-05 17:35:35,553 - INFO - Epoch [67/200], Train Loss: 0.0145, Val Loss: 0.0132\n",
      "2025-04-05 17:36:15,143 - INFO - Epoch [68/200], Train Loss: 0.0145, Val Loss: 0.0141\n",
      "2025-04-05 17:36:55,584 - INFO - Epoch [69/200], Train Loss: 0.0145, Val Loss: 0.0126\n",
      "2025-04-05 17:37:36,872 - INFO - Epoch [70/200], Train Loss: 0.0144, Val Loss: 0.0106\n",
      "2025-04-05 17:38:18,159 - INFO - Epoch [71/200], Train Loss: 0.0145, Val Loss: 0.0093\n",
      "2025-04-05 17:38:58,250 - INFO - Epoch [72/200], Train Loss: 0.0144, Val Loss: 0.0095\n",
      "2025-04-05 17:39:37,924 - INFO - Epoch [73/200], Train Loss: 0.0144, Val Loss: 0.0113\n",
      "2025-04-05 17:40:16,985 - INFO - Epoch [74/200], Train Loss: 0.0144, Val Loss: 0.0101\n",
      "2025-04-05 17:40:57,530 - INFO - Epoch [75/200], Train Loss: 0.0144, Val Loss: 0.0118\n",
      "2025-04-05 17:41:38,745 - INFO - Epoch [76/200], Train Loss: 0.0144, Val Loss: 0.0092\n",
      "2025-04-05 17:42:19,129 - INFO - Epoch [77/200], Train Loss: 0.0144, Val Loss: 0.0097\n",
      "2025-04-05 17:42:59,616 - INFO - Epoch [78/200], Train Loss: 0.0144, Val Loss: 0.0092\n",
      "2025-04-05 17:43:39,900 - INFO - Epoch [79/200], Train Loss: 0.0144, Val Loss: 0.0084\n",
      "2025-04-05 17:44:21,161 - INFO - Epoch [80/200], Train Loss: 0.0144, Val Loss: 0.0096\n",
      "2025-04-05 17:45:01,941 - INFO - Epoch [81/200], Train Loss: 0.0144, Val Loss: 0.0112\n",
      "2025-04-05 17:45:42,593 - INFO - Epoch [82/200], Train Loss: 0.0143, Val Loss: 0.0091\n",
      "2025-04-05 17:46:22,715 - INFO - Epoch [83/200], Train Loss: 0.0144, Val Loss: 0.0086\n",
      "2025-04-05 17:47:03,010 - INFO - Epoch [84/200], Train Loss: 0.0143, Val Loss: 0.0105\n",
      "2025-04-05 17:47:43,393 - INFO - Epoch [85/200], Train Loss: 0.0144, Val Loss: 0.0097\n",
      "2025-04-05 17:48:24,777 - INFO - Epoch [86/200], Train Loss: 0.0144, Val Loss: 0.0109\n",
      "2025-04-05 17:49:05,286 - INFO - Epoch [87/200], Train Loss: 0.0144, Val Loss: 0.0069\n",
      "2025-04-05 17:49:44,848 - INFO - Epoch [88/200], Train Loss: 0.0144, Val Loss: 0.0107\n",
      "2025-04-05 17:50:24,766 - INFO - Epoch [89/200], Train Loss: 0.0144, Val Loss: 0.0079\n",
      "2025-04-05 17:51:03,796 - INFO - Epoch [90/200], Train Loss: 0.0144, Val Loss: 0.0082\n",
      "2025-04-05 17:51:44,700 - INFO - Epoch [91/200], Train Loss: 0.0144, Val Loss: 0.0094\n",
      "2025-04-05 17:52:25,750 - INFO - Epoch [92/200], Train Loss: 0.0143, Val Loss: 0.0105\n",
      "2025-04-05 17:53:05,364 - INFO - Epoch [93/200], Train Loss: 0.0143, Val Loss: 0.0112\n",
      "2025-04-05 17:53:44,357 - INFO - Epoch [94/200], Train Loss: 0.0143, Val Loss: 0.0086\n",
      "2025-04-05 17:54:23,123 - INFO - Epoch [95/200], Train Loss: 0.0143, Val Loss: 0.0091\n",
      "2025-04-05 17:55:02,448 - INFO - Epoch [96/200], Train Loss: 0.0143, Val Loss: 0.0084\n",
      "2025-04-05 17:55:41,453 - INFO - Epoch [97/200], Train Loss: 0.0143, Val Loss: 0.0079\n",
      "2025-04-05 17:56:20,377 - INFO - Epoch [98/200], Train Loss: 0.0143, Val Loss: 0.0099\n",
      "2025-04-05 17:57:00,747 - INFO - Epoch [99/200], Train Loss: 0.0143, Val Loss: 0.0086\n",
      "2025-04-05 17:57:41,711 - INFO - Epoch [100/200], Train Loss: 0.0143, Val Loss: 0.0111\n",
      "2025-04-05 17:58:20,523 - INFO - Epoch [101/200], Train Loss: 0.0143, Val Loss: 0.0101\n",
      "2025-04-05 17:58:59,804 - INFO - Epoch [102/200], Train Loss: 0.0143, Val Loss: 0.0098\n",
      "2025-04-05 17:59:39,252 - INFO - Epoch [103/200], Train Loss: 0.0143, Val Loss: 0.0068\n",
      "2025-04-05 18:00:20,170 - INFO - Epoch [104/200], Train Loss: 0.0142, Val Loss: 0.0094\n",
      "2025-04-05 18:01:05,129 - INFO - Epoch [105/200], Train Loss: 0.0142, Val Loss: 0.0068\n",
      "2025-04-05 18:01:50,690 - INFO - Epoch [106/200], Train Loss: 0.0142, Val Loss: 0.0083\n",
      "2025-04-05 18:02:29,168 - INFO - Epoch [107/200], Train Loss: 0.0143, Val Loss: 0.0088\n",
      "2025-04-05 18:03:08,962 - INFO - Epoch [108/200], Train Loss: 0.0142, Val Loss: 0.0071\n",
      "2025-04-05 18:03:48,855 - INFO - Epoch [109/200], Train Loss: 0.0143, Val Loss: 0.0089\n",
      "2025-04-05 18:04:28,493 - INFO - Epoch [110/200], Train Loss: 0.0143, Val Loss: 0.0077\n",
      "2025-04-05 18:05:08,750 - INFO - Epoch [111/200], Train Loss: 0.0143, Val Loss: 0.0085\n",
      "2025-04-05 18:05:48,459 - INFO - Epoch [112/200], Train Loss: 0.0142, Val Loss: 0.0080\n",
      "2025-04-05 18:06:31,272 - INFO - Epoch [113/200], Train Loss: 0.0142, Val Loss: 0.0056\n",
      "2025-04-05 18:07:14,148 - INFO - Epoch [114/200], Train Loss: 0.0142, Val Loss: 0.0070\n",
      "2025-04-05 18:07:53,304 - INFO - Epoch [115/200], Train Loss: 0.0142, Val Loss: 0.0089\n",
      "2025-04-05 18:09:13,325 - INFO - Epoch [116/200], Train Loss: 0.0142, Val Loss: 0.0067\n",
      "2025-04-05 18:09:52,638 - INFO - Epoch [117/200], Train Loss: 0.0142, Val Loss: 0.0077\n",
      "2025-04-05 18:10:32,054 - INFO - Epoch [118/200], Train Loss: 0.0142, Val Loss: 0.0078\n",
      "2025-04-05 18:11:11,473 - INFO - Epoch [119/200], Train Loss: 0.0142, Val Loss: 0.0097\n",
      "2025-04-05 18:11:50,804 - INFO - Epoch [120/200], Train Loss: 0.0142, Val Loss: 0.0115\n",
      "2025-04-05 18:12:30,495 - INFO - Epoch [121/200], Train Loss: 0.0142, Val Loss: 0.0065\n",
      "2025-04-05 18:13:10,140 - INFO - Epoch [122/200], Train Loss: 0.0142, Val Loss: 0.0055\n",
      "2025-04-05 18:13:49,746 - INFO - Epoch [123/200], Train Loss: 0.0142, Val Loss: 0.0098\n",
      "2025-04-05 18:14:29,303 - INFO - Epoch [124/200], Train Loss: 0.0142, Val Loss: 0.0085\n",
      "2025-04-05 18:15:09,260 - INFO - Epoch [125/200], Train Loss: 0.0142, Val Loss: 0.0058\n",
      "2025-04-05 18:15:49,030 - INFO - Epoch [126/200], Train Loss: 0.0141, Val Loss: 0.0083\n",
      "2025-04-05 18:16:28,950 - INFO - Epoch [127/200], Train Loss: 0.0142, Val Loss: 0.0064\n",
      "2025-04-05 18:17:09,166 - INFO - Epoch [128/200], Train Loss: 0.0142, Val Loss: 0.0073\n",
      "2025-04-05 18:17:50,131 - INFO - Epoch [129/200], Train Loss: 0.0142, Val Loss: 0.0071\n",
      "2025-04-05 18:18:30,057 - INFO - Epoch [130/200], Train Loss: 0.0141, Val Loss: 0.0066\n",
      "2025-04-05 18:19:09,648 - INFO - Epoch [131/200], Train Loss: 0.0142, Val Loss: 0.0060\n",
      "2025-04-05 18:19:49,113 - INFO - Epoch [132/200], Train Loss: 0.0141, Val Loss: 0.0097\n",
      "2025-04-05 18:20:28,792 - INFO - Epoch [133/200], Train Loss: 0.0141, Val Loss: 0.0101\n",
      "2025-04-05 18:21:08,298 - INFO - Epoch [134/200], Train Loss: 0.0141, Val Loss: 0.0087\n",
      "2025-04-05 18:21:47,687 - INFO - Epoch [135/200], Train Loss: 0.0142, Val Loss: 0.0044\n",
      "2025-04-05 18:22:27,313 - INFO - Epoch [136/200], Train Loss: 0.0141, Val Loss: 0.0073\n",
      "2025-04-05 18:23:06,799 - INFO - Epoch [137/200], Train Loss: 0.0141, Val Loss: 0.0061\n",
      "2025-04-05 18:23:46,178 - INFO - Epoch [138/200], Train Loss: 0.0141, Val Loss: 0.0064\n",
      "2025-04-05 18:24:25,615 - INFO - Epoch [139/200], Train Loss: 0.0141, Val Loss: 0.0067\n",
      "2025-04-05 18:25:05,295 - INFO - Epoch [140/200], Train Loss: 0.0141, Val Loss: 0.0063\n",
      "2025-04-05 18:25:44,715 - INFO - Epoch [141/200], Train Loss: 0.0141, Val Loss: 0.0068\n",
      "2025-04-05 18:26:24,113 - INFO - Epoch [142/200], Train Loss: 0.0141, Val Loss: 0.0113\n",
      "2025-04-05 18:27:03,596 - INFO - Epoch [143/200], Train Loss: 0.0141, Val Loss: 0.0064\n",
      "2025-04-05 18:27:42,980 - INFO - Epoch [144/200], Train Loss: 0.0141, Val Loss: 0.0087\n",
      "2025-04-05 18:28:22,369 - INFO - Epoch [145/200], Train Loss: 0.0141, Val Loss: 0.0070\n",
      "2025-04-05 18:29:01,666 - INFO - Epoch [146/200], Train Loss: 0.0140, Val Loss: 0.0068\n",
      "2025-04-05 18:29:41,199 - INFO - Epoch [147/200], Train Loss: 0.0141, Val Loss: 0.0101\n",
      "2025-04-05 18:30:20,628 - INFO - Epoch [148/200], Train Loss: 0.0140, Val Loss: 0.0067\n",
      "2025-04-05 18:31:00,038 - INFO - Epoch [149/200], Train Loss: 0.0141, Val Loss: 0.0059\n",
      "2025-04-05 18:31:39,410 - INFO - Epoch [150/200], Train Loss: 0.0141, Val Loss: 0.0078\n",
      "2025-04-05 18:32:18,997 - INFO - Epoch [151/200], Train Loss: 0.0141, Val Loss: 0.0066\n",
      "2025-04-05 18:32:58,404 - INFO - Epoch [152/200], Train Loss: 0.0140, Val Loss: 0.0064\n",
      "2025-04-05 18:33:37,764 - INFO - Epoch [153/200], Train Loss: 0.0140, Val Loss: 0.0077\n",
      "2025-04-05 18:34:17,120 - INFO - Epoch [154/200], Train Loss: 0.0141, Val Loss: 0.0067\n",
      "2025-04-05 18:34:56,739 - INFO - Epoch [155/200], Train Loss: 0.0140, Val Loss: 0.0066\n",
      "2025-04-05 18:34:56,740 - INFO - Early stopping triggered at epoch 155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本外测试 R²: 0.5203\n",
      "样本外测试调整后 R²: 0.5202\n",
      "\n",
      "样本外测试回归结果:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.520\n",
      "Model:                            OLS   Adj. R-squared:                  0.520\n",
      "Method:                 Least Squares   F-statistic:                     6518.\n",
      "Date:                Sat, 05 Apr 2025   Prob (F-statistic):               0.00\n",
      "Time:                        18:34:57   Log-Likelihood:             1.7601e+05\n",
      "No. Observations:              180354   AIC:                        -3.520e+05\n",
      "Df Residuals:                  180323   BIC:                        -3.516e+05\n",
      "Df Model:                          30                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.5018      0.000   2273.341      0.000       0.501       0.502\n",
      "x1         -6989.9192   1549.495     -4.511      0.000      -1e+04   -3952.945\n",
      "x2         -2384.8302   1450.329     -1.644      0.100   -5227.443     457.782\n",
      "x3         -1.006e+04   1595.484     -6.305      0.000   -1.32e+04   -6931.943\n",
      "x4          2117.4005   1439.701      1.471      0.141    -704.381    4939.182\n",
      "x5          1.185e+04   1510.631      7.846      0.000    8891.867    1.48e+04\n",
      "x6         -1165.7049   1592.878     -0.732      0.464   -4287.710    1956.300\n",
      "x7         -1.097e+04   1527.725     -7.182      0.000    -1.4e+04   -7977.969\n",
      "x8         -9427.1854   1503.056     -6.272      0.000   -1.24e+04   -6481.230\n",
      "x9          8479.1890   1597.804      5.307      0.000    5347.529    1.16e+04\n",
      "x10         8720.5646   1586.602      5.496      0.000    5610.861    1.18e+04\n",
      "x11         1.007e+04   1505.948      6.690      0.000    7122.487     1.3e+04\n",
      "x12         2351.0291   1488.579      1.579      0.114    -566.552    5268.610\n",
      "x13        -7535.6540   1464.909     -5.144      0.000   -1.04e+04   -4664.465\n",
      "x14         3983.8895   1405.104      2.835      0.005    1229.918    6737.861\n",
      "x15        -1.163e+04   1606.197     -7.241      0.000   -1.48e+04   -8482.487\n",
      "x16         9729.4864   1464.222      6.645      0.000    6859.646    1.26e+04\n",
      "x17         9547.0300   1506.048      6.339      0.000    6595.211    1.25e+04\n",
      "x18        -1.048e+04   1511.675     -6.932      0.000   -1.34e+04   -7516.019\n",
      "x19         1.231e+04   1599.953      7.694      0.000    9174.226    1.54e+04\n",
      "x20        -4094.2525   1520.997     -2.692      0.007   -7075.372   -1113.133\n",
      "x21         -1.09e+04   1427.893     -7.632      0.000   -1.37e+04   -8098.930\n",
      "x22         6141.1829   1599.300      3.840      0.000    3006.591    9275.775\n",
      "x23         1.133e+04   1593.464      7.110      0.000    8206.970    1.45e+04\n",
      "x24         5621.7116   1420.496      3.958      0.000    2837.572    8405.852\n",
      "x25        -9327.0392   1621.143     -5.753      0.000   -1.25e+04   -6149.636\n",
      "x26        -7130.9437   1448.863     -4.922      0.000   -9970.682   -4291.205\n",
      "x27         6637.0248   1494.042      4.442      0.000    3708.736    9565.314\n",
      "x28         5127.2349   1510.161      3.395      0.001    2167.353    8087.116\n",
      "x29         5325.7062   1425.149      3.737      0.000    2532.448    8118.965\n",
      "x30         7394.3492   1510.427      4.896      0.000    4433.948    1.04e+04\n",
      "==============================================================================\n",
      "Omnibus:                   122142.996   Durbin-Watson:                   0.028\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          3392412.826\n",
      "Skew:                           2.878   Prob(JB):                         0.00\n",
      "Kurtosis:                      23.452   Cond. No.                     4.24e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.24e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "eval_env: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/statsmodels/graphics/regressionplots.py:429: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fig = abline_plot(0, fitted_line.params[0], color='k', ax=ax)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本外测试结果图表已保存\n",
      "交叉验证结果已保存到 'cross_validation_results.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 18:35:05,305 - INFO - 训练因子数量为 25 的模型\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本外测试结果已保存到 'out_of_sample_results.csv'\n",
      "\n",
      "开始测试不同因子数量(25-35)的样本外R方...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 18:35:44,720 - INFO - Epoch [1/200], Train Loss: 1.1121, Val Loss: 0.1099\n",
      "2025-04-05 18:36:24,061 - INFO - Epoch [2/200], Train Loss: 0.0387, Val Loss: 0.0452\n",
      "2025-04-05 18:37:03,631 - INFO - Epoch [3/200], Train Loss: 0.0194, Val Loss: 0.0180\n",
      "2025-04-05 18:37:42,917 - INFO - Epoch [4/200], Train Loss: 0.0153, Val Loss: 0.0196\n",
      "2025-04-05 18:38:22,225 - INFO - Epoch [5/200], Train Loss: 0.0153, Val Loss: 0.0202\n",
      "2025-04-05 18:39:01,503 - INFO - Epoch [6/200], Train Loss: 0.0154, Val Loss: 0.0204\n",
      "2025-04-05 18:39:40,987 - INFO - Epoch [7/200], Train Loss: 0.0154, Val Loss: 0.0206\n",
      "2025-04-05 18:40:20,251 - INFO - Epoch [8/200], Train Loss: 0.0154, Val Loss: 0.0179\n",
      "2025-04-05 18:40:59,291 - INFO - Epoch [9/200], Train Loss: 0.0154, Val Loss: 0.0199\n",
      "2025-04-05 18:41:38,661 - INFO - Epoch [10/200], Train Loss: 0.0154, Val Loss: 0.0185\n",
      "2025-04-05 18:42:17,900 - INFO - Epoch [11/200], Train Loss: 0.0153, Val Loss: 0.0186\n",
      "2025-04-05 18:42:57,295 - INFO - Epoch [12/200], Train Loss: 0.0152, Val Loss: 0.0181\n",
      "2025-04-05 18:43:36,564 - INFO - Epoch [13/200], Train Loss: 0.0152, Val Loss: 0.0173\n",
      "2025-04-05 18:44:15,986 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0187\n",
      "2025-04-05 18:44:55,275 - INFO - Epoch [15/200], Train Loss: 0.0152, Val Loss: 0.0195\n",
      "2025-04-05 18:45:34,566 - INFO - Epoch [16/200], Train Loss: 0.0151, Val Loss: 0.0178\n",
      "2025-04-05 18:46:13,884 - INFO - Epoch [17/200], Train Loss: 0.0151, Val Loss: 0.0184\n",
      "2025-04-05 18:46:53,365 - INFO - Epoch [18/200], Train Loss: 0.0151, Val Loss: 0.0199\n",
      "2025-04-05 18:47:32,728 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0180\n",
      "2025-04-05 18:48:12,068 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0164\n",
      "2025-04-05 18:48:51,389 - INFO - Epoch [21/200], Train Loss: 0.0150, Val Loss: 0.0162\n",
      "2025-04-05 18:49:30,822 - INFO - Epoch [22/200], Train Loss: 0.0150, Val Loss: 0.0167\n",
      "2025-04-05 18:50:10,213 - INFO - Epoch [23/200], Train Loss: 0.0150, Val Loss: 0.0174\n",
      "2025-04-05 18:50:49,599 - INFO - Epoch [24/200], Train Loss: 0.0149, Val Loss: 0.0165\n",
      "2025-04-05 18:51:27,777 - INFO - Epoch [25/200], Train Loss: 0.0149, Val Loss: 0.0132\n",
      "2025-04-05 18:52:07,063 - INFO - Epoch [26/200], Train Loss: 0.0149, Val Loss: 0.0169\n",
      "2025-04-05 18:52:46,460 - INFO - Epoch [27/200], Train Loss: 0.0149, Val Loss: 0.0148\n",
      "2025-04-05 18:53:25,790 - INFO - Epoch [28/200], Train Loss: 0.0149, Val Loss: 0.0167\n",
      "2025-04-05 18:54:05,436 - INFO - Epoch [29/200], Train Loss: 0.0149, Val Loss: 0.0150\n",
      "2025-04-05 18:54:44,829 - INFO - Epoch [30/200], Train Loss: 0.0149, Val Loss: 0.0146\n",
      "2025-04-05 18:55:24,152 - INFO - Epoch [31/200], Train Loss: 0.0149, Val Loss: 0.0151\n",
      "2025-04-05 18:56:03,491 - INFO - Epoch [32/200], Train Loss: 0.0149, Val Loss: 0.0138\n",
      "2025-04-05 18:56:42,836 - INFO - Epoch [33/200], Train Loss: 0.0148, Val Loss: 0.0128\n",
      "2025-04-05 18:57:22,165 - INFO - Epoch [34/200], Train Loss: 0.0148, Val Loss: 0.0153\n",
      "2025-04-05 18:58:01,477 - INFO - Epoch [35/200], Train Loss: 0.0148, Val Loss: 0.0125\n",
      "2025-04-05 18:58:40,803 - INFO - Epoch [36/200], Train Loss: 0.0148, Val Loss: 0.0141\n",
      "2025-04-05 18:59:20,290 - INFO - Epoch [37/200], Train Loss: 0.0148, Val Loss: 0.0115\n",
      "2025-04-05 18:59:59,543 - INFO - Epoch [38/200], Train Loss: 0.0148, Val Loss: 0.0145\n",
      "2025-04-05 19:00:38,565 - INFO - Epoch [39/200], Train Loss: 0.0148, Val Loss: 0.0134\n",
      "2025-04-05 19:01:17,832 - INFO - Epoch [40/200], Train Loss: 0.0148, Val Loss: 0.0159\n",
      "2025-04-05 19:01:57,139 - INFO - Epoch [41/200], Train Loss: 0.0148, Val Loss: 0.0148\n",
      "2025-04-05 19:02:36,361 - INFO - Epoch [42/200], Train Loss: 0.0147, Val Loss: 0.0135\n",
      "2025-04-05 19:03:15,680 - INFO - Epoch [43/200], Train Loss: 0.0147, Val Loss: 0.0128\n",
      "2025-04-05 19:03:55,218 - INFO - Epoch [44/200], Train Loss: 0.0147, Val Loss: 0.0126\n",
      "2025-04-05 19:04:34,679 - INFO - Epoch [45/200], Train Loss: 0.0147, Val Loss: 0.0148\n",
      "2025-04-05 19:05:13,966 - INFO - Epoch [46/200], Train Loss: 0.0147, Val Loss: 0.0125\n",
      "2025-04-05 19:05:53,457 - INFO - Epoch [47/200], Train Loss: 0.0147, Val Loss: 0.0132\n",
      "2025-04-05 19:06:32,747 - INFO - Epoch [48/200], Train Loss: 0.0147, Val Loss: 0.0130\n",
      "2025-04-05 19:07:12,049 - INFO - Epoch [49/200], Train Loss: 0.0147, Val Loss: 0.0132\n",
      "2025-04-05 19:07:51,414 - INFO - Epoch [50/200], Train Loss: 0.0147, Val Loss: 0.0119\n",
      "2025-04-05 19:08:30,912 - INFO - Epoch [51/200], Train Loss: 0.0147, Val Loss: 0.0136\n",
      "2025-04-05 19:09:10,149 - INFO - Epoch [52/200], Train Loss: 0.0146, Val Loss: 0.0107\n",
      "2025-04-05 19:09:49,366 - INFO - Epoch [53/200], Train Loss: 0.0146, Val Loss: 0.0098\n",
      "2025-04-05 19:10:29,301 - INFO - Epoch [54/200], Train Loss: 0.0146, Val Loss: 0.0129\n",
      "2025-04-05 19:26:30,179 - INFO - Epoch [55/200], Train Loss: 0.0146, Val Loss: 0.0133\n",
      "2025-04-05 19:27:07,876 - INFO - Epoch [56/200], Train Loss: 0.0146, Val Loss: 0.0123\n",
      "2025-04-05 19:27:45,746 - INFO - Epoch [57/200], Train Loss: 0.0146, Val Loss: 0.0105\n",
      "2025-04-05 19:28:23,389 - INFO - Epoch [58/200], Train Loss: 0.0146, Val Loss: 0.0112\n",
      "2025-04-05 19:29:01,309 - INFO - Epoch [59/200], Train Loss: 0.0146, Val Loss: 0.0117\n",
      "2025-04-05 19:29:38,971 - INFO - Epoch [60/200], Train Loss: 0.0146, Val Loss: 0.0118\n",
      "2025-04-05 19:30:16,981 - INFO - Epoch [61/200], Train Loss: 0.0146, Val Loss: 0.0141\n",
      "2025-04-05 19:30:56,061 - INFO - Epoch [62/200], Train Loss: 0.0146, Val Loss: 0.0114\n",
      "2025-04-05 19:31:35,358 - INFO - Epoch [63/200], Train Loss: 0.0145, Val Loss: 0.0105\n",
      "2025-04-05 19:32:13,523 - INFO - Epoch [64/200], Train Loss: 0.0146, Val Loss: 0.0111\n",
      "2025-04-05 19:32:52,979 - INFO - Epoch [65/200], Train Loss: 0.0145, Val Loss: 0.0103\n",
      "2025-04-05 19:33:32,377 - INFO - Epoch [66/200], Train Loss: 0.0145, Val Loss: 0.0113\n",
      "2025-04-05 19:34:11,621 - INFO - Epoch [67/200], Train Loss: 0.0145, Val Loss: 0.0112\n",
      "2025-04-05 19:34:52,397 - INFO - Epoch [68/200], Train Loss: 0.0145, Val Loss: 0.0107\n",
      "2025-04-05 19:35:31,845 - INFO - Epoch [69/200], Train Loss: 0.0145, Val Loss: 0.0100\n",
      "2025-04-05 19:36:11,829 - INFO - Epoch [70/200], Train Loss: 0.0145, Val Loss: 0.0127\n",
      "2025-04-05 19:36:51,687 - INFO - Epoch [71/200], Train Loss: 0.0145, Val Loss: 0.0115\n",
      "2025-04-05 19:37:31,206 - INFO - Epoch [72/200], Train Loss: 0.0145, Val Loss: 0.0101\n",
      "2025-04-05 19:38:12,033 - INFO - Epoch [73/200], Train Loss: 0.0145, Val Loss: 0.0106\n",
      "2025-04-05 19:38:12,034 - INFO - Early stopping triggered at epoch 73\n",
      "2025-04-05 19:38:12,709 - INFO - 训练因子数量为 26 的模型\n",
      "2025-04-05 19:38:53,543 - INFO - Epoch [1/200], Train Loss: 1.1059, Val Loss: 0.5775\n",
      "2025-04-05 19:39:33,752 - INFO - Epoch [2/200], Train Loss: 0.0481, Val Loss: 0.0540\n",
      "2025-04-05 19:40:13,744 - INFO - Epoch [3/200], Train Loss: 0.0191, Val Loss: 0.0127\n",
      "2025-04-05 19:40:53,028 - INFO - Epoch [4/200], Train Loss: 0.0153, Val Loss: 0.0194\n",
      "2025-04-05 19:41:33,096 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0210\n",
      "2025-04-05 19:42:13,853 - INFO - Epoch [6/200], Train Loss: 0.0154, Val Loss: 0.0208\n",
      "2025-04-05 19:42:53,571 - INFO - Epoch [7/200], Train Loss: 0.0154, Val Loss: 0.0200\n",
      "2025-04-05 19:43:33,075 - INFO - Epoch [8/200], Train Loss: 0.0155, Val Loss: 0.0206\n",
      "2025-04-05 19:44:13,052 - INFO - Epoch [9/200], Train Loss: 0.0154, Val Loss: 0.0187\n",
      "2025-04-05 19:44:53,695 - INFO - Epoch [10/200], Train Loss: 0.0154, Val Loss: 0.0173\n",
      "2025-04-05 19:45:34,342 - INFO - Epoch [11/200], Train Loss: 0.0154, Val Loss: 0.0168\n",
      "2025-04-05 19:46:13,618 - INFO - Epoch [12/200], Train Loss: 0.0152, Val Loss: 0.0181\n",
      "2025-04-05 19:46:54,035 - INFO - Epoch [13/200], Train Loss: 0.0152, Val Loss: 0.0159\n",
      "2025-04-05 19:47:34,701 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0196\n",
      "2025-04-05 19:48:15,701 - INFO - Epoch [15/200], Train Loss: 0.0152, Val Loss: 0.0168\n",
      "2025-04-05 19:48:56,040 - INFO - Epoch [16/200], Train Loss: 0.0151, Val Loss: 0.0176\n",
      "2025-04-05 19:49:36,876 - INFO - Epoch [17/200], Train Loss: 0.0151, Val Loss: 0.0156\n",
      "2025-04-05 19:50:18,395 - INFO - Epoch [18/200], Train Loss: 0.0151, Val Loss: 0.0173\n",
      "2025-04-05 19:50:57,974 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0180\n",
      "2025-04-05 19:51:37,509 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0149\n",
      "2025-04-05 19:52:16,675 - INFO - Epoch [21/200], Train Loss: 0.0151, Val Loss: 0.0166\n",
      "2025-04-05 19:52:56,220 - INFO - Epoch [22/200], Train Loss: 0.0150, Val Loss: 0.0135\n",
      "2025-04-05 19:53:35,548 - INFO - Epoch [23/200], Train Loss: 0.0150, Val Loss: 0.0166\n",
      "2025-04-05 19:53:35,548 - INFO - Early stopping triggered at epoch 23\n",
      "2025-04-05 19:53:36,260 - INFO - 训练因子数量为 27 的模型\n",
      "2025-04-05 19:54:15,745 - INFO - Epoch [1/200], Train Loss: 0.9364, Val Loss: 0.0708\n",
      "2025-04-05 19:54:55,111 - INFO - Epoch [2/200], Train Loss: 0.0633, Val Loss: 0.0375\n",
      "2025-04-05 19:55:34,911 - INFO - Epoch [3/200], Train Loss: 0.0217, Val Loss: 0.0194\n",
      "2025-04-05 19:56:14,286 - INFO - Epoch [4/200], Train Loss: 0.0154, Val Loss: 0.0183\n",
      "2025-04-05 19:56:53,706 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0207\n",
      "2025-04-05 19:57:33,298 - INFO - Epoch [6/200], Train Loss: 0.0154, Val Loss: 0.0199\n",
      "2025-04-05 19:58:12,600 - INFO - Epoch [7/200], Train Loss: 0.0154, Val Loss: 0.0197\n",
      "2025-04-05 19:58:51,456 - INFO - Epoch [8/200], Train Loss: 0.0154, Val Loss: 0.0195\n",
      "2025-04-05 19:59:31,505 - INFO - Epoch [9/200], Train Loss: 0.0154, Val Loss: 0.0191\n",
      "2025-04-05 20:00:11,105 - INFO - Epoch [10/200], Train Loss: 0.0154, Val Loss: 0.0184\n",
      "2025-04-05 20:00:50,727 - INFO - Epoch [11/200], Train Loss: 0.0154, Val Loss: 0.0201\n",
      "2025-04-05 20:01:30,242 - INFO - Epoch [12/200], Train Loss: 0.0152, Val Loss: 0.0189\n",
      "2025-04-05 20:02:09,271 - INFO - Epoch [13/200], Train Loss: 0.0152, Val Loss: 0.0194\n",
      "2025-04-05 20:02:48,786 - INFO - Epoch [14/200], Train Loss: 0.0151, Val Loss: 0.0162\n",
      "2025-04-05 20:03:29,091 - INFO - Epoch [15/200], Train Loss: 0.0151, Val Loss: 0.0174\n",
      "2025-04-05 20:04:08,239 - INFO - Epoch [16/200], Train Loss: 0.0151, Val Loss: 0.0187\n",
      "2025-04-05 20:04:47,691 - INFO - Epoch [17/200], Train Loss: 0.0151, Val Loss: 0.0172\n",
      "2025-04-05 20:05:27,032 - INFO - Epoch [18/200], Train Loss: 0.0150, Val Loss: 0.0185\n",
      "2025-04-05 20:06:06,144 - INFO - Epoch [19/200], Train Loss: 0.0150, Val Loss: 0.0185\n",
      "2025-04-05 20:06:44,743 - INFO - Epoch [20/200], Train Loss: 0.0150, Val Loss: 0.0164\n",
      "2025-04-05 20:07:23,197 - INFO - Epoch [21/200], Train Loss: 0.0150, Val Loss: 0.0159\n",
      "2025-04-05 20:08:01,703 - INFO - Epoch [22/200], Train Loss: 0.0149, Val Loss: 0.0143\n",
      "2025-04-05 20:08:40,260 - INFO - Epoch [23/200], Train Loss: 0.0149, Val Loss: 0.0134\n",
      "2025-04-05 20:09:18,774 - INFO - Epoch [24/200], Train Loss: 0.0149, Val Loss: 0.0165\n",
      "2025-04-05 20:09:57,241 - INFO - Epoch [25/200], Train Loss: 0.0149, Val Loss: 0.0177\n",
      "2025-04-05 20:10:35,783 - INFO - Epoch [26/200], Train Loss: 0.0149, Val Loss: 0.0146\n",
      "2025-04-05 20:11:14,289 - INFO - Epoch [27/200], Train Loss: 0.0149, Val Loss: 0.0140\n",
      "2025-04-05 20:11:52,007 - INFO - Epoch [28/200], Train Loss: 0.0149, Val Loss: 0.0125\n",
      "2025-04-05 20:12:31,336 - INFO - Epoch [29/200], Train Loss: 0.0149, Val Loss: 0.0161\n",
      "2025-04-05 20:13:10,253 - INFO - Epoch [30/200], Train Loss: 0.0148, Val Loss: 0.0147\n",
      "2025-04-05 20:13:49,019 - INFO - Epoch [31/200], Train Loss: 0.0148, Val Loss: 0.0164\n",
      "2025-04-05 20:14:27,795 - INFO - Epoch [32/200], Train Loss: 0.0148, Val Loss: 0.0142\n",
      "2025-04-05 20:15:06,557 - INFO - Epoch [33/200], Train Loss: 0.0148, Val Loss: 0.0118\n",
      "2025-04-05 20:15:45,706 - INFO - Epoch [34/200], Train Loss: 0.0148, Val Loss: 0.0130\n",
      "2025-04-05 20:16:24,826 - INFO - Epoch [35/200], Train Loss: 0.0147, Val Loss: 0.0154\n",
      "2025-04-05 20:17:04,612 - INFO - Epoch [36/200], Train Loss: 0.0147, Val Loss: 0.0158\n",
      "2025-04-05 20:17:44,081 - INFO - Epoch [37/200], Train Loss: 0.0147, Val Loss: 0.0139\n",
      "2025-04-05 20:18:23,656 - INFO - Epoch [38/200], Train Loss: 0.0147, Val Loss: 0.0138\n",
      "2025-04-05 20:19:02,788 - INFO - Epoch [39/200], Train Loss: 0.0147, Val Loss: 0.0152\n",
      "2025-04-05 20:19:42,499 - INFO - Epoch [40/200], Train Loss: 0.0147, Val Loss: 0.0145\n",
      "2025-04-05 20:20:22,617 - INFO - Epoch [41/200], Train Loss: 0.0147, Val Loss: 0.0135\n",
      "2025-04-05 20:21:01,570 - INFO - Epoch [42/200], Train Loss: 0.0146, Val Loss: 0.0114\n",
      "2025-04-05 20:21:40,159 - INFO - Epoch [43/200], Train Loss: 0.0146, Val Loss: 0.0132\n",
      "2025-04-05 20:22:18,477 - INFO - Epoch [44/200], Train Loss: 0.0146, Val Loss: 0.0129\n",
      "2025-04-05 20:22:56,804 - INFO - Epoch [45/200], Train Loss: 0.0146, Val Loss: 0.0138\n",
      "2025-04-05 20:23:34,929 - INFO - Epoch [46/200], Train Loss: 0.0146, Val Loss: 0.0119\n",
      "2025-04-05 20:24:13,080 - INFO - Epoch [47/200], Train Loss: 0.0146, Val Loss: 0.0127\n",
      "2025-04-05 20:24:51,073 - INFO - Epoch [48/200], Train Loss: 0.0146, Val Loss: 0.0116\n",
      "2025-04-05 20:25:29,131 - INFO - Epoch [49/200], Train Loss: 0.0146, Val Loss: 0.0138\n",
      "2025-04-05 20:26:07,198 - INFO - Epoch [50/200], Train Loss: 0.0146, Val Loss: 0.0125\n",
      "2025-04-05 20:26:45,232 - INFO - Epoch [51/200], Train Loss: 0.0146, Val Loss: 0.0122\n",
      "2025-04-05 20:27:23,282 - INFO - Epoch [52/200], Train Loss: 0.0145, Val Loss: 0.0114\n",
      "2025-04-05 20:28:01,675 - INFO - Epoch [53/200], Train Loss: 0.0145, Val Loss: 0.0105\n",
      "2025-04-05 20:28:40,766 - INFO - Epoch [54/200], Train Loss: 0.0145, Val Loss: 0.0124\n",
      "2025-04-05 20:29:20,053 - INFO - Epoch [55/200], Train Loss: 0.0145, Val Loss: 0.0106\n",
      "2025-04-05 20:29:59,363 - INFO - Epoch [56/200], Train Loss: 0.0145, Val Loss: 0.0114\n",
      "2025-04-05 20:30:38,806 - INFO - Epoch [57/200], Train Loss: 0.0145, Val Loss: 0.0106\n",
      "2025-04-05 20:31:18,353 - INFO - Epoch [58/200], Train Loss: 0.0145, Val Loss: 0.0124\n",
      "2025-04-05 20:31:58,078 - INFO - Epoch [59/200], Train Loss: 0.0145, Val Loss: 0.0121\n",
      "2025-04-05 20:32:38,072 - INFO - Epoch [60/200], Train Loss: 0.0145, Val Loss: 0.0096\n",
      "2025-04-05 20:33:18,264 - INFO - Epoch [61/200], Train Loss: 0.0145, Val Loss: 0.0093\n",
      "2025-04-05 20:33:58,600 - INFO - Epoch [62/200], Train Loss: 0.0145, Val Loss: 0.0117\n",
      "2025-04-05 20:34:38,681 - INFO - Epoch [63/200], Train Loss: 0.0145, Val Loss: 0.0109\n",
      "2025-04-05 20:35:17,879 - INFO - Epoch [64/200], Train Loss: 0.0145, Val Loss: 0.0101\n",
      "2025-04-05 20:35:57,220 - INFO - Epoch [65/200], Train Loss: 0.0144, Val Loss: 0.0105\n",
      "2025-04-05 20:36:36,430 - INFO - Epoch [66/200], Train Loss: 0.0145, Val Loss: 0.0102\n",
      "2025-04-05 20:37:15,463 - INFO - Epoch [67/200], Train Loss: 0.0144, Val Loss: 0.0095\n",
      "2025-04-05 20:37:54,558 - INFO - Epoch [68/200], Train Loss: 0.0145, Val Loss: 0.0102\n",
      "2025-04-05 20:38:33,506 - INFO - Epoch [69/200], Train Loss: 0.0145, Val Loss: 0.0121\n",
      "2025-04-05 20:39:12,426 - INFO - Epoch [70/200], Train Loss: 0.0144, Val Loss: 0.0103\n",
      "2025-04-05 20:39:51,357 - INFO - Epoch [71/200], Train Loss: 0.0145, Val Loss: 0.0118\n",
      "2025-04-05 20:40:30,248 - INFO - Epoch [72/200], Train Loss: 0.0144, Val Loss: 0.0091\n",
      "2025-04-05 20:41:09,114 - INFO - Epoch [73/200], Train Loss: 0.0144, Val Loss: 0.0108\n",
      "2025-04-05 20:41:48,069 - INFO - Epoch [74/200], Train Loss: 0.0144, Val Loss: 0.0104\n",
      "2025-04-05 20:42:26,929 - INFO - Epoch [75/200], Train Loss: 0.0144, Val Loss: 0.0112\n",
      "2025-04-05 20:43:05,851 - INFO - Epoch [76/200], Train Loss: 0.0144, Val Loss: 0.0090\n",
      "2025-04-05 20:43:44,622 - INFO - Epoch [77/200], Train Loss: 0.0144, Val Loss: 0.0109\n",
      "2025-04-05 20:44:23,367 - INFO - Epoch [78/200], Train Loss: 0.0144, Val Loss: 0.0098\n",
      "2025-04-05 20:45:02,178 - INFO - Epoch [79/200], Train Loss: 0.0144, Val Loss: 0.0092\n",
      "2025-04-05 20:45:40,817 - INFO - Epoch [80/200], Train Loss: 0.0144, Val Loss: 0.0112\n",
      "2025-04-05 20:46:19,551 - INFO - Epoch [81/200], Train Loss: 0.0144, Val Loss: 0.0118\n",
      "2025-04-05 20:46:58,201 - INFO - Epoch [82/200], Train Loss: 0.0143, Val Loss: 0.0080\n",
      "2025-04-05 20:47:37,026 - INFO - Epoch [83/200], Train Loss: 0.0143, Val Loss: 0.0118\n",
      "2025-04-05 20:48:15,687 - INFO - Epoch [84/200], Train Loss: 0.0143, Val Loss: 0.0083\n",
      "2025-04-05 20:48:54,358 - INFO - Epoch [85/200], Train Loss: 0.0143, Val Loss: 0.0095\n",
      "2025-04-05 20:49:33,111 - INFO - Epoch [86/200], Train Loss: 0.0143, Val Loss: 0.0090\n",
      "2025-04-05 20:50:11,751 - INFO - Epoch [87/200], Train Loss: 0.0143, Val Loss: 0.0118\n",
      "2025-04-05 20:50:50,451 - INFO - Epoch [88/200], Train Loss: 0.0143, Val Loss: 0.0097\n",
      "2025-04-05 20:51:29,109 - INFO - Epoch [89/200], Train Loss: 0.0143, Val Loss: 0.0070\n",
      "2025-04-05 20:52:07,796 - INFO - Epoch [90/200], Train Loss: 0.0143, Val Loss: 0.0106\n",
      "2025-04-05 20:52:46,319 - INFO - Epoch [91/200], Train Loss: 0.0144, Val Loss: 0.0105\n",
      "2025-04-05 20:53:24,529 - INFO - Epoch [92/200], Train Loss: 0.0143, Val Loss: 0.0090\n",
      "2025-04-05 20:54:02,666 - INFO - Epoch [93/200], Train Loss: 0.0143, Val Loss: 0.0076\n",
      "2025-04-05 20:54:40,788 - INFO - Epoch [94/200], Train Loss: 0.0143, Val Loss: 0.0070\n",
      "2025-04-05 20:55:19,256 - INFO - Epoch [95/200], Train Loss: 0.0143, Val Loss: 0.0094\n",
      "2025-04-05 20:55:58,394 - INFO - Epoch [96/200], Train Loss: 0.0143, Val Loss: 0.0091\n",
      "2025-04-05 20:56:37,649 - INFO - Epoch [97/200], Train Loss: 0.0143, Val Loss: 0.0078\n",
      "2025-04-05 20:57:17,050 - INFO - Epoch [98/200], Train Loss: 0.0143, Val Loss: 0.0075\n",
      "2025-04-05 20:57:56,533 - INFO - Epoch [99/200], Train Loss: 0.0143, Val Loss: 0.0105\n",
      "2025-04-05 20:58:36,195 - INFO - Epoch [100/200], Train Loss: 0.0143, Val Loss: 0.0092\n",
      "2025-04-05 20:59:15,817 - INFO - Epoch [101/200], Train Loss: 0.0143, Val Loss: 0.0088\n",
      "2025-04-05 20:59:56,006 - INFO - Epoch [102/200], Train Loss: 0.0142, Val Loss: 0.0086\n",
      "2025-04-05 21:01:05,014 - INFO - Epoch [103/200], Train Loss: 0.0143, Val Loss: 0.0087\n",
      "2025-04-05 21:01:44,088 - INFO - Epoch [104/200], Train Loss: 0.0142, Val Loss: 0.0074\n",
      "2025-04-05 21:03:36,206 - INFO - Epoch [105/200], Train Loss: 0.0142, Val Loss: 0.0053\n",
      "2025-04-05 21:04:14,759 - INFO - Epoch [106/200], Train Loss: 0.0143, Val Loss: 0.0072\n",
      "2025-04-05 21:04:53,624 - INFO - Epoch [107/200], Train Loss: 0.0143, Val Loss: 0.0072\n",
      "2025-04-05 21:05:32,971 - INFO - Epoch [108/200], Train Loss: 0.0143, Val Loss: 0.0070\n",
      "2025-04-05 21:06:13,486 - INFO - Epoch [109/200], Train Loss: 0.0142, Val Loss: 0.0066\n",
      "2025-04-05 21:06:52,825 - INFO - Epoch [110/200], Train Loss: 0.0143, Val Loss: 0.0086\n",
      "2025-04-05 21:07:32,759 - INFO - Epoch [111/200], Train Loss: 0.0142, Val Loss: 0.0089\n",
      "2025-04-05 21:08:12,238 - INFO - Epoch [112/200], Train Loss: 0.0142, Val Loss: 0.0081\n",
      "2025-04-05 21:08:51,406 - INFO - Epoch [113/200], Train Loss: 0.0142, Val Loss: 0.0096\n",
      "2025-04-05 21:09:30,651 - INFO - Epoch [114/200], Train Loss: 0.0142, Val Loss: 0.0080\n",
      "2025-04-05 21:10:09,772 - INFO - Epoch [115/200], Train Loss: 0.0142, Val Loss: 0.0086\n",
      "2025-04-05 21:10:48,694 - INFO - Epoch [116/200], Train Loss: 0.0142, Val Loss: 0.0081\n",
      "2025-04-05 21:11:27,726 - INFO - Epoch [117/200], Train Loss: 0.0142, Val Loss: 0.0090\n",
      "2025-04-05 21:12:07,011 - INFO - Epoch [118/200], Train Loss: 0.0142, Val Loss: 0.0049\n",
      "2025-04-05 21:12:46,082 - INFO - Epoch [119/200], Train Loss: 0.0142, Val Loss: 0.0083\n",
      "2025-04-05 21:13:25,139 - INFO - Epoch [120/200], Train Loss: 0.0142, Val Loss: 0.0082\n",
      "2025-04-05 21:14:04,108 - INFO - Epoch [121/200], Train Loss: 0.0142, Val Loss: 0.0088\n",
      "2025-04-05 21:14:42,964 - INFO - Epoch [122/200], Train Loss: 0.0141, Val Loss: 0.0079\n",
      "2025-04-05 21:15:22,096 - INFO - Epoch [123/200], Train Loss: 0.0142, Val Loss: 0.0060\n",
      "2025-04-05 21:16:00,672 - INFO - Epoch [124/200], Train Loss: 0.0142, Val Loss: 0.0064\n",
      "2025-04-05 21:16:39,063 - INFO - Epoch [125/200], Train Loss: 0.0142, Val Loss: 0.0058\n",
      "2025-04-05 21:17:17,357 - INFO - Epoch [126/200], Train Loss: 0.0141, Val Loss: 0.0066\n",
      "2025-04-05 21:17:55,698 - INFO - Epoch [127/200], Train Loss: 0.0142, Val Loss: 0.0076\n",
      "2025-04-05 21:18:33,868 - INFO - Epoch [128/200], Train Loss: 0.0141, Val Loss: 0.0072\n",
      "2025-04-05 21:19:12,079 - INFO - Epoch [129/200], Train Loss: 0.0142, Val Loss: 0.0062\n",
      "2025-04-05 21:19:50,223 - INFO - Epoch [130/200], Train Loss: 0.0141, Val Loss: 0.0071\n",
      "2025-04-05 21:20:28,415 - INFO - Epoch [131/200], Train Loss: 0.0142, Val Loss: 0.0085\n",
      "2025-04-05 21:21:08,098 - INFO - Epoch [132/200], Train Loss: 0.0141, Val Loss: 0.0067\n",
      "2025-04-05 21:21:46,743 - INFO - Epoch [133/200], Train Loss: 0.0141, Val Loss: 0.0065\n",
      "2025-04-05 21:22:25,355 - INFO - Epoch [134/200], Train Loss: 0.0141, Val Loss: 0.0073\n",
      "2025-04-05 21:23:04,102 - INFO - Epoch [135/200], Train Loss: 0.0141, Val Loss: 0.0064\n",
      "2025-04-05 21:23:42,839 - INFO - Epoch [136/200], Train Loss: 0.0141, Val Loss: 0.0085\n",
      "2025-04-05 21:24:21,583 - INFO - Epoch [137/200], Train Loss: 0.0141, Val Loss: 0.0063\n",
      "2025-04-05 21:25:00,386 - INFO - Epoch [138/200], Train Loss: 0.0141, Val Loss: 0.0080\n",
      "2025-04-05 21:25:00,386 - INFO - Early stopping triggered at epoch 138\n",
      "2025-04-05 21:25:01,090 - INFO - 训练因子数量为 28 的模型\n",
      "2025-04-05 21:25:40,074 - INFO - Epoch [1/200], Train Loss: 0.6322, Val Loss: 0.2433\n",
      "2025-04-05 21:26:18,950 - INFO - Epoch [2/200], Train Loss: 0.0394, Val Loss: 0.1129\n",
      "2025-04-05 21:26:57,754 - INFO - Epoch [3/200], Train Loss: 0.0175, Val Loss: 0.0149\n",
      "2025-04-05 21:27:36,535 - INFO - Epoch [4/200], Train Loss: 0.0152, Val Loss: 0.0197\n",
      "2025-04-05 21:28:15,256 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0184\n",
      "2025-04-05 21:28:53,945 - INFO - Epoch [6/200], Train Loss: 0.0154, Val Loss: 0.0200\n",
      "2025-04-05 21:29:32,610 - INFO - Epoch [7/200], Train Loss: 0.0153, Val Loss: 0.0187\n",
      "2025-04-05 21:30:11,324 - INFO - Epoch [8/200], Train Loss: 0.0154, Val Loss: 0.0186\n",
      "2025-04-05 21:30:49,981 - INFO - Epoch [9/200], Train Loss: 0.0154, Val Loss: 0.0186\n",
      "2025-04-05 21:31:28,707 - INFO - Epoch [10/200], Train Loss: 0.0153, Val Loss: 0.0207\n",
      "2025-04-05 21:32:07,413 - INFO - Epoch [11/200], Train Loss: 0.0153, Val Loss: 0.0171\n",
      "2025-04-05 21:32:46,352 - INFO - Epoch [12/200], Train Loss: 0.0152, Val Loss: 0.0173\n",
      "2025-04-05 21:33:24,575 - INFO - Epoch [13/200], Train Loss: 0.0152, Val Loss: 0.0196\n",
      "2025-04-05 21:34:02,659 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0184\n",
      "2025-04-05 21:34:40,783 - INFO - Epoch [15/200], Train Loss: 0.0151, Val Loss: 0.0178\n",
      "2025-04-05 21:35:18,812 - INFO - Epoch [16/200], Train Loss: 0.0151, Val Loss: 0.0182\n",
      "2025-04-05 21:35:57,276 - INFO - Epoch [17/200], Train Loss: 0.0150, Val Loss: 0.0178\n",
      "2025-04-05 21:36:36,089 - INFO - Epoch [18/200], Train Loss: 0.0150, Val Loss: 0.0173\n",
      "2025-04-05 21:37:14,936 - INFO - Epoch [19/200], Train Loss: 0.0150, Val Loss: 0.0181\n",
      "2025-04-05 21:37:54,112 - INFO - Epoch [20/200], Train Loss: 0.0150, Val Loss: 0.0163\n",
      "2025-04-05 21:38:33,498 - INFO - Epoch [21/200], Train Loss: 0.0150, Val Loss: 0.0151\n",
      "2025-04-05 21:39:13,067 - INFO - Epoch [22/200], Train Loss: 0.0149, Val Loss: 0.0150\n",
      "2025-04-05 21:39:52,745 - INFO - Epoch [23/200], Train Loss: 0.0148, Val Loss: 0.0150\n",
      "2025-04-05 21:39:52,746 - INFO - Early stopping triggered at epoch 23\n",
      "2025-04-05 21:39:53,488 - INFO - 训练因子数量为 29 的模型\n",
      "2025-04-05 21:40:34,133 - INFO - Epoch [1/200], Train Loss: 1.2622, Val Loss: 0.2916\n",
      "2025-04-05 21:41:14,718 - INFO - Epoch [2/200], Train Loss: 0.0606, Val Loss: 0.1030\n",
      "2025-04-05 21:41:55,613 - INFO - Epoch [3/200], Train Loss: 0.0269, Val Loss: 0.0155\n",
      "2025-04-05 21:42:36,655 - INFO - Epoch [4/200], Train Loss: 0.0155, Val Loss: 0.0203\n",
      "2025-04-05 21:43:17,687 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0218\n",
      "2025-04-05 21:43:57,314 - INFO - Epoch [6/200], Train Loss: 0.0155, Val Loss: 0.0199\n",
      "2025-04-05 21:44:36,804 - INFO - Epoch [7/200], Train Loss: 0.0155, Val Loss: 0.0203\n",
      "2025-04-05 21:45:16,142 - INFO - Epoch [8/200], Train Loss: 0.0155, Val Loss: 0.0199\n",
      "2025-04-05 21:45:55,473 - INFO - Epoch [9/200], Train Loss: 0.0155, Val Loss: 0.0208\n",
      "2025-04-05 21:46:34,709 - INFO - Epoch [10/200], Train Loss: 0.0155, Val Loss: 0.0193\n",
      "2025-04-05 21:47:14,038 - INFO - Epoch [11/200], Train Loss: 0.0154, Val Loss: 0.0196\n",
      "2025-04-05 21:47:53,344 - INFO - Epoch [12/200], Train Loss: 0.0153, Val Loss: 0.0177\n",
      "2025-04-05 21:48:32,576 - INFO - Epoch [13/200], Train Loss: 0.0152, Val Loss: 0.0190\n",
      "2025-04-05 21:49:11,681 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0184\n",
      "2025-04-05 21:49:50,845 - INFO - Epoch [15/200], Train Loss: 0.0151, Val Loss: 0.0174\n",
      "2025-04-05 21:50:29,875 - INFO - Epoch [16/200], Train Loss: 0.0152, Val Loss: 0.0190\n",
      "2025-04-05 21:51:09,037 - INFO - Epoch [17/200], Train Loss: 0.0152, Val Loss: 0.0178\n",
      "2025-04-05 21:51:48,179 - INFO - Epoch [18/200], Train Loss: 0.0151, Val Loss: 0.0161\n",
      "2025-04-05 21:52:27,264 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0190\n",
      "2025-04-05 21:53:06,386 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0187\n",
      "2025-04-05 21:53:45,398 - INFO - Epoch [21/200], Train Loss: 0.0151, Val Loss: 0.0169\n",
      "2025-04-05 21:54:24,511 - INFO - Epoch [22/200], Train Loss: 0.0150, Val Loss: 0.0175\n",
      "2025-04-05 21:55:03,623 - INFO - Epoch [23/200], Train Loss: 0.0149, Val Loss: 0.0163\n",
      "2025-04-05 21:55:03,623 - INFO - Early stopping triggered at epoch 23\n",
      "2025-04-05 21:55:04,395 - INFO - 训练因子数量为 30 的模型\n",
      "2025-04-05 21:55:43,521 - INFO - Epoch [1/200], Train Loss: 0.6410, Val Loss: 0.0862\n",
      "2025-04-05 21:56:22,538 - INFO - Epoch [2/200], Train Loss: 0.0378, Val Loss: 0.0269\n",
      "2025-04-05 21:57:01,517 - INFO - Epoch [3/200], Train Loss: 0.0176, Val Loss: 0.0210\n",
      "2025-04-05 21:57:40,489 - INFO - Epoch [4/200], Train Loss: 0.0153, Val Loss: 0.0196\n",
      "2025-04-05 21:58:19,349 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0195\n",
      "2025-04-05 21:58:58,233 - INFO - Epoch [6/200], Train Loss: 0.0154, Val Loss: 0.0190\n",
      "2025-04-05 21:59:37,052 - INFO - Epoch [7/200], Train Loss: 0.0154, Val Loss: 0.0194\n",
      "2025-04-05 22:00:15,963 - INFO - Epoch [8/200], Train Loss: 0.0154, Val Loss: 0.0190\n",
      "2025-04-05 22:00:54,756 - INFO - Epoch [9/200], Train Loss: 0.0155, Val Loss: 0.0170\n",
      "2025-04-05 22:01:33,623 - INFO - Epoch [10/200], Train Loss: 0.0155, Val Loss: 0.0193\n",
      "2025-04-05 22:02:12,498 - INFO - Epoch [11/200], Train Loss: 0.0154, Val Loss: 0.0203\n",
      "2025-04-05 22:02:51,346 - INFO - Epoch [12/200], Train Loss: 0.0153, Val Loss: 0.0187\n",
      "2025-04-05 22:03:30,268 - INFO - Epoch [13/200], Train Loss: 0.0153, Val Loss: 0.0172\n",
      "2025-04-05 22:04:09,123 - INFO - Epoch [14/200], Train Loss: 0.0153, Val Loss: 0.0178\n",
      "2025-04-05 22:04:48,029 - INFO - Epoch [15/200], Train Loss: 0.0153, Val Loss: 0.0183\n",
      "2025-04-05 22:05:26,842 - INFO - Epoch [16/200], Train Loss: 0.0152, Val Loss: 0.0174\n",
      "2025-04-05 22:06:05,612 - INFO - Epoch [17/200], Train Loss: 0.0152, Val Loss: 0.0165\n",
      "2025-04-05 22:06:44,408 - INFO - Epoch [18/200], Train Loss: 0.0152, Val Loss: 0.0174\n",
      "2025-04-05 22:07:23,200 - INFO - Epoch [19/200], Train Loss: 0.0152, Val Loss: 0.0171\n",
      "2025-04-05 22:08:02,071 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0181\n",
      "2025-04-05 22:08:40,878 - INFO - Epoch [21/200], Train Loss: 0.0151, Val Loss: 0.0165\n",
      "2025-04-05 22:09:19,707 - INFO - Epoch [22/200], Train Loss: 0.0150, Val Loss: 0.0156\n",
      "2025-04-05 22:09:58,476 - INFO - Epoch [23/200], Train Loss: 0.0150, Val Loss: 0.0174\n",
      "2025-04-05 22:10:37,256 - INFO - Epoch [24/200], Train Loss: 0.0149, Val Loss: 0.0169\n",
      "2025-04-05 22:11:16,689 - INFO - Epoch [25/200], Train Loss: 0.0149, Val Loss: 0.0144\n",
      "2025-04-05 22:11:56,236 - INFO - Epoch [26/200], Train Loss: 0.0149, Val Loss: 0.0147\n",
      "2025-04-05 22:12:35,972 - INFO - Epoch [27/200], Train Loss: 0.0149, Val Loss: 0.0153\n",
      "2025-04-05 22:13:15,571 - INFO - Epoch [28/200], Train Loss: 0.0149, Val Loss: 0.0150\n",
      "2025-04-05 22:13:55,208 - INFO - Epoch [29/200], Train Loss: 0.0149, Val Loss: 0.0129\n",
      "2025-04-05 22:14:37,947 - INFO - Epoch [30/200], Train Loss: 0.0149, Val Loss: 0.0147\n",
      "2025-04-05 22:15:19,180 - INFO - Epoch [31/200], Train Loss: 0.0149, Val Loss: 0.0154\n",
      "2025-04-05 22:16:00,722 - INFO - Epoch [32/200], Train Loss: 0.0148, Val Loss: 0.0145\n",
      "2025-04-05 22:16:40,632 - INFO - Epoch [33/200], Train Loss: 0.0148, Val Loss: 0.0139\n",
      "2025-04-05 22:17:19,953 - INFO - Epoch [34/200], Train Loss: 0.0147, Val Loss: 0.0126\n",
      "2025-04-05 22:17:59,401 - INFO - Epoch [35/200], Train Loss: 0.0147, Val Loss: 0.0129\n",
      "2025-04-05 22:18:38,799 - INFO - Epoch [36/200], Train Loss: 0.0147, Val Loss: 0.0133\n",
      "2025-04-05 22:19:18,654 - INFO - Epoch [37/200], Train Loss: 0.0147, Val Loss: 0.0124\n",
      "2025-04-05 22:19:58,132 - INFO - Epoch [38/200], Train Loss: 0.0147, Val Loss: 0.0141\n",
      "2025-04-05 22:20:38,166 - INFO - Epoch [39/200], Train Loss: 0.0147, Val Loss: 0.0142\n",
      "2025-04-05 22:21:17,239 - INFO - Epoch [40/200], Train Loss: 0.0147, Val Loss: 0.0120\n",
      "2025-04-05 22:21:56,434 - INFO - Epoch [41/200], Train Loss: 0.0147, Val Loss: 0.0136\n",
      "2025-04-05 22:22:34,822 - INFO - Epoch [42/200], Train Loss: 0.0147, Val Loss: 0.0119\n",
      "2025-04-05 22:23:13,237 - INFO - Epoch [43/200], Train Loss: 0.0146, Val Loss: 0.0120\n",
      "2025-04-05 22:23:51,519 - INFO - Epoch [44/200], Train Loss: 0.0147, Val Loss: 0.0136\n",
      "2025-04-05 22:24:29,780 - INFO - Epoch [45/200], Train Loss: 0.0146, Val Loss: 0.0118\n",
      "2025-04-05 22:25:07,990 - INFO - Epoch [46/200], Train Loss: 0.0147, Val Loss: 0.0121\n",
      "2025-04-05 22:25:46,141 - INFO - Epoch [47/200], Train Loss: 0.0146, Val Loss: 0.0140\n",
      "2025-04-05 22:26:24,348 - INFO - Epoch [48/200], Train Loss: 0.0147, Val Loss: 0.0143\n",
      "2025-04-05 22:27:02,610 - INFO - Epoch [49/200], Train Loss: 0.0147, Val Loss: 0.0127\n",
      "2025-04-05 22:27:40,907 - INFO - Epoch [50/200], Train Loss: 0.0146, Val Loss: 0.0118\n",
      "2025-04-05 22:28:19,211 - INFO - Epoch [51/200], Train Loss: 0.0147, Val Loss: 0.0117\n",
      "2025-04-05 22:28:57,366 - INFO - Epoch [52/200], Train Loss: 0.0146, Val Loss: 0.0120\n",
      "2025-04-05 22:29:35,540 - INFO - Epoch [53/200], Train Loss: 0.0146, Val Loss: 0.0115\n",
      "2025-04-05 22:30:13,881 - INFO - Epoch [54/200], Train Loss: 0.0145, Val Loss: 0.0099\n",
      "2025-04-05 22:30:52,502 - INFO - Epoch [55/200], Train Loss: 0.0146, Val Loss: 0.0113\n",
      "2025-04-05 22:31:31,271 - INFO - Epoch [56/200], Train Loss: 0.0146, Val Loss: 0.0140\n",
      "2025-04-05 22:32:10,009 - INFO - Epoch [57/200], Train Loss: 0.0146, Val Loss: 0.0104\n",
      "2025-04-05 22:32:48,874 - INFO - Epoch [58/200], Train Loss: 0.0146, Val Loss: 0.0097\n",
      "2025-04-05 22:33:27,575 - INFO - Epoch [59/200], Train Loss: 0.0146, Val Loss: 0.0121\n",
      "2025-04-05 22:34:06,414 - INFO - Epoch [60/200], Train Loss: 0.0146, Val Loss: 0.0105\n",
      "2025-04-05 22:34:45,213 - INFO - Epoch [61/200], Train Loss: 0.0146, Val Loss: 0.0115\n",
      "2025-04-05 22:35:24,040 - INFO - Epoch [62/200], Train Loss: 0.0145, Val Loss: 0.0120\n",
      "2025-04-05 22:36:02,948 - INFO - Epoch [63/200], Train Loss: 0.0145, Val Loss: 0.0091\n",
      "2025-04-05 22:36:41,759 - INFO - Epoch [64/200], Train Loss: 0.0145, Val Loss: 0.0102\n",
      "2025-04-05 22:37:20,612 - INFO - Epoch [65/200], Train Loss: 0.0145, Val Loss: 0.0102\n",
      "2025-04-05 22:37:59,474 - INFO - Epoch [66/200], Train Loss: 0.0145, Val Loss: 0.0111\n",
      "2025-04-05 22:38:38,365 - INFO - Epoch [67/200], Train Loss: 0.0145, Val Loss: 0.0118\n",
      "2025-04-05 22:39:17,195 - INFO - Epoch [68/200], Train Loss: 0.0145, Val Loss: 0.0115\n",
      "2025-04-05 22:39:56,007 - INFO - Epoch [69/200], Train Loss: 0.0145, Val Loss: 0.0105\n",
      "2025-04-05 22:40:34,898 - INFO - Epoch [70/200], Train Loss: 0.0145, Val Loss: 0.0118\n",
      "2025-04-05 22:41:13,741 - INFO - Epoch [71/200], Train Loss: 0.0145, Val Loss: 0.0105\n",
      "2025-04-05 22:41:52,596 - INFO - Epoch [72/200], Train Loss: 0.0145, Val Loss: 0.0101\n",
      "2025-04-05 22:42:31,570 - INFO - Epoch [73/200], Train Loss: 0.0145, Val Loss: 0.0123\n",
      "2025-04-05 22:43:10,556 - INFO - Epoch [74/200], Train Loss: 0.0145, Val Loss: 0.0145\n",
      "2025-04-05 22:43:49,404 - INFO - Epoch [75/200], Train Loss: 0.0145, Val Loss: 0.0103\n",
      "2025-04-05 22:44:28,291 - INFO - Epoch [76/200], Train Loss: 0.0145, Val Loss: 0.0094\n",
      "2025-04-05 22:45:07,193 - INFO - Epoch [77/200], Train Loss: 0.0145, Val Loss: 0.0136\n",
      "2025-04-05 22:45:46,068 - INFO - Epoch [78/200], Train Loss: 0.0145, Val Loss: 0.0117\n",
      "2025-04-05 22:46:25,021 - INFO - Epoch [79/200], Train Loss: 0.0145, Val Loss: 0.0092\n",
      "2025-04-05 22:47:03,939 - INFO - Epoch [80/200], Train Loss: 0.0145, Val Loss: 0.0092\n",
      "2025-04-05 22:47:42,988 - INFO - Epoch [81/200], Train Loss: 0.0145, Val Loss: 0.0098\n",
      "2025-04-05 22:48:21,881 - INFO - Epoch [82/200], Train Loss: 0.0144, Val Loss: 0.0089\n",
      "2025-04-05 22:49:00,626 - INFO - Epoch [83/200], Train Loss: 0.0144, Val Loss: 0.0101\n",
      "2025-04-05 22:49:39,460 - INFO - Epoch [84/200], Train Loss: 0.0144, Val Loss: 0.0094\n",
      "2025-04-05 22:50:19,253 - INFO - Epoch [85/200], Train Loss: 0.0144, Val Loss: 0.0109\n",
      "2025-04-05 22:50:58,974 - INFO - Epoch [86/200], Train Loss: 0.0144, Val Loss: 0.0103\n",
      "2025-04-05 22:51:37,576 - INFO - Epoch [87/200], Train Loss: 0.0144, Val Loss: 0.0090\n",
      "2025-04-05 22:52:16,097 - INFO - Epoch [88/200], Train Loss: 0.0144, Val Loss: 0.0093\n",
      "2025-04-05 22:52:54,571 - INFO - Epoch [89/200], Train Loss: 0.0144, Val Loss: 0.0099\n",
      "2025-04-05 22:53:32,908 - INFO - Epoch [90/200], Train Loss: 0.0144, Val Loss: 0.0116\n",
      "2025-04-05 22:54:11,263 - INFO - Epoch [91/200], Train Loss: 0.0144, Val Loss: 0.0117\n",
      "2025-04-05 22:54:49,598 - INFO - Epoch [92/200], Train Loss: 0.0144, Val Loss: 0.0089\n",
      "2025-04-05 22:55:27,933 - INFO - Epoch [93/200], Train Loss: 0.0144, Val Loss: 0.0080\n",
      "2025-04-05 22:56:06,256 - INFO - Epoch [94/200], Train Loss: 0.0144, Val Loss: 0.0125\n",
      "2025-04-05 22:56:44,790 - INFO - Epoch [95/200], Train Loss: 0.0143, Val Loss: 0.0078\n",
      "2025-04-05 22:57:23,659 - INFO - Epoch [96/200], Train Loss: 0.0144, Val Loss: 0.0091\n",
      "2025-04-05 22:58:02,755 - INFO - Epoch [97/200], Train Loss: 0.0143, Val Loss: 0.0090\n",
      "2025-04-05 22:58:41,797 - INFO - Epoch [98/200], Train Loss: 0.0143, Val Loss: 0.0094\n",
      "2025-04-05 22:59:20,708 - INFO - Epoch [99/200], Train Loss: 0.0143, Val Loss: 0.0094\n",
      "2025-04-05 22:59:59,730 - INFO - Epoch [100/200], Train Loss: 0.0144, Val Loss: 0.0096\n",
      "2025-04-05 23:00:38,736 - INFO - Epoch [101/200], Train Loss: 0.0143, Val Loss: 0.0104\n",
      "2025-04-05 23:01:17,743 - INFO - Epoch [102/200], Train Loss: 0.0143, Val Loss: 0.0088\n",
      "2025-04-05 23:01:56,896 - INFO - Epoch [103/200], Train Loss: 0.0143, Val Loss: 0.0089\n",
      "2025-04-05 23:02:36,029 - INFO - Epoch [104/200], Train Loss: 0.0143, Val Loss: 0.0105\n",
      "2025-04-05 23:03:15,181 - INFO - Epoch [105/200], Train Loss: 0.0143, Val Loss: 0.0092\n",
      "2025-04-05 23:03:54,306 - INFO - Epoch [106/200], Train Loss: 0.0143, Val Loss: 0.0098\n",
      "2025-04-05 23:04:33,398 - INFO - Epoch [107/200], Train Loss: 0.0143, Val Loss: 0.0098\n",
      "2025-04-05 23:05:12,472 - INFO - Epoch [108/200], Train Loss: 0.0143, Val Loss: 0.0087\n",
      "2025-04-05 23:05:51,576 - INFO - Epoch [109/200], Train Loss: 0.0143, Val Loss: 0.0096\n",
      "2025-04-05 23:06:32,105 - INFO - Epoch [110/200], Train Loss: 0.0143, Val Loss: 0.0106\n",
      "2025-04-05 23:07:12,534 - INFO - Epoch [111/200], Train Loss: 0.0143, Val Loss: 0.0080\n",
      "2025-04-05 23:07:55,241 - INFO - Epoch [112/200], Train Loss: 0.0143, Val Loss: 0.0067\n",
      "2025-04-05 23:08:34,805 - INFO - Epoch [113/200], Train Loss: 0.0143, Val Loss: 0.0095\n",
      "2025-04-05 23:09:13,988 - INFO - Epoch [114/200], Train Loss: 0.0143, Val Loss: 0.0073\n",
      "2025-04-05 23:09:53,129 - INFO - Epoch [115/200], Train Loss: 0.0143, Val Loss: 0.0092\n",
      "2025-04-05 23:10:32,172 - INFO - Epoch [116/200], Train Loss: 0.0143, Val Loss: 0.0055\n",
      "2025-04-05 23:11:11,275 - INFO - Epoch [117/200], Train Loss: 0.0143, Val Loss: 0.0076\n",
      "2025-04-05 23:11:50,313 - INFO - Epoch [118/200], Train Loss: 0.0143, Val Loss: 0.0093\n",
      "2025-04-05 23:12:29,225 - INFO - Epoch [119/200], Train Loss: 0.0143, Val Loss: 0.0089\n",
      "2025-04-05 23:13:08,148 - INFO - Epoch [120/200], Train Loss: 0.0143, Val Loss: 0.0091\n",
      "2025-04-05 23:13:47,131 - INFO - Epoch [121/200], Train Loss: 0.0143, Val Loss: 0.0086\n",
      "2025-04-05 23:14:26,388 - INFO - Epoch [122/200], Train Loss: 0.0142, Val Loss: 0.0096\n",
      "2025-04-05 23:15:05,600 - INFO - Epoch [123/200], Train Loss: 0.0142, Val Loss: 0.0101\n",
      "2025-04-05 23:15:45,073 - INFO - Epoch [124/200], Train Loss: 0.0142, Val Loss: 0.0093\n",
      "2025-04-05 23:16:24,415 - INFO - Epoch [125/200], Train Loss: 0.0142, Val Loss: 0.0065\n",
      "2025-04-05 23:17:04,147 - INFO - Epoch [126/200], Train Loss: 0.0142, Val Loss: 0.0095\n",
      "2025-04-05 23:17:43,497 - INFO - Epoch [127/200], Train Loss: 0.0143, Val Loss: 0.0073\n",
      "2025-04-05 23:18:22,873 - INFO - Epoch [128/200], Train Loss: 0.0142, Val Loss: 0.0081\n",
      "2025-04-05 23:19:02,335 - INFO - Epoch [129/200], Train Loss: 0.0142, Val Loss: 0.0089\n",
      "2025-04-05 23:19:41,627 - INFO - Epoch [130/200], Train Loss: 0.0142, Val Loss: 0.0090\n",
      "2025-04-05 23:20:21,003 - INFO - Epoch [131/200], Train Loss: 0.0142, Val Loss: 0.0070\n",
      "2025-04-05 23:21:00,243 - INFO - Epoch [132/200], Train Loss: 0.0142, Val Loss: 0.0086\n",
      "2025-04-05 23:21:39,484 - INFO - Epoch [133/200], Train Loss: 0.0142, Val Loss: 0.0094\n",
      "2025-04-05 23:22:18,486 - INFO - Epoch [134/200], Train Loss: 0.0142, Val Loss: 0.0077\n",
      "2025-04-05 23:22:57,400 - INFO - Epoch [135/200], Train Loss: 0.0142, Val Loss: 0.0081\n",
      "2025-04-05 23:23:36,128 - INFO - Epoch [136/200], Train Loss: 0.0142, Val Loss: 0.0057\n",
      "2025-04-05 23:23:36,129 - INFO - Early stopping triggered at epoch 136\n",
      "2025-04-05 23:23:36,976 - INFO - 训练因子数量为 31 的模型\n",
      "2025-04-05 23:24:15,821 - INFO - Epoch [1/200], Train Loss: 1.3260, Val Loss: 0.6608\n",
      "2025-04-05 23:24:54,660 - INFO - Epoch [2/200], Train Loss: 0.0701, Val Loss: 0.2095\n",
      "2025-04-05 23:25:33,235 - INFO - Epoch [3/200], Train Loss: 0.0244, Val Loss: 0.0139\n",
      "2025-04-05 23:26:11,603 - INFO - Epoch [4/200], Train Loss: 0.0154, Val Loss: 0.0191\n",
      "2025-04-05 23:26:49,747 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0206\n",
      "2025-04-05 23:27:27,769 - INFO - Epoch [6/200], Train Loss: 0.0155, Val Loss: 0.0210\n",
      "2025-04-05 23:28:06,012 - INFO - Epoch [7/200], Train Loss: 0.0155, Val Loss: 0.0206\n",
      "2025-04-05 23:28:44,582 - INFO - Epoch [8/200], Train Loss: 0.0155, Val Loss: 0.0190\n",
      "2025-04-05 23:29:23,460 - INFO - Epoch [9/200], Train Loss: 0.0155, Val Loss: 0.0187\n",
      "2025-04-05 23:30:02,542 - INFO - Epoch [10/200], Train Loss: 0.0155, Val Loss: 0.0191\n",
      "2025-04-05 23:30:41,636 - INFO - Epoch [11/200], Train Loss: 0.0155, Val Loss: 0.0198\n",
      "2025-04-05 23:31:20,644 - INFO - Epoch [12/200], Train Loss: 0.0153, Val Loss: 0.0177\n",
      "2025-04-05 23:31:59,935 - INFO - Epoch [13/200], Train Loss: 0.0153, Val Loss: 0.0197\n",
      "2025-04-05 23:32:39,252 - INFO - Epoch [14/200], Train Loss: 0.0153, Val Loss: 0.0166\n",
      "2025-04-05 23:33:18,511 - INFO - Epoch [15/200], Train Loss: 0.0153, Val Loss: 0.0180\n",
      "2025-04-05 23:33:57,735 - INFO - Epoch [16/200], Train Loss: 0.0152, Val Loss: 0.0183\n",
      "2025-04-05 23:34:36,915 - INFO - Epoch [17/200], Train Loss: 0.0152, Val Loss: 0.0162\n",
      "2025-04-05 23:35:16,195 - INFO - Epoch [18/200], Train Loss: 0.0151, Val Loss: 0.0134\n",
      "2025-04-05 23:35:55,654 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0191\n",
      "2025-04-05 23:36:35,131 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0199\n",
      "2025-04-05 23:37:14,440 - INFO - Epoch [21/200], Train Loss: 0.0150, Val Loss: 0.0164\n",
      "2025-04-05 23:37:53,887 - INFO - Epoch [22/200], Train Loss: 0.0149, Val Loss: 0.0142\n",
      "2025-04-05 23:38:33,172 - INFO - Epoch [23/200], Train Loss: 0.0149, Val Loss: 0.0167\n",
      "2025-04-05 23:39:12,500 - INFO - Epoch [24/200], Train Loss: 0.0149, Val Loss: 0.0149\n",
      "2025-04-05 23:39:51,955 - INFO - Epoch [25/200], Train Loss: 0.0149, Val Loss: 0.0150\n",
      "2025-04-05 23:40:31,481 - INFO - Epoch [26/200], Train Loss: 0.0149, Val Loss: 0.0151\n",
      "2025-04-05 23:41:10,926 - INFO - Epoch [27/200], Train Loss: 0.0148, Val Loss: 0.0149\n",
      "2025-04-05 23:41:50,390 - INFO - Epoch [28/200], Train Loss: 0.0148, Val Loss: 0.0127\n",
      "2025-04-05 23:42:29,748 - INFO - Epoch [29/200], Train Loss: 0.0148, Val Loss: 0.0169\n",
      "2025-04-05 23:43:09,281 - INFO - Epoch [30/200], Train Loss: 0.0148, Val Loss: 0.0162\n",
      "2025-04-05 23:43:48,783 - INFO - Epoch [31/200], Train Loss: 0.0148, Val Loss: 0.0140\n",
      "2025-04-05 23:44:28,212 - INFO - Epoch [32/200], Train Loss: 0.0147, Val Loss: 0.0129\n",
      "2025-04-05 23:45:07,738 - INFO - Epoch [33/200], Train Loss: 0.0147, Val Loss: 0.0139\n",
      "2025-04-05 23:45:47,331 - INFO - Epoch [34/200], Train Loss: 0.0147, Val Loss: 0.0125\n",
      "2025-04-05 23:46:26,930 - INFO - Epoch [35/200], Train Loss: 0.0147, Val Loss: 0.0139\n",
      "2025-04-05 23:47:06,450 - INFO - Epoch [36/200], Train Loss: 0.0147, Val Loss: 0.0133\n",
      "2025-04-05 23:47:46,226 - INFO - Epoch [37/200], Train Loss: 0.0147, Val Loss: 0.0114\n",
      "2025-04-05 23:48:25,881 - INFO - Epoch [38/200], Train Loss: 0.0147, Val Loss: 0.0130\n",
      "2025-04-05 23:49:05,404 - INFO - Epoch [39/200], Train Loss: 0.0147, Val Loss: 0.0125\n",
      "2025-04-05 23:49:44,516 - INFO - Epoch [40/200], Train Loss: 0.0147, Val Loss: 0.0168\n",
      "2025-04-05 23:50:23,328 - INFO - Epoch [41/200], Train Loss: 0.0147, Val Loss: 0.0155\n",
      "2025-04-05 23:51:01,985 - INFO - Epoch [42/200], Train Loss: 0.0146, Val Loss: 0.0145\n",
      "2025-04-05 23:51:40,533 - INFO - Epoch [43/200], Train Loss: 0.0146, Val Loss: 0.0134\n",
      "2025-04-05 23:52:19,033 - INFO - Epoch [44/200], Train Loss: 0.0146, Val Loss: 0.0113\n",
      "2025-04-05 23:52:57,483 - INFO - Epoch [45/200], Train Loss: 0.0146, Val Loss: 0.0119\n",
      "2025-04-05 23:53:35,823 - INFO - Epoch [46/200], Train Loss: 0.0146, Val Loss: 0.0110\n",
      "2025-04-05 23:54:14,054 - INFO - Epoch [47/200], Train Loss: 0.0146, Val Loss: 0.0097\n",
      "2025-04-05 23:54:52,257 - INFO - Epoch [48/200], Train Loss: 0.0146, Val Loss: 0.0121\n",
      "2025-04-05 23:55:30,499 - INFO - Epoch [49/200], Train Loss: 0.0146, Val Loss: 0.0110\n",
      "2025-04-05 23:56:08,738 - INFO - Epoch [50/200], Train Loss: 0.0146, Val Loss: 0.0127\n",
      "2025-04-05 23:56:47,036 - INFO - Epoch [51/200], Train Loss: 0.0146, Val Loss: 0.0142\n",
      "2025-04-05 23:57:25,221 - INFO - Epoch [52/200], Train Loss: 0.0145, Val Loss: 0.0111\n",
      "2025-04-05 23:58:03,442 - INFO - Epoch [53/200], Train Loss: 0.0145, Val Loss: 0.0119\n",
      "2025-04-05 23:58:41,555 - INFO - Epoch [54/200], Train Loss: 0.0145, Val Loss: 0.0110\n",
      "2025-04-05 23:59:19,569 - INFO - Epoch [55/200], Train Loss: 0.0145, Val Loss: 0.0133\n",
      "2025-04-05 23:59:57,947 - INFO - Epoch [56/200], Train Loss: 0.0145, Val Loss: 0.0110\n",
      "2025-04-06 00:00:36,903 - INFO - Epoch [57/200], Train Loss: 0.0145, Val Loss: 0.0111\n",
      "2025-04-06 00:01:16,209 - INFO - Epoch [58/200], Train Loss: 0.0145, Val Loss: 0.0130\n",
      "2025-04-06 00:01:55,577 - INFO - Epoch [59/200], Train Loss: 0.0145, Val Loss: 0.0118\n",
      "2025-04-06 00:02:34,961 - INFO - Epoch [60/200], Train Loss: 0.0145, Val Loss: 0.0110\n",
      "2025-04-06 00:03:14,401 - INFO - Epoch [61/200], Train Loss: 0.0145, Val Loss: 0.0104\n",
      "2025-04-06 00:03:53,838 - INFO - Epoch [62/200], Train Loss: 0.0145, Val Loss: 0.0107\n",
      "2025-04-06 00:04:33,316 - INFO - Epoch [63/200], Train Loss: 0.0145, Val Loss: 0.0101\n",
      "2025-04-06 00:05:12,751 - INFO - Epoch [64/200], Train Loss: 0.0145, Val Loss: 0.0100\n",
      "2025-04-06 00:05:52,323 - INFO - Epoch [65/200], Train Loss: 0.0145, Val Loss: 0.0109\n",
      "2025-04-06 00:06:31,866 - INFO - Epoch [66/200], Train Loss: 0.0145, Val Loss: 0.0115\n",
      "2025-04-06 00:07:11,640 - INFO - Epoch [67/200], Train Loss: 0.0145, Val Loss: 0.0114\n",
      "2025-04-06 00:07:11,640 - INFO - Early stopping triggered at epoch 67\n",
      "2025-04-06 00:07:12,460 - INFO - 训练因子数量为 32 的模型\n",
      "2025-04-06 00:07:52,501 - INFO - Epoch [1/200], Train Loss: 1.2520, Val Loss: 0.2006\n",
      "2025-04-06 00:08:31,963 - INFO - Epoch [2/200], Train Loss: 0.0707, Val Loss: 0.0286\n",
      "2025-04-06 00:09:11,013 - INFO - Epoch [3/200], Train Loss: 0.0279, Val Loss: 0.0947\n",
      "2025-04-06 00:09:49,842 - INFO - Epoch [4/200], Train Loss: 0.0156, Val Loss: 0.0209\n",
      "2025-04-06 00:10:28,512 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0199\n",
      "2025-04-06 00:11:07,015 - INFO - Epoch [6/200], Train Loss: 0.0155, Val Loss: 0.0198\n",
      "2025-04-06 00:11:45,570 - INFO - Epoch [7/200], Train Loss: 0.0155, Val Loss: 0.0198\n",
      "2025-04-06 00:12:23,892 - INFO - Epoch [8/200], Train Loss: 0.0155, Val Loss: 0.0191\n",
      "2025-04-06 00:13:02,414 - INFO - Epoch [9/200], Train Loss: 0.0155, Val Loss: 0.0202\n",
      "2025-04-06 00:13:40,716 - INFO - Epoch [10/200], Train Loss: 0.0154, Val Loss: 0.0209\n",
      "2025-04-06 00:14:18,975 - INFO - Epoch [11/200], Train Loss: 0.0154, Val Loss: 0.0198\n",
      "2025-04-06 00:14:57,313 - INFO - Epoch [12/200], Train Loss: 0.0153, Val Loss: 0.0185\n",
      "2025-04-06 00:15:35,589 - INFO - Epoch [13/200], Train Loss: 0.0153, Val Loss: 0.0181\n",
      "2025-04-06 00:16:13,909 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0179\n",
      "2025-04-06 00:16:52,238 - INFO - Epoch [15/200], Train Loss: 0.0152, Val Loss: 0.0196\n",
      "2025-04-06 00:17:30,414 - INFO - Epoch [16/200], Train Loss: 0.0152, Val Loss: 0.0149\n",
      "2025-04-06 00:18:08,671 - INFO - Epoch [17/200], Train Loss: 0.0151, Val Loss: 0.0172\n",
      "2025-04-06 00:18:46,803 - INFO - Epoch [18/200], Train Loss: 0.0151, Val Loss: 0.0175\n",
      "2025-04-06 00:19:24,955 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0183\n",
      "2025-04-06 00:20:03,171 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0176\n",
      "2025-04-06 00:20:41,248 - INFO - Epoch [21/200], Train Loss: 0.0151, Val Loss: 0.0183\n",
      "2025-04-06 00:21:19,415 - INFO - Epoch [22/200], Train Loss: 0.0149, Val Loss: 0.0171\n",
      "2025-04-06 00:21:57,562 - INFO - Epoch [23/200], Train Loss: 0.0149, Val Loss: 0.0150\n",
      "2025-04-06 00:22:35,721 - INFO - Epoch [24/200], Train Loss: 0.0149, Val Loss: 0.0141\n",
      "2025-04-06 00:23:13,972 - INFO - Epoch [25/200], Train Loss: 0.0149, Val Loss: 0.0154\n",
      "2025-04-06 00:23:52,179 - INFO - Epoch [26/200], Train Loss: 0.0149, Val Loss: 0.0149\n",
      "2025-04-06 00:24:30,963 - INFO - Epoch [27/200], Train Loss: 0.0149, Val Loss: 0.0151\n",
      "2025-04-06 00:25:10,026 - INFO - Epoch [28/200], Train Loss: 0.0149, Val Loss: 0.0141\n",
      "2025-04-06 00:25:49,241 - INFO - Epoch [29/200], Train Loss: 0.0148, Val Loss: 0.0136\n",
      "2025-04-06 00:26:28,472 - INFO - Epoch [30/200], Train Loss: 0.0148, Val Loss: 0.0147\n",
      "2025-04-06 00:27:07,745 - INFO - Epoch [31/200], Train Loss: 0.0148, Val Loss: 0.0155\n",
      "2025-04-06 00:27:47,243 - INFO - Epoch [32/200], Train Loss: 0.0148, Val Loss: 0.0125\n",
      "2025-04-06 00:28:26,631 - INFO - Epoch [33/200], Train Loss: 0.0148, Val Loss: 0.0144\n",
      "2025-04-06 00:29:06,176 - INFO - Epoch [34/200], Train Loss: 0.0147, Val Loss: 0.0131\n",
      "2025-04-06 00:29:45,643 - INFO - Epoch [35/200], Train Loss: 0.0148, Val Loss: 0.0128\n",
      "2025-04-06 00:30:25,191 - INFO - Epoch [36/200], Train Loss: 0.0147, Val Loss: 0.0128\n",
      "2025-04-06 00:31:04,862 - INFO - Epoch [37/200], Train Loss: 0.0147, Val Loss: 0.0152\n",
      "2025-04-06 00:31:44,699 - INFO - Epoch [38/200], Train Loss: 0.0147, Val Loss: 0.0127\n",
      "2025-04-06 00:32:24,055 - INFO - Epoch [39/200], Train Loss: 0.0147, Val Loss: 0.0126\n",
      "2025-04-06 00:33:03,201 - INFO - Epoch [40/200], Train Loss: 0.0147, Val Loss: 0.0136\n",
      "2025-04-06 00:33:42,016 - INFO - Epoch [41/200], Train Loss: 0.0147, Val Loss: 0.0115\n",
      "2025-04-06 00:34:20,701 - INFO - Epoch [42/200], Train Loss: 0.0146, Val Loss: 0.0130\n",
      "2025-04-06 00:34:59,438 - INFO - Epoch [43/200], Train Loss: 0.0146, Val Loss: 0.0124\n",
      "2025-04-06 00:35:38,021 - INFO - Epoch [44/200], Train Loss: 0.0146, Val Loss: 0.0121\n",
      "2025-04-06 00:36:16,568 - INFO - Epoch [45/200], Train Loss: 0.0146, Val Loss: 0.0142\n",
      "2025-04-06 00:36:55,002 - INFO - Epoch [46/200], Train Loss: 0.0146, Val Loss: 0.0122\n",
      "2025-04-06 00:37:33,432 - INFO - Epoch [47/200], Train Loss: 0.0146, Val Loss: 0.0116\n",
      "2025-04-06 00:38:11,950 - INFO - Epoch [48/200], Train Loss: 0.0146, Val Loss: 0.0119\n",
      "2025-04-06 00:38:50,303 - INFO - Epoch [49/200], Train Loss: 0.0146, Val Loss: 0.0130\n",
      "2025-04-06 00:39:28,723 - INFO - Epoch [50/200], Train Loss: 0.0146, Val Loss: 0.0132\n",
      "2025-04-06 00:40:06,967 - INFO - Epoch [51/200], Train Loss: 0.0146, Val Loss: 0.0122\n",
      "2025-04-06 00:40:45,211 - INFO - Epoch [52/200], Train Loss: 0.0145, Val Loss: 0.0112\n",
      "2025-04-06 00:41:23,474 - INFO - Epoch [53/200], Train Loss: 0.0145, Val Loss: 0.0138\n",
      "2025-04-06 00:42:01,740 - INFO - Epoch [54/200], Train Loss: 0.0145, Val Loss: 0.0115\n",
      "2025-04-06 00:42:40,092 - INFO - Epoch [55/200], Train Loss: 0.0145, Val Loss: 0.0111\n",
      "2025-04-06 00:43:18,360 - INFO - Epoch [56/200], Train Loss: 0.0145, Val Loss: 0.0121\n",
      "2025-04-06 00:43:56,622 - INFO - Epoch [57/200], Train Loss: 0.0145, Val Loss: 0.0106\n",
      "2025-04-06 00:44:34,952 - INFO - Epoch [58/200], Train Loss: 0.0145, Val Loss: 0.0105\n",
      "2025-04-06 00:45:13,206 - INFO - Epoch [59/200], Train Loss: 0.0145, Val Loss: 0.0133\n",
      "2025-04-06 00:45:51,423 - INFO - Epoch [60/200], Train Loss: 0.0145, Val Loss: 0.0095\n",
      "2025-04-06 00:46:29,614 - INFO - Epoch [61/200], Train Loss: 0.0146, Val Loss: 0.0121\n",
      "2025-04-06 00:47:07,838 - INFO - Epoch [62/200], Train Loss: 0.0145, Val Loss: 0.0092\n",
      "2025-04-06 00:47:46,219 - INFO - Epoch [63/200], Train Loss: 0.0145, Val Loss: 0.0117\n",
      "2025-04-06 00:48:24,363 - INFO - Epoch [64/200], Train Loss: 0.0145, Val Loss: 0.0105\n",
      "2025-04-06 00:49:02,590 - INFO - Epoch [65/200], Train Loss: 0.0145, Val Loss: 0.0097\n",
      "2025-04-06 00:49:40,741 - INFO - Epoch [66/200], Train Loss: 0.0145, Val Loss: 0.0123\n",
      "2025-04-06 00:50:19,296 - INFO - Epoch [67/200], Train Loss: 0.0145, Val Loss: 0.0130\n",
      "2025-04-06 00:50:58,391 - INFO - Epoch [68/200], Train Loss: 0.0145, Val Loss: 0.0094\n",
      "2025-04-06 00:51:37,557 - INFO - Epoch [69/200], Train Loss: 0.0145, Val Loss: 0.0095\n",
      "2025-04-06 00:52:16,806 - INFO - Epoch [70/200], Train Loss: 0.0145, Val Loss: 0.0112\n",
      "2025-04-06 00:52:56,210 - INFO - Epoch [71/200], Train Loss: 0.0145, Val Loss: 0.0092\n",
      "2025-04-06 00:53:35,546 - INFO - Epoch [72/200], Train Loss: 0.0144, Val Loss: 0.0139\n",
      "2025-04-06 00:54:14,962 - INFO - Epoch [73/200], Train Loss: 0.0144, Val Loss: 0.0085\n",
      "2025-04-06 00:54:54,483 - INFO - Epoch [74/200], Train Loss: 0.0144, Val Loss: 0.0104\n",
      "2025-04-06 00:55:34,023 - INFO - Epoch [75/200], Train Loss: 0.0145, Val Loss: 0.0100\n",
      "2025-04-06 00:56:13,667 - INFO - Epoch [76/200], Train Loss: 0.0144, Val Loss: 0.0123\n",
      "2025-04-06 00:56:53,365 - INFO - Epoch [77/200], Train Loss: 0.0144, Val Loss: 0.0121\n",
      "2025-04-06 00:57:33,034 - INFO - Epoch [78/200], Train Loss: 0.0144, Val Loss: 0.0103\n",
      "2025-04-06 00:58:12,273 - INFO - Epoch [79/200], Train Loss: 0.0144, Val Loss: 0.0097\n",
      "2025-04-06 00:58:51,263 - INFO - Epoch [80/200], Train Loss: 0.0144, Val Loss: 0.0114\n",
      "2025-04-06 00:59:30,116 - INFO - Epoch [81/200], Train Loss: 0.0144, Val Loss: 0.0103\n",
      "2025-04-06 01:00:08,849 - INFO - Epoch [82/200], Train Loss: 0.0143, Val Loss: 0.0109\n",
      "2025-04-06 01:00:47,592 - INFO - Epoch [83/200], Train Loss: 0.0144, Val Loss: 0.0115\n",
      "2025-04-06 01:01:26,175 - INFO - Epoch [84/200], Train Loss: 0.0144, Val Loss: 0.0102\n",
      "2025-04-06 01:02:04,743 - INFO - Epoch [85/200], Train Loss: 0.0144, Val Loss: 0.0084\n",
      "2025-04-06 01:57:24,192 - INFO - Epoch [86/200], Train Loss: 0.0144, Val Loss: 0.0081\n",
      "2025-04-06 03:40:37,515 - INFO - Epoch [87/200], Train Loss: 0.0144, Val Loss: 0.0082\n",
      "2025-04-06 05:26:56,799 - INFO - Epoch [88/200], Train Loss: 0.0144, Val Loss: 0.0091\n",
      "2025-04-06 07:00:05,174 - INFO - Epoch [89/200], Train Loss: 0.0144, Val Loss: 0.0088\n",
      "2025-04-06 08:14:19,469 - INFO - Epoch [90/200], Train Loss: 0.0144, Val Loss: 0.0094\n",
      "2025-04-06 09:06:37,555 - INFO - Epoch [91/200], Train Loss: 0.0144, Val Loss: 0.0079\n",
      "2025-04-06 09:29:43,847 - INFO - Epoch [92/200], Train Loss: 0.0143, Val Loss: 0.0075\n",
      "2025-04-06 09:30:22,259 - INFO - Epoch [93/200], Train Loss: 0.0144, Val Loss: 0.0090\n",
      "2025-04-06 09:31:01,283 - INFO - Epoch [94/200], Train Loss: 0.0143, Val Loss: 0.0065\n",
      "2025-04-06 09:31:42,033 - INFO - Epoch [95/200], Train Loss: 0.0143, Val Loss: 0.0074\n",
      "2025-04-06 09:32:20,704 - INFO - Epoch [96/200], Train Loss: 0.0143, Val Loss: 0.0119\n",
      "2025-04-06 09:32:59,151 - INFO - Epoch [97/200], Train Loss: 0.0143, Val Loss: 0.0086\n",
      "2025-04-06 09:33:37,548 - INFO - Epoch [98/200], Train Loss: 0.0143, Val Loss: 0.0112\n",
      "2025-04-06 09:34:16,341 - INFO - Epoch [99/200], Train Loss: 0.0143, Val Loss: 0.0064\n",
      "2025-04-06 09:34:55,863 - INFO - Epoch [100/200], Train Loss: 0.0144, Val Loss: 0.0090\n",
      "2025-04-06 09:35:35,088 - INFO - Epoch [101/200], Train Loss: 0.0143, Val Loss: 0.0084\n",
      "2025-04-06 09:36:14,252 - INFO - Epoch [102/200], Train Loss: 0.0143, Val Loss: 0.0070\n",
      "2025-04-06 09:36:53,529 - INFO - Epoch [103/200], Train Loss: 0.0143, Val Loss: 0.0098\n",
      "2025-04-06 09:37:34,560 - INFO - Epoch [104/200], Train Loss: 0.0143, Val Loss: 0.0090\n",
      "2025-04-06 09:38:14,359 - INFO - Epoch [105/200], Train Loss: 0.0142, Val Loss: 0.0077\n",
      "2025-04-06 09:38:55,305 - INFO - Epoch [106/200], Train Loss: 0.0143, Val Loss: 0.0081\n",
      "2025-04-06 09:39:38,336 - INFO - Epoch [107/200], Train Loss: 0.0143, Val Loss: 0.0094\n",
      "2025-04-06 09:40:21,373 - INFO - Epoch [108/200], Train Loss: 0.0143, Val Loss: 0.0079\n",
      "2025-04-06 09:41:05,609 - INFO - Epoch [109/200], Train Loss: 0.0143, Val Loss: 0.0117\n",
      "2025-04-06 09:41:51,081 - INFO - Epoch [110/200], Train Loss: 0.0142, Val Loss: 0.0089\n",
      "2025-04-06 09:42:34,989 - INFO - Epoch [111/200], Train Loss: 0.0143, Val Loss: 0.0090\n",
      "2025-04-06 09:43:17,470 - INFO - Epoch [112/200], Train Loss: 0.0142, Val Loss: 0.0078\n",
      "2025-04-06 09:44:00,256 - INFO - Epoch [113/200], Train Loss: 0.0142, Val Loss: 0.0069\n",
      "2025-04-06 09:44:42,920 - INFO - Epoch [114/200], Train Loss: 0.0142, Val Loss: 0.0109\n",
      "2025-04-06 09:45:27,797 - INFO - Epoch [115/200], Train Loss: 0.0142, Val Loss: 0.0078\n",
      "2025-04-06 09:46:11,863 - INFO - Epoch [116/200], Train Loss: 0.0142, Val Loss: 0.0088\n",
      "2025-04-06 09:46:55,065 - INFO - Epoch [117/200], Train Loss: 0.0142, Val Loss: 0.0076\n",
      "2025-04-06 09:47:38,894 - INFO - Epoch [118/200], Train Loss: 0.0142, Val Loss: 0.0075\n",
      "2025-04-06 09:48:23,517 - INFO - Epoch [119/200], Train Loss: 0.0142, Val Loss: 0.0073\n",
      "2025-04-06 09:48:23,518 - INFO - Early stopping triggered at epoch 119\n",
      "2025-04-06 09:48:24,521 - INFO - 训练因子数量为 33 的模型\n",
      "2025-04-06 09:49:03,779 - INFO - Epoch [1/200], Train Loss: 0.9018, Val Loss: 0.3488\n",
      "2025-04-06 09:49:47,776 - INFO - Epoch [2/200], Train Loss: 0.0540, Val Loss: 0.0354\n",
      "2025-04-06 09:50:31,239 - INFO - Epoch [3/200], Train Loss: 0.0192, Val Loss: 0.0209\n",
      "2025-04-06 09:51:14,785 - INFO - Epoch [4/200], Train Loss: 0.0154, Val Loss: 0.0196\n",
      "2025-04-06 09:51:58,871 - INFO - Epoch [5/200], Train Loss: 0.0155, Val Loss: 0.0197\n",
      "2025-04-06 09:52:43,786 - INFO - Epoch [6/200], Train Loss: 0.0155, Val Loss: 0.0188\n",
      "2025-04-06 09:53:28,850 - INFO - Epoch [7/200], Train Loss: 0.0155, Val Loss: 0.0207\n",
      "2025-04-06 09:54:13,329 - INFO - Epoch [8/200], Train Loss: 0.0155, Val Loss: 0.0202\n",
      "2025-04-06 09:54:56,960 - INFO - Epoch [9/200], Train Loss: 0.0155, Val Loss: 0.0187\n",
      "2025-04-06 09:55:41,484 - INFO - Epoch [10/200], Train Loss: 0.0154, Val Loss: 0.0191\n",
      "2025-04-06 09:56:26,334 - INFO - Epoch [11/200], Train Loss: 0.0154, Val Loss: 0.0163\n",
      "2025-04-06 09:57:11,042 - INFO - Epoch [12/200], Train Loss: 0.0152, Val Loss: 0.0196\n",
      "2025-04-06 09:57:55,898 - INFO - Epoch [13/200], Train Loss: 0.0153, Val Loss: 0.0191\n",
      "2025-04-06 09:58:40,034 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0177\n",
      "2025-04-06 09:59:24,447 - INFO - Epoch [15/200], Train Loss: 0.0152, Val Loss: 0.0186\n",
      "2025-04-06 10:00:08,685 - INFO - Epoch [16/200], Train Loss: 0.0152, Val Loss: 0.0189\n",
      "2025-04-06 10:00:53,474 - INFO - Epoch [17/200], Train Loss: 0.0152, Val Loss: 0.0154\n",
      "2025-04-06 10:01:38,489 - INFO - Epoch [18/200], Train Loss: 0.0152, Val Loss: 0.0167\n",
      "2025-04-06 10:02:22,776 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0129\n",
      "2025-04-06 10:03:07,112 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0171\n",
      "2025-04-06 10:03:51,984 - INFO - Epoch [21/200], Train Loss: 0.0151, Val Loss: 0.0180\n",
      "2025-04-06 10:04:36,978 - INFO - Epoch [22/200], Train Loss: 0.0150, Val Loss: 0.0168\n",
      "2025-04-06 10:05:22,046 - INFO - Epoch [23/200], Train Loss: 0.0150, Val Loss: 0.0179\n",
      "2025-04-06 10:06:06,541 - INFO - Epoch [24/200], Train Loss: 0.0150, Val Loss: 0.0159\n",
      "2025-04-06 10:06:51,978 - INFO - Epoch [25/200], Train Loss: 0.0150, Val Loss: 0.0142\n",
      "2025-04-06 10:07:37,340 - INFO - Epoch [26/200], Train Loss: 0.0150, Val Loss: 0.0156\n",
      "2025-04-06 10:08:23,106 - INFO - Epoch [27/200], Train Loss: 0.0150, Val Loss: 0.0147\n",
      "2025-04-06 10:09:08,662 - INFO - Epoch [28/200], Train Loss: 0.0150, Val Loss: 0.0174\n",
      "2025-04-06 10:09:54,002 - INFO - Epoch [29/200], Train Loss: 0.0150, Val Loss: 0.0166\n",
      "2025-04-06 10:10:39,043 - INFO - Epoch [30/200], Train Loss: 0.0150, Val Loss: 0.0143\n",
      "2025-04-06 10:11:24,179 - INFO - Epoch [31/200], Train Loss: 0.0149, Val Loss: 0.0161\n",
      "2025-04-06 10:12:08,753 - INFO - Epoch [32/200], Train Loss: 0.0148, Val Loss: 0.0144\n",
      "2025-04-06 10:12:50,813 - INFO - Epoch [33/200], Train Loss: 0.0148, Val Loss: 0.0145\n",
      "2025-04-06 10:13:31,994 - INFO - Epoch [34/200], Train Loss: 0.0148, Val Loss: 0.0139\n",
      "2025-04-06 10:14:12,581 - INFO - Epoch [35/200], Train Loss: 0.0148, Val Loss: 0.0120\n",
      "2025-04-06 10:14:53,071 - INFO - Epoch [36/200], Train Loss: 0.0148, Val Loss: 0.0157\n",
      "2025-04-06 10:15:33,714 - INFO - Epoch [37/200], Train Loss: 0.0148, Val Loss: 0.0127\n",
      "2025-04-06 10:16:14,606 - INFO - Epoch [38/200], Train Loss: 0.0147, Val Loss: 0.0135\n",
      "2025-04-06 10:16:56,053 - INFO - Epoch [39/200], Train Loss: 0.0148, Val Loss: 0.0114\n",
      "2025-04-06 10:17:37,751 - INFO - Epoch [40/200], Train Loss: 0.0148, Val Loss: 0.0117\n",
      "2025-04-06 10:18:19,355 - INFO - Epoch [41/200], Train Loss: 0.0147, Val Loss: 0.0133\n",
      "2025-04-06 10:19:01,203 - INFO - Epoch [42/200], Train Loss: 0.0147, Val Loss: 0.0132\n",
      "2025-04-06 10:19:42,868 - INFO - Epoch [43/200], Train Loss: 0.0147, Val Loss: 0.0123\n",
      "2025-04-06 10:20:24,684 - INFO - Epoch [44/200], Train Loss: 0.0147, Val Loss: 0.0128\n",
      "2025-04-06 10:21:06,389 - INFO - Epoch [45/200], Train Loss: 0.0147, Val Loss: 0.0129\n",
      "2025-04-06 10:21:48,296 - INFO - Epoch [46/200], Train Loss: 0.0146, Val Loss: 0.0143\n",
      "2025-04-06 10:22:30,332 - INFO - Epoch [47/200], Train Loss: 0.0147, Val Loss: 0.0138\n",
      "2025-04-06 10:23:12,283 - INFO - Epoch [48/200], Train Loss: 0.0147, Val Loss: 0.0132\n",
      "2025-04-06 10:23:54,268 - INFO - Epoch [49/200], Train Loss: 0.0147, Val Loss: 0.0129\n",
      "2025-04-06 10:24:35,973 - INFO - Epoch [50/200], Train Loss: 0.0146, Val Loss: 0.0121\n",
      "2025-04-06 10:25:17,518 - INFO - Epoch [51/200], Train Loss: 0.0146, Val Loss: 0.0121\n",
      "2025-04-06 10:25:59,016 - INFO - Epoch [52/200], Train Loss: 0.0146, Val Loss: 0.0129\n",
      "2025-04-06 10:26:40,708 - INFO - Epoch [53/200], Train Loss: 0.0146, Val Loss: 0.0125\n",
      "2025-04-06 10:27:22,232 - INFO - Epoch [54/200], Train Loss: 0.0146, Val Loss: 0.0113\n",
      "2025-04-06 10:28:03,674 - INFO - Epoch [55/200], Train Loss: 0.0146, Val Loss: 0.0126\n",
      "2025-04-06 10:28:45,097 - INFO - Epoch [56/200], Train Loss: 0.0146, Val Loss: 0.0135\n",
      "2025-04-06 10:29:26,604 - INFO - Epoch [57/200], Train Loss: 0.0146, Val Loss: 0.0113\n",
      "2025-04-06 10:30:08,079 - INFO - Epoch [58/200], Train Loss: 0.0146, Val Loss: 0.0111\n",
      "2025-04-06 10:30:49,478 - INFO - Epoch [59/200], Train Loss: 0.0146, Val Loss: 0.0120\n",
      "2025-04-06 10:31:30,751 - INFO - Epoch [60/200], Train Loss: 0.0146, Val Loss: 0.0116\n",
      "2025-04-06 10:32:12,130 - INFO - Epoch [61/200], Train Loss: 0.0146, Val Loss: 0.0113\n",
      "2025-04-06 10:32:53,598 - INFO - Epoch [62/200], Train Loss: 0.0145, Val Loss: 0.0116\n",
      "2025-04-06 10:33:34,926 - INFO - Epoch [63/200], Train Loss: 0.0145, Val Loss: 0.0103\n",
      "2025-04-06 10:34:16,387 - INFO - Epoch [64/200], Train Loss: 0.0145, Val Loss: 0.0125\n",
      "2025-04-06 10:34:57,912 - INFO - Epoch [65/200], Train Loss: 0.0145, Val Loss: 0.0109\n",
      "2025-04-06 10:35:39,296 - INFO - Epoch [66/200], Train Loss: 0.0145, Val Loss: 0.0134\n",
      "2025-04-06 10:36:20,761 - INFO - Epoch [67/200], Train Loss: 0.0145, Val Loss: 0.0100\n",
      "2025-04-06 10:37:02,111 - INFO - Epoch [68/200], Train Loss: 0.0145, Val Loss: 0.0109\n",
      "2025-04-06 10:37:43,503 - INFO - Epoch [69/200], Train Loss: 0.0145, Val Loss: 0.0088\n",
      "2025-04-06 10:38:24,879 - INFO - Epoch [70/200], Train Loss: 0.0145, Val Loss: 0.0114\n",
      "2025-04-06 10:39:06,376 - INFO - Epoch [71/200], Train Loss: 0.0145, Val Loss: 0.0091\n",
      "2025-04-06 10:39:48,063 - INFO - Epoch [72/200], Train Loss: 0.0145, Val Loss: 0.0094\n",
      "2025-04-06 10:40:29,511 - INFO - Epoch [73/200], Train Loss: 0.0145, Val Loss: 0.0105\n",
      "2025-04-06 10:41:11,086 - INFO - Epoch [74/200], Train Loss: 0.0145, Val Loss: 0.0122\n",
      "2025-04-06 10:41:52,576 - INFO - Epoch [75/200], Train Loss: 0.0145, Val Loss: 0.0112\n",
      "2025-04-06 10:42:34,108 - INFO - Epoch [76/200], Train Loss: 0.0145, Val Loss: 0.0112\n",
      "2025-04-06 10:43:15,661 - INFO - Epoch [77/200], Train Loss: 0.0145, Val Loss: 0.0095\n",
      "2025-04-06 10:43:57,254 - INFO - Epoch [78/200], Train Loss: 0.0145, Val Loss: 0.0105\n",
      "2025-04-06 10:44:38,778 - INFO - Epoch [79/200], Train Loss: 0.0145, Val Loss: 0.0112\n",
      "2025-04-06 10:45:20,210 - INFO - Epoch [80/200], Train Loss: 0.0145, Val Loss: 0.0092\n",
      "2025-04-06 10:46:01,453 - INFO - Epoch [81/200], Train Loss: 0.0145, Val Loss: 0.0103\n",
      "2025-04-06 10:46:42,716 - INFO - Epoch [82/200], Train Loss: 0.0144, Val Loss: 0.0091\n",
      "2025-04-06 10:47:24,194 - INFO - Epoch [83/200], Train Loss: 0.0144, Val Loss: 0.0126\n",
      "2025-04-06 10:48:05,801 - INFO - Epoch [84/200], Train Loss: 0.0144, Val Loss: 0.0114\n",
      "2025-04-06 10:48:47,440 - INFO - Epoch [85/200], Train Loss: 0.0144, Val Loss: 0.0106\n",
      "2025-04-06 10:49:29,095 - INFO - Epoch [86/200], Train Loss: 0.0144, Val Loss: 0.0102\n",
      "2025-04-06 10:50:40,361 - INFO - Epoch [87/200], Train Loss: 0.0144, Val Loss: 0.0089\n",
      "2025-04-06 10:52:05,705 - INFO - Epoch [88/200], Train Loss: 0.0144, Val Loss: 0.0100\n",
      "2025-04-06 10:53:30,808 - INFO - Epoch [89/200], Train Loss: 0.0144, Val Loss: 0.0120\n",
      "2025-04-06 10:53:30,810 - INFO - Early stopping triggered at epoch 89\n",
      "2025-04-06 10:53:33,241 - INFO - 训练因子数量为 34 的模型\n",
      "2025-04-06 10:54:55,799 - INFO - Epoch [1/200], Train Loss: 0.8208, Val Loss: 0.1084\n",
      "2025-04-06 10:56:17,740 - INFO - Epoch [2/200], Train Loss: 0.0395, Val Loss: 0.0395\n",
      "2025-04-06 10:57:43,439 - INFO - Epoch [3/200], Train Loss: 0.0178, Val Loss: 0.0150\n",
      "2025-04-06 10:59:05,907 - INFO - Epoch [4/200], Train Loss: 0.0153, Val Loss: 0.0205\n",
      "2025-04-06 11:00:29,873 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0198\n",
      "2025-04-06 11:01:54,377 - INFO - Epoch [6/200], Train Loss: 0.0155, Val Loss: 0.0202\n",
      "2025-04-06 11:03:18,397 - INFO - Epoch [7/200], Train Loss: 0.0155, Val Loss: 0.0198\n",
      "2025-04-06 11:04:44,462 - INFO - Epoch [8/200], Train Loss: 0.0154, Val Loss: 0.0198\n",
      "2025-04-06 11:59:40,915 - INFO - Epoch [9/200], Train Loss: 0.0154, Val Loss: 0.0197\n",
      "2025-04-06 12:00:22,412 - INFO - Epoch [10/200], Train Loss: 0.0154, Val Loss: 0.0181\n",
      "2025-04-06 12:01:03,036 - INFO - Epoch [11/200], Train Loss: 0.0153, Val Loss: 0.0195\n",
      "2025-04-06 12:01:43,725 - INFO - Epoch [12/200], Train Loss: 0.0152, Val Loss: 0.0165\n",
      "2025-04-06 12:02:24,448 - INFO - Epoch [13/200], Train Loss: 0.0152, Val Loss: 0.0184\n",
      "2025-04-06 12:03:04,487 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0193\n",
      "2025-04-06 12:03:47,603 - INFO - Epoch [15/200], Train Loss: 0.0151, Val Loss: 0.0174\n",
      "2025-04-06 12:04:31,785 - INFO - Epoch [16/200], Train Loss: 0.0151, Val Loss: 0.0184\n",
      "2025-04-06 12:05:14,455 - INFO - Epoch [17/200], Train Loss: 0.0151, Val Loss: 0.0193\n",
      "2025-04-06 12:05:57,572 - INFO - Epoch [18/200], Train Loss: 0.0151, Val Loss: 0.0189\n",
      "2025-04-06 12:06:40,079 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0155\n",
      "2025-04-06 12:07:23,993 - INFO - Epoch [20/200], Train Loss: 0.0150, Val Loss: 0.0176\n",
      "2025-04-06 12:08:04,977 - INFO - Epoch [21/200], Train Loss: 0.0150, Val Loss: 0.0169\n",
      "2025-04-06 12:08:48,178 - INFO - Epoch [22/200], Train Loss: 0.0149, Val Loss: 0.0164\n",
      "2025-04-06 12:09:30,509 - INFO - Epoch [23/200], Train Loss: 0.0149, Val Loss: 0.0165\n",
      "2025-04-06 12:09:30,510 - INFO - Early stopping triggered at epoch 23\n",
      "2025-04-06 12:09:31,584 - INFO - 训练因子数量为 35 的模型\n",
      "2025-04-06 12:10:13,481 - INFO - Epoch [1/200], Train Loss: 1.0839, Val Loss: 0.2684\n",
      "2025-04-06 12:10:57,037 - INFO - Epoch [2/200], Train Loss: 0.0552, Val Loss: 0.1079\n",
      "2025-04-06 12:11:41,857 - INFO - Epoch [3/200], Train Loss: 0.0211, Val Loss: 0.0169\n",
      "2025-04-06 12:12:25,684 - INFO - Epoch [4/200], Train Loss: 0.0154, Val Loss: 0.0207\n",
      "2025-04-06 12:13:07,778 - INFO - Epoch [5/200], Train Loss: 0.0154, Val Loss: 0.0213\n",
      "2025-04-06 12:13:49,431 - INFO - Epoch [6/200], Train Loss: 0.0154, Val Loss: 0.0203\n",
      "2025-04-06 12:14:31,343 - INFO - Epoch [7/200], Train Loss: 0.0154, Val Loss: 0.0197\n",
      "2025-04-06 12:15:14,042 - INFO - Epoch [8/200], Train Loss: 0.0154, Val Loss: 0.0209\n",
      "2025-04-06 12:15:57,689 - INFO - Epoch [9/200], Train Loss: 0.0154, Val Loss: 0.0170\n",
      "2025-04-06 12:16:39,970 - INFO - Epoch [10/200], Train Loss: 0.0154, Val Loss: 0.0191\n",
      "2025-04-06 12:17:23,028 - INFO - Epoch [11/200], Train Loss: 0.0154, Val Loss: 0.0190\n",
      "2025-04-06 12:18:05,003 - INFO - Epoch [12/200], Train Loss: 0.0153, Val Loss: 0.0188\n",
      "2025-04-06 12:18:47,453 - INFO - Epoch [13/200], Train Loss: 0.0153, Val Loss: 0.0184\n",
      "2025-04-06 12:19:30,079 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0189\n",
      "2025-04-06 12:20:13,448 - INFO - Epoch [15/200], Train Loss: 0.0152, Val Loss: 0.0178\n",
      "2025-04-06 12:20:56,130 - INFO - Epoch [16/200], Train Loss: 0.0152, Val Loss: 0.0191\n",
      "2025-04-06 12:21:37,749 - INFO - Epoch [17/200], Train Loss: 0.0152, Val Loss: 0.0188\n",
      "2025-04-06 12:22:19,651 - INFO - Epoch [18/200], Train Loss: 0.0151, Val Loss: 0.0168\n",
      "2025-04-06 12:23:01,139 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0192\n",
      "2025-04-06 12:23:42,709 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0158\n",
      "2025-04-06 12:24:24,479 - INFO - Epoch [21/200], Train Loss: 0.0151, Val Loss: 0.0163\n",
      "2025-04-06 12:25:06,122 - INFO - Epoch [22/200], Train Loss: 0.0150, Val Loss: 0.0172\n",
      "2025-04-06 12:25:47,835 - INFO - Epoch [23/200], Train Loss: 0.0150, Val Loss: 0.0172\n",
      "2025-04-06 12:26:29,557 - INFO - Epoch [24/200], Train Loss: 0.0150, Val Loss: 0.0166\n",
      "2025-04-06 12:27:11,293 - INFO - Epoch [25/200], Train Loss: 0.0150, Val Loss: 0.0171\n",
      "2025-04-06 12:27:53,227 - INFO - Epoch [26/200], Train Loss: 0.0150, Val Loss: 0.0158\n",
      "2025-04-06 12:28:34,887 - INFO - Epoch [27/200], Train Loss: 0.0149, Val Loss: 0.0135\n",
      "2025-04-06 12:29:16,643 - INFO - Epoch [28/200], Train Loss: 0.0149, Val Loss: 0.0152\n",
      "2025-04-06 12:29:58,306 - INFO - Epoch [29/200], Train Loss: 0.0149, Val Loss: 0.0151\n",
      "2025-04-06 12:30:37,762 - INFO - Epoch [30/200], Train Loss: 0.0149, Val Loss: 0.0163\n",
      "2025-04-06 12:31:19,619 - INFO - Epoch [31/200], Train Loss: 0.0149, Val Loss: 0.0152\n",
      "2025-04-06 12:32:01,412 - INFO - Epoch [32/200], Train Loss: 0.0148, Val Loss: 0.0138\n",
      "2025-04-06 12:32:43,323 - INFO - Epoch [33/200], Train Loss: 0.0148, Val Loss: 0.0142\n",
      "2025-04-06 12:33:25,063 - INFO - Epoch [34/200], Train Loss: 0.0148, Val Loss: 0.0127\n",
      "2025-04-06 12:34:06,652 - INFO - Epoch [35/200], Train Loss: 0.0148, Val Loss: 0.0142\n",
      "2025-04-06 12:34:48,463 - INFO - Epoch [36/200], Train Loss: 0.0148, Val Loss: 0.0134\n",
      "2025-04-06 12:35:30,187 - INFO - Epoch [37/200], Train Loss: 0.0148, Val Loss: 0.0158\n",
      "2025-04-06 12:36:12,135 - INFO - Epoch [38/200], Train Loss: 0.0148, Val Loss: 0.0143\n",
      "2025-04-06 12:36:53,874 - INFO - Epoch [39/200], Train Loss: 0.0148, Val Loss: 0.0152\n",
      "2025-04-06 12:37:35,700 - INFO - Epoch [40/200], Train Loss: 0.0148, Val Loss: 0.0162\n",
      "2025-04-06 12:38:17,380 - INFO - Epoch [41/200], Train Loss: 0.0148, Val Loss: 0.0139\n",
      "2025-04-06 12:38:58,988 - INFO - Epoch [42/200], Train Loss: 0.0147, Val Loss: 0.0136\n",
      "2025-04-06 12:39:40,495 - INFO - Epoch [43/200], Train Loss: 0.0147, Val Loss: 0.0139\n",
      "2025-04-06 12:40:22,175 - INFO - Epoch [44/200], Train Loss: 0.0147, Val Loss: 0.0144\n",
      "2025-04-06 12:41:03,829 - INFO - Epoch [45/200], Train Loss: 0.0147, Val Loss: 0.0135\n",
      "2025-04-06 12:41:45,515 - INFO - Epoch [46/200], Train Loss: 0.0147, Val Loss: 0.0148\n",
      "2025-04-06 12:42:27,157 - INFO - Epoch [47/200], Train Loss: 0.0147, Val Loss: 0.0118\n",
      "2025-04-06 12:43:08,694 - INFO - Epoch [48/200], Train Loss: 0.0147, Val Loss: 0.0118\n",
      "2025-04-06 12:43:50,446 - INFO - Epoch [49/200], Train Loss: 0.0146, Val Loss: 0.0126\n",
      "2025-04-06 12:44:32,075 - INFO - Epoch [50/200], Train Loss: 0.0147, Val Loss: 0.0132\n",
      "2025-04-06 12:45:13,658 - INFO - Epoch [51/200], Train Loss: 0.0147, Val Loss: 0.0091\n",
      "2025-04-06 12:45:57,427 - INFO - Epoch [52/200], Train Loss: 0.0146, Val Loss: 0.0120\n",
      "2025-04-06 12:46:38,900 - INFO - Epoch [53/200], Train Loss: 0.0146, Val Loss: 0.0120\n",
      "2025-04-06 12:47:20,438 - INFO - Epoch [54/200], Train Loss: 0.0146, Val Loss: 0.0125\n",
      "2025-04-06 12:48:01,988 - INFO - Epoch [55/200], Train Loss: 0.0146, Val Loss: 0.0134\n",
      "2025-04-06 12:48:43,558 - INFO - Epoch [56/200], Train Loss: 0.0146, Val Loss: 0.0140\n",
      "2025-04-06 12:49:25,071 - INFO - Epoch [57/200], Train Loss: 0.0146, Val Loss: 0.0124\n",
      "2025-04-06 12:50:06,709 - INFO - Epoch [58/200], Train Loss: 0.0146, Val Loss: 0.0106\n",
      "2025-04-06 12:50:48,337 - INFO - Epoch [59/200], Train Loss: 0.0146, Val Loss: 0.0107\n",
      "2025-04-06 12:51:28,538 - INFO - Epoch [60/200], Train Loss: 0.0146, Val Loss: 0.0127\n",
      "2025-04-06 12:52:10,499 - INFO - Epoch [61/200], Train Loss: 0.0146, Val Loss: 0.0133\n",
      "2025-04-06 12:52:53,555 - INFO - Epoch [62/200], Train Loss: 0.0145, Val Loss: 0.0088\n",
      "2025-04-06 12:53:36,433 - INFO - Epoch [63/200], Train Loss: 0.0145, Val Loss: 0.0135\n",
      "2025-04-06 12:54:20,260 - INFO - Epoch [64/200], Train Loss: 0.0145, Val Loss: 0.0085\n",
      "2025-04-06 12:55:02,784 - INFO - Epoch [65/200], Train Loss: 0.0145, Val Loss: 0.0107\n",
      "2025-04-06 12:55:43,798 - INFO - Epoch [66/200], Train Loss: 0.0145, Val Loss: 0.0114\n",
      "2025-04-06 12:56:25,984 - INFO - Epoch [67/200], Train Loss: 0.0145, Val Loss: 0.0120\n",
      "2025-04-06 12:57:08,247 - INFO - Epoch [68/200], Train Loss: 0.0145, Val Loss: 0.0108\n",
      "2025-04-06 12:57:49,133 - INFO - Epoch [69/200], Train Loss: 0.0145, Val Loss: 0.0102\n",
      "2025-04-06 12:58:32,144 - INFO - Epoch [70/200], Train Loss: 0.0145, Val Loss: 0.0100\n",
      "2025-04-06 12:59:14,382 - INFO - Epoch [71/200], Train Loss: 0.0145, Val Loss: 0.0119\n",
      "2025-04-06 12:59:55,882 - INFO - Epoch [72/200], Train Loss: 0.0145, Val Loss: 0.0117\n",
      "2025-04-06 13:00:36,595 - INFO - Epoch [73/200], Train Loss: 0.0145, Val Loss: 0.0134\n",
      "2025-04-06 13:01:17,233 - INFO - Epoch [74/200], Train Loss: 0.0145, Val Loss: 0.0085\n",
      "2025-04-06 13:01:58,464 - INFO - Epoch [75/200], Train Loss: 0.0145, Val Loss: 0.0092\n",
      "2025-04-06 13:02:45,574 - INFO - Epoch [76/200], Train Loss: 0.0145, Val Loss: 0.0107\n",
      "2025-04-06 13:03:30,469 - INFO - Epoch [77/200], Train Loss: 0.0145, Val Loss: 0.0082\n",
      "2025-04-06 13:04:14,598 - INFO - Epoch [78/200], Train Loss: 0.0145, Val Loss: 0.0108\n",
      "2025-04-06 13:04:56,656 - INFO - Epoch [79/200], Train Loss: 0.0145, Val Loss: 0.0104\n",
      "2025-04-06 13:05:38,474 - INFO - Epoch [80/200], Train Loss: 0.0145, Val Loss: 0.0111\n",
      "2025-04-06 13:06:20,707 - INFO - Epoch [81/200], Train Loss: 0.0144, Val Loss: 0.0124\n",
      "2025-04-06 13:07:02,674 - INFO - Epoch [82/200], Train Loss: 0.0144, Val Loss: 0.0099\n",
      "2025-04-06 13:07:44,273 - INFO - Epoch [83/200], Train Loss: 0.0144, Val Loss: 0.0104\n",
      "2025-04-06 13:08:25,152 - INFO - Epoch [84/200], Train Loss: 0.0144, Val Loss: 0.0098\n",
      "2025-04-06 13:09:05,947 - INFO - Epoch [85/200], Train Loss: 0.0144, Val Loss: 0.0099\n",
      "2025-04-06 13:09:46,443 - INFO - Epoch [86/200], Train Loss: 0.0144, Val Loss: 0.0121\n",
      "2025-04-06 13:10:27,400 - INFO - Epoch [87/200], Train Loss: 0.0144, Val Loss: 0.0081\n",
      "2025-04-06 13:11:08,360 - INFO - Epoch [88/200], Train Loss: 0.0144, Val Loss: 0.0093\n",
      "2025-04-06 13:11:49,999 - INFO - Epoch [89/200], Train Loss: 0.0144, Val Loss: 0.0107\n",
      "2025-04-06 13:12:31,347 - INFO - Epoch [90/200], Train Loss: 0.0144, Val Loss: 0.0099\n",
      "2025-04-06 13:13:16,793 - INFO - Epoch [91/200], Train Loss: 0.0144, Val Loss: 0.0116\n",
      "2025-04-06 13:13:58,392 - INFO - Epoch [92/200], Train Loss: 0.0143, Val Loss: 0.0082\n",
      "2025-04-06 13:14:39,801 - INFO - Epoch [93/200], Train Loss: 0.0143, Val Loss: 0.0089\n",
      "2025-04-06 13:15:22,041 - INFO - Epoch [94/200], Train Loss: 0.0143, Val Loss: 0.0095\n",
      "2025-04-06 13:16:04,548 - INFO - Epoch [95/200], Train Loss: 0.0143, Val Loss: 0.0078\n",
      "2025-04-06 13:16:45,595 - INFO - Epoch [96/200], Train Loss: 0.0143, Val Loss: 0.0116\n",
      "2025-04-06 13:17:27,136 - INFO - Epoch [97/200], Train Loss: 0.0143, Val Loss: 0.0107\n",
      "2025-04-06 13:18:09,475 - INFO - Epoch [98/200], Train Loss: 0.0143, Val Loss: 0.0075\n",
      "2025-04-06 13:18:51,818 - INFO - Epoch [99/200], Train Loss: 0.0143, Val Loss: 0.0087\n",
      "2025-04-06 13:19:32,635 - INFO - Epoch [100/200], Train Loss: 0.0143, Val Loss: 0.0084\n",
      "2025-04-06 13:20:13,274 - INFO - Epoch [101/200], Train Loss: 0.0143, Val Loss: 0.0089\n",
      "2025-04-06 13:20:53,706 - INFO - Epoch [102/200], Train Loss: 0.0143, Val Loss: 0.0082\n",
      "2025-04-06 13:21:34,110 - INFO - Epoch [103/200], Train Loss: 0.0143, Val Loss: 0.0102\n",
      "2025-04-06 13:22:14,278 - INFO - Epoch [104/200], Train Loss: 0.0143, Val Loss: 0.0079\n",
      "2025-04-06 13:22:54,430 - INFO - Epoch [105/200], Train Loss: 0.0142, Val Loss: 0.0089\n",
      "2025-04-06 13:23:33,983 - INFO - Epoch [106/200], Train Loss: 0.0143, Val Loss: 0.0083\n",
      "2025-04-06 13:24:13,527 - INFO - Epoch [107/200], Train Loss: 0.0142, Val Loss: 0.0070\n",
      "2025-04-06 13:24:52,990 - INFO - Epoch [108/200], Train Loss: 0.0142, Val Loss: 0.0077\n",
      "2025-04-06 13:25:32,564 - INFO - Epoch [109/200], Train Loss: 0.0143, Val Loss: 0.0072\n",
      "2025-04-06 13:26:12,105 - INFO - Epoch [110/200], Train Loss: 0.0143, Val Loss: 0.0082\n",
      "2025-04-06 13:26:51,587 - INFO - Epoch [111/200], Train Loss: 0.0142, Val Loss: 0.0093\n",
      "2025-04-06 13:27:30,829 - INFO - Epoch [112/200], Train Loss: 0.0142, Val Loss: 0.0078\n",
      "2025-04-06 13:28:10,311 - INFO - Epoch [113/200], Train Loss: 0.0142, Val Loss: 0.0117\n",
      "2025-04-06 13:28:49,571 - INFO - Epoch [114/200], Train Loss: 0.0142, Val Loss: 0.0082\n",
      "2025-04-06 13:29:28,844 - INFO - Epoch [115/200], Train Loss: 0.0142, Val Loss: 0.0075\n",
      "2025-04-06 13:30:07,996 - INFO - Epoch [116/200], Train Loss: 0.0142, Val Loss: 0.0073\n",
      "2025-04-06 13:30:47,134 - INFO - Epoch [117/200], Train Loss: 0.0142, Val Loss: 0.0095\n",
      "2025-04-06 13:31:26,207 - INFO - Epoch [118/200], Train Loss: 0.0142, Val Loss: 0.0066\n",
      "2025-04-06 13:32:05,226 - INFO - Epoch [119/200], Train Loss: 0.0142, Val Loss: 0.0100\n",
      "2025-04-06 13:32:44,280 - INFO - Epoch [120/200], Train Loss: 0.0142, Val Loss: 0.0123\n",
      "2025-04-06 13:33:23,228 - INFO - Epoch [121/200], Train Loss: 0.0142, Val Loss: 0.0097\n",
      "2025-04-06 13:34:02,203 - INFO - Epoch [122/200], Train Loss: 0.0142, Val Loss: 0.0069\n",
      "2025-04-06 13:34:41,188 - INFO - Epoch [123/200], Train Loss: 0.0141, Val Loss: 0.0088\n",
      "2025-04-06 13:35:20,224 - INFO - Epoch [124/200], Train Loss: 0.0142, Val Loss: 0.0105\n",
      "2025-04-06 13:35:59,074 - INFO - Epoch [125/200], Train Loss: 0.0142, Val Loss: 0.0059\n",
      "2025-04-06 13:36:37,962 - INFO - Epoch [126/200], Train Loss: 0.0142, Val Loss: 0.0059\n",
      "2025-04-06 13:37:16,820 - INFO - Epoch [127/200], Train Loss: 0.0142, Val Loss: 0.0099\n",
      "2025-04-06 13:37:55,759 - INFO - Epoch [128/200], Train Loss: 0.0142, Val Loss: 0.0082\n",
      "2025-04-06 13:38:34,695 - INFO - Epoch [129/200], Train Loss: 0.0142, Val Loss: 0.0079\n",
      "2025-04-06 13:39:13,589 - INFO - Epoch [130/200], Train Loss: 0.0142, Val Loss: 0.0095\n",
      "2025-04-06 13:39:52,527 - INFO - Epoch [131/200], Train Loss: 0.0142, Val Loss: 0.0077\n",
      "2025-04-06 13:40:31,389 - INFO - Epoch [132/200], Train Loss: 0.0141, Val Loss: 0.0064\n",
      "2025-04-06 13:41:10,332 - INFO - Epoch [133/200], Train Loss: 0.0141, Val Loss: 0.0075\n",
      "2025-04-06 13:41:49,199 - INFO - Epoch [134/200], Train Loss: 0.0141, Val Loss: 0.0046\n",
      "2025-04-06 13:42:27,988 - INFO - Epoch [135/200], Train Loss: 0.0141, Val Loss: 0.0100\n",
      "2025-04-06 13:43:06,709 - INFO - Epoch [136/200], Train Loss: 0.0141, Val Loss: 0.0079\n",
      "2025-04-06 13:43:45,376 - INFO - Epoch [137/200], Train Loss: 0.0141, Val Loss: 0.0066\n",
      "2025-04-06 13:44:24,226 - INFO - Epoch [138/200], Train Loss: 0.0141, Val Loss: 0.0083\n",
      "2025-04-06 13:45:02,984 - INFO - Epoch [139/200], Train Loss: 0.0141, Val Loss: 0.0078\n",
      "2025-04-06 13:45:41,722 - INFO - Epoch [140/200], Train Loss: 0.0141, Val Loss: 0.0055\n",
      "2025-04-06 13:46:20,599 - INFO - Epoch [141/200], Train Loss: 0.0141, Val Loss: 0.0073\n",
      "2025-04-06 13:46:59,319 - INFO - Epoch [142/200], Train Loss: 0.0141, Val Loss: 0.0062\n",
      "2025-04-06 13:47:38,537 - INFO - Epoch [143/200], Train Loss: 0.0141, Val Loss: 0.0078\n",
      "2025-04-06 13:48:18,066 - INFO - Epoch [144/200], Train Loss: 0.0141, Val Loss: 0.0078\n",
      "2025-04-06 13:48:57,678 - INFO - Epoch [145/200], Train Loss: 0.0141, Val Loss: 0.0088\n",
      "2025-04-06 13:49:37,314 - INFO - Epoch [146/200], Train Loss: 0.0141, Val Loss: 0.0066\n",
      "2025-04-06 13:50:17,303 - INFO - Epoch [147/200], Train Loss: 0.0141, Val Loss: 0.0073\n",
      "2025-04-06 13:50:57,214 - INFO - Epoch [148/200], Train Loss: 0.0141, Val Loss: 0.0072\n",
      "2025-04-06 13:51:37,426 - INFO - Epoch [149/200], Train Loss: 0.0141, Val Loss: 0.0047\n",
      "2025-04-06 13:52:17,635 - INFO - Epoch [150/200], Train Loss: 0.0141, Val Loss: 0.0062\n",
      "2025-04-06 13:52:58,074 - INFO - Epoch [151/200], Train Loss: 0.0141, Val Loss: 0.0057\n",
      "2025-04-06 13:53:38,679 - INFO - Epoch [152/200], Train Loss: 0.0140, Val Loss: 0.0087\n",
      "2025-04-06 13:54:19,393 - INFO - Epoch [153/200], Train Loss: 0.0140, Val Loss: 0.0101\n",
      "2025-04-06 13:55:00,086 - INFO - Epoch [154/200], Train Loss: 0.0140, Val Loss: 0.0078\n",
      "2025-04-06 13:55:00,087 - INFO - Early stopping triggered at epoch 154\n",
      "2025-04-06 13:55:01,098 - INFO - 不同因子数量的测试结果已保存到 'factor_count_test_results_20250406_135501.csv'\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 20540 (\\N{CJK UNIFIED IDEOGRAPH-503C}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 19981 (\\N{CJK UNIFIED IDEOGRAPH-4E0D}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 21516 (\\N{CJK UNIFIED IDEOGRAPH-540C}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 22240 (\\N{CJK UNIFIED IDEOGRAPH-56E0}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 23376 (\\N{CJK UNIFIED IDEOGRAPH-5B50}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 26679 (\\N{CJK UNIFIED IDEOGRAPH-6837}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 26412 (\\N{CJK UNIFIED IDEOGRAPH-672C}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:237: UserWarning: Glyph 22806 (\\N{CJK UNIFIED IDEOGRAPH-5916}) missing from current font.\n",
      "  plt.savefig(factor_count_plot)\n",
      "2025-04-06 13:55:01,257 - INFO - 不同因子数量的R²图表已保存到 'factor_count_r_squared_plot_20250406_135501.png'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不同因子数量的样本外R方测试完成\n",
      "\n",
      "计算30个因子与原始特征的相关性矩阵...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 13:55:08,685 - INFO - 因子与原始特征的相关性矩阵已保存到 'factor_feature_correlation_20250406_135508.csv'\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 22240 (\\N{CJK UNIFIED IDEOGRAPH-56E0}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 23376 (\\N{CJK UNIFIED IDEOGRAPH-5B50}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 19982 (\\N{CJK UNIFIED IDEOGRAPH-4E0E}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 22987 (\\N{CJK UNIFIED IDEOGRAPH-59CB}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 29305 (\\N{CJK UNIFIED IDEOGRAPH-7279}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 24449 (\\N{CJK UNIFIED IDEOGRAPH-5F81}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 30456 (\\N{CJK UNIFIED IDEOGRAPH-76F8}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 20851 (\\N{CJK UNIFIED IDEOGRAPH-5173}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 24615 (\\N{CJK UNIFIED IDEOGRAPH-6027}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 30697 (\\N{CJK UNIFIED IDEOGRAPH-77E9}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:272: UserWarning: Glyph 38453 (\\N{CJK UNIFIED IDEOGRAPH-9635}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 22240 (\\N{CJK UNIFIED IDEOGRAPH-56E0}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 23376 (\\N{CJK UNIFIED IDEOGRAPH-5B50}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 19982 (\\N{CJK UNIFIED IDEOGRAPH-4E0E}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 22987 (\\N{CJK UNIFIED IDEOGRAPH-59CB}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 29305 (\\N{CJK UNIFIED IDEOGRAPH-7279}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 24449 (\\N{CJK UNIFIED IDEOGRAPH-5F81}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 30456 (\\N{CJK UNIFIED IDEOGRAPH-76F8}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 20851 (\\N{CJK UNIFIED IDEOGRAPH-5173}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 24615 (\\N{CJK UNIFIED IDEOGRAPH-6027}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 30697 (\\N{CJK UNIFIED IDEOGRAPH-77E9}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "/var/folders/cl/wbfw5l6x06qggs_8v4x0__th0000gn/T/ipykernel_53721/1599212766.py:274: UserWarning: Glyph 38453 (\\N{CJK UNIFIED IDEOGRAPH-9635}) missing from current font.\n",
      "  plt.savefig(corr_plot)\n",
      "2025-04-06 13:55:09,063 - INFO - 相关性热力图已保存到 'factor_feature_correlation_heatmap_20250406_135508.png'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相关性矩阵计算完成并保存\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 1. 数据准备\n",
    "def load_and_preprocess_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logging.info(f\"Successfully loaded data from {file_path}\")\n",
    "        \n",
    "        # 保存原始特征数据，用于后续计算相关性矩阵\n",
    "        original_features = data.iloc[:, 3:36].values\n",
    "        feature_names = data.columns[3:36].tolist()\n",
    "        \n",
    "        Z = data.iloc[:, 3:36].values  \n",
    "        # 特征扩展：两两交互\n",
    "        expanded_features = []\n",
    "        for i, j in combinations(range(Z.shape[1]), 2):\n",
    "            expanded_features.append(Z[:, i] * Z[:, j])\n",
    "        \n",
    "        Z_expanded = np.column_stack([Z] + expanded_features)\n",
    "        logging.info(f\"Expanded features from {Z.shape[1]} to {Z_expanded.shape[1]}\")\n",
    "\n",
    "        r = data.iloc[:, 2].values  # 收益率，对应r_t\n",
    "\n",
    "        # 使用均值填充NaN\n",
    "        Z_expanded = np.nan_to_num(Z_expanded, nan=np.nanmean(Z_expanded))\n",
    "        r = np.nan_to_num(r, nan=np.nanmean(r))\n",
    "        logging.info(\"Filled NaN values with mean\")\n",
    "\n",
    "        # 使用StandardScaler来标准化特征\n",
    "        scaler = StandardScaler()\n",
    "        Z_scaled = scaler.fit_transform(Z_expanded)\n",
    "\n",
    "        Z = Z_scaled.astype(np.float32)\n",
    "        r = r.astype(np.float32)\n",
    "\n",
    "        return Z, r, original_features, feature_names\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading or preprocessing data: {e}\")\n",
    "        raise\n",
    "\n",
    "# 2. 模型设计\n",
    "class ConditionalAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout_rate=0.2):\n",
    "        super(ConditionalAutoencoder, self).__init__()\n",
    "        \n",
    "        # 深度学习自解码因子荷载神经网络 B(Z_{t-1})\n",
    "        self.beta_net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(hidden_dim, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(hidden_dim // 2, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(latent_dim, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        ])\n",
    "        \n",
    "        # 深度学习自解码的因子提取网络 f_t\n",
    "        self.factor_net = tf.keras.layers.Dense(latent_dim, use_bias=False, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        Z = inputs\n",
    "        beta = self.beta_net(Z, training=training)  # B(Z_{t-1})\n",
    "        f = self.factor_net(tf.ones((tf.shape(Z)[0], 1)))  # f_t\n",
    "        return tf.reduce_sum(beta * f, axis=1)  # r_t = B(Z_{t-1})f_t\n",
    "\n",
    "# 3. 实验设计\n",
    "def prepare_data(Z, r, test_size=0.2, val_size=0.2):\n",
    "    Z_train_val, Z_test, r_train_val, r_test = train_test_split(Z, r, test_size=test_size, random_state=42)\n",
    "    Z_train, Z_val, r_train, r_val = train_test_split(Z_train_val, r_train_val, test_size=val_size, random_state=42)\n",
    "    return (Z_train, r_train), (Z_val, r_val), (Z_test, r_test)\n",
    "\n",
    "def train_model(model, train_data, val_data, num_epochs=200, batch_size=128, patience=20):\n",
    "    Z_train, r_train = train_data\n",
    "    Z_val, r_val = val_data\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = tf.keras.metrics.Mean()\n",
    "        for i in range(0, len(Z_train), batch_size):\n",
    "            batch_Z = Z_train[i:i+batch_size]\n",
    "            batch_r = r_train[i:i+batch_size]\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_Z, training=True)\n",
    "                loss = tf.reduce_mean(tf.square(batch_r - predictions))\n",
    "                # 添加L2正则化损失\n",
    "                l2_loss = sum(tf.nn.l2_loss(v) for v in model.trainable_variables if 'kernel' in v.name)\n",
    "                total_loss = loss + 0.01 * l2_loss\n",
    "            gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            train_loss.update_state(loss)\n",
    "\n",
    "        # Validation\n",
    "        val_predictions = model(Z_val, training=False)\n",
    "        val_loss = tf.reduce_mean(tf.square(r_val - val_predictions))\n",
    "\n",
    "        train_losses.append(train_loss.result().numpy())\n",
    "        val_losses.append(val_loss.numpy())\n",
    "\n",
    "        logging.info(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.result():.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # 早停\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                logging.info(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # 学习率调整\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            optimizer.learning_rate = optimizer.learning_rate * 0.9\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 提取共同因子的函数\n",
    "def extract_common_factors(model, Z):\n",
    "    beta = model.beta_net(Z).numpy()\n",
    "    factor_weights = model.factor_net.weights[0].numpy()\n",
    "    common_factors = np.einsum('ij,kj->ik', beta, factor_weights.T)\n",
    "    return common_factors\n",
    "\n",
    "# 时间序列交叉验证\n",
    "def time_series_cv(Z, r, n_splits=3):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_scores = []\n",
    "    cv_results = []\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(Z)):\n",
    "        Z_train, Z_test = Z[train_index], Z[test_index]\n",
    "        r_train, r_test = r[train_index], r[test_index]\n",
    "        \n",
    "        model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=30)\n",
    "        train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_test, r_test))\n",
    "        \n",
    "        # 评估模型\n",
    "        common_factors = extract_common_factors(model, Z_test)\n",
    "        scaler = StandardScaler()\n",
    "        common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "        X = sm.add_constant(common_factors_scaled)\n",
    "        y = r_test\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        results = ols_model.fit()\n",
    "        \n",
    "        cv_scores.append(results.rsquared_adj)\n",
    "        cv_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'r_squared': results.rsquared,\n",
    "            'adj_r_squared': results.rsquared_adj,\n",
    "            'aic': results.aic,\n",
    "            'bic': results.bic\n",
    "        })\n",
    "    \n",
    "    # 保存时间序列交叉验证结果到本地\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    cv_file_path = f'time_series_cv_results_{timestamp}.csv'\n",
    "    cv_df.to_csv(cv_file_path, index=False)\n",
    "    logging.info(f\"时间序列交叉验证结果已保存到 '{cv_file_path}'\")\n",
    "    \n",
    "    return cv_scores, cv_results\n",
    "\n",
    "# 不同因子数量的样本外测试\n",
    "def factor_count_test(Z, r, min_factors=25, max_factors=35, train_ratio=0.6, val_ratio=0.2):\n",
    "    n = len(Z)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    Z_train, r_train = Z[:train_end], r[:train_end]\n",
    "    Z_val, r_val = Z[train_end:val_end], r[train_end:val_end]\n",
    "    Z_test, r_test = Z[val_end:], r[val_end:]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for latent_dim in range(min_factors, max_factors + 1):\n",
    "        logging.info(f\"训练因子数量为 {latent_dim} 的模型\")\n",
    "        model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=latent_dim)\n",
    "        train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_val, r_val))\n",
    "        \n",
    "        # 在样本外数据上评估模型\n",
    "        common_factors = extract_common_factors(model, Z_test)\n",
    "        scaler = StandardScaler()\n",
    "        common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "        X = sm.add_constant(common_factors_scaled)\n",
    "        y = r_test\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        ols_results = ols_model.fit()\n",
    "        \n",
    "        results.append({\n",
    "            'factor_count': latent_dim,\n",
    "            'r_squared': ols_results.rsquared,\n",
    "            'adj_r_squared': ols_results.rsquared_adj,\n",
    "            'aic': ols_results.aic,\n",
    "            'bic': ols_results.bic\n",
    "        })\n",
    "    \n",
    "    # 保存不同因子数量的结果到本地\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    factor_count_df = pd.DataFrame(results)\n",
    "    factor_count_file = f'factor_count_test_results_{timestamp}.csv'\n",
    "    factor_count_df.to_csv(factor_count_file, index=False)\n",
    "    logging.info(f\"不同因子数量的测试结果已保存到 '{factor_count_file}'\")\n",
    "    \n",
    "    # 绘制不同因子数量的R方图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(factor_count_df['factor_count'], factor_count_df['r_squared'], marker='o', label='R²')\n",
    "    plt.plot(factor_count_df['factor_count'], factor_count_df['adj_r_squared'], marker='s', label='Adjusted R²')\n",
    "    plt.xlabel('因子数量')\n",
    "    plt.ylabel('R²值')\n",
    "    plt.title('不同因子数量的样本外R²')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    factor_count_plot = f'factor_count_r_squared_plot_{timestamp}.png'\n",
    "    plt.savefig(factor_count_plot)\n",
    "    plt.close()\n",
    "    logging.info(f\"不同因子数量的R²图表已保存到 '{factor_count_plot}'\")\n",
    "    \n",
    "    return factor_count_df\n",
    "\n",
    "\n",
    "# 样本外测试\n",
    "def out_of_sample_test(Z, r, train_ratio=0.6, val_ratio=0.2):\n",
    "    n = len(Z)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    Z_train, r_train = Z[:train_end], r[:train_end]\n",
    "    Z_val, r_val = Z[train_end:val_end], r[train_end:val_end]\n",
    "    Z_test, r_test = Z[val_end:], r[val_end:]\n",
    "    \n",
    "    model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=30)\n",
    "    train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_val, r_val))\n",
    "    \n",
    "    # 在样本外数据上评估模型\n",
    "    common_factors = extract_common_factors(model, Z_test)\n",
    "    scaler = StandardScaler()\n",
    "    common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "    X = sm.add_constant(common_factors_scaled)\n",
    "    y = r_test\n",
    "    ols_model = sm.OLS(y, X)\n",
    "    results = ols_model.fit()\n",
    "    \n",
    "    return results, model, Z_test\n",
    "\n",
    "# 可视化函数\n",
    "def plot_cv_results(cv_results):\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(data=df[['r_squared', 'adj_r_squared']])\n",
    "    plt.title('Cross-Validation Results: R-squared and Adjusted R-squared')\n",
    "    plt.savefig('cv_results_boxplot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data=df, x='fold', y='adj_r_squared')\n",
    "    plt.title('Adjusted R-squared across CV Folds')\n",
    "    plt.savefig('cv_results_scatter.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_oos_results(oos_results):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(oos_results.fittedvalues, oos_results.resid)\n",
    "    plt.xlabel('Fitted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Out-of-Sample Test: Residuals vs Fitted')\n",
    "    plt.savefig('oos_residuals_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sm.graphics.plot_regress_exog(oos_results, 'x1', fig=plt.gcf())\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('oos_regress_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    try:\n",
    "        # 创建输出目录\n",
    "        output_dir = 'model_results_' + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.chdir(output_dir)\n",
    "        logging.info(f\"创建输出目录: {output_dir}\")\n",
    "        \n",
    "        # 文件路径与数据导入\n",
    "        file_path = '/Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/DL_Data8.csv'\n",
    "        Z, r, original_features, feature_names = load_and_preprocess_data(file_path)\n",
    "\n",
    "        # 原始模型训练和评估\n",
    "        train_data, val_data, test_data = prepare_data(Z, r)\n",
    "        Z_train, r_train = train_data\n",
    "        Z_val, r_val = val_data\n",
    "        Z_test, r_test = test_data\n",
    "\n",
    "        model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=30)\n",
    "        train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_val, r_val))\n",
    "\n",
    "        # 提取测试集的共同因子\n",
    "        common_factors = extract_common_factors(model, Z_test)\n",
    "        scaler = StandardScaler()\n",
    "        common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "\n",
    "        # 准备回归数据\n",
    "        X = sm.add_constant(common_factors_scaled)\n",
    "        y = r_test\n",
    "\n",
    "        # 进行OLS回归\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        results = ols_model.fit()\n",
    "\n",
    "        # 输出回归结果\n",
    "        print(results.summary())\n",
    "\n",
    "        # 保存共同因子数据\n",
    "        factor_df = pd.DataFrame(common_factors_scaled, columns=[f'Factor_{i+1}' for i in range(common_factors_scaled.shape[1])])\n",
    "        factor_df['Return'] = r_test\n",
    "        factor_df.to_csv('common_factors_and_returns.csv', index=False)\n",
    "\n",
    "        print(\"共同因子数据已保存到 'common_factors_and_returns.csv'\")\n",
    "\n",
    "        # 计算调整后的R方\n",
    "        adjusted_r_squared = results.rsquared_adj\n",
    "        print(f\"调整后的R方: {adjusted_r_squared:.4f}\")\n",
    "\n",
    "        # 绘制训练和验证损失\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('loss_plot.png')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"损失图表已保存为 'loss_plot.png'\")\n",
    "\n",
    "        # 添加时间序列交叉验证\n",
    "        cv_scores, cv_results = time_series_cv(Z, r)\n",
    "        print(f\"交叉验证 R² 分数: {np.mean(cv_scores):.4f} (±{np.std(cv_scores):.4f})\")\n",
    "        \n",
    "        # 可视化交叉验证结果\n",
    "        plot_cv_results(cv_results)\n",
    "        print(\"交叉验证结果图表已保存\")\n",
    "\n",
    "        # 样本外测试\n",
    "        oos_results, oos_model, Z_oos = out_of_sample_test(Z, r)\n",
    "        print(f\"样本外测试 R²: {oos_results.rsquared:.4f}\")\n",
    "        print(f\"样本外测试调整后 R²: {oos_results.rsquared_adj:.4f}\")\n",
    "        \n",
    "        # 输出样本外测试的完整回归结果\n",
    "        print(\"\\n样本外测试回归结果:\")\n",
    "        print(oos_results.summary())\n",
    "\n",
    "        # 可视化样本外测试结果\n",
    "        plot_oos_results(oos_results)\n",
    "        print(\"样本外测试结果图表已保存\")\n",
    "\n",
    "        # 保存交叉验证和样本外测试结果\n",
    "        cv_df = pd.DataFrame(cv_results)\n",
    "        cv_df.to_csv('cross_validation_results.csv', index=False)\n",
    "        print(\"交叉验证结果已保存到 'cross_validation_results.csv'\")\n",
    "\n",
    "        oos_df = pd.DataFrame({\n",
    "            'Actual': oos_results.model.endog,\n",
    "            'Predicted': oos_results.fittedvalues,\n",
    "            'Residuals': oos_results.resid\n",
    "        })\n",
    "        oos_df.to_csv('out_of_sample_results.csv', index=False)\n",
    "        print(\"样本外测试结果已保存到 'out_of_sample_results.csv'\")\n",
    "\n",
    "        # 不同因子数量的样本外R方测试\n",
    "        print(\"\\n开始测试不同因子数量(25-35)的样本外R方...\")\n",
    "        factor_count_results = factor_count_test(Z, r, min_factors=25, max_factors=35)\n",
    "        print(\"不同因子数量的样本外R方测试完成\")\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01be685-ca33-4a27-a649-2a4826b09b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
