{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c3492c-a127-4c1b-a441-9254d6158382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 22:25:56.758770: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-22 22:26:01,006 - INFO - Successfully loaded data from /Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/DL_Data7.csv\n",
      "2024-09-22 22:26:05,076 - INFO - Expanded features from 32 to 528\n",
      "2024-09-22 22:26:22.920643: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-22 22:26:39,244 - INFO - Epoch [1/200], Train Loss: 0.6745, Val Loss: 0.0079\n",
      "2024-09-22 22:26:55,357 - INFO - Epoch [2/200], Train Loss: 0.0087, Val Loss: 0.0053\n",
      "2024-09-22 22:27:11,472 - INFO - Epoch [3/200], Train Loss: 0.0097, Val Loss: 0.0044\n",
      "2024-09-22 22:27:27,708 - INFO - Epoch [4/200], Train Loss: 0.0037, Val Loss: 0.0028\n",
      "2024-09-22 22:27:43,912 - INFO - Epoch [5/200], Train Loss: 0.0022, Val Loss: 0.0039\n",
      "2024-09-22 22:28:00,272 - INFO - Epoch [6/200], Train Loss: 0.0020, Val Loss: 0.0018\n",
      "2024-09-22 22:28:16,654 - INFO - Epoch [7/200], Train Loss: 0.0022, Val Loss: 0.0039\n",
      "2024-09-22 22:28:33,101 - INFO - Epoch [8/200], Train Loss: 0.0024, Val Loss: 0.0033\n",
      "2024-09-22 22:28:49,532 - INFO - Epoch [9/200], Train Loss: 0.0024, Val Loss: 0.0032\n",
      "2024-09-22 22:29:05,975 - INFO - Epoch [10/200], Train Loss: 0.0024, Val Loss: 0.0031\n",
      "2024-09-22 22:29:22,425 - INFO - Epoch [11/200], Train Loss: 0.0024, Val Loss: 0.0033\n",
      "2024-09-22 22:29:38,836 - INFO - Epoch [12/200], Train Loss: 0.0023, Val Loss: 0.0030\n",
      "2024-09-22 22:29:55,290 - INFO - Epoch [13/200], Train Loss: 0.0023, Val Loss: 0.0023\n",
      "2024-09-22 22:30:11,742 - INFO - Epoch [14/200], Train Loss: 0.0023, Val Loss: 0.0056\n",
      "2024-09-22 22:30:28,195 - INFO - Epoch [15/200], Train Loss: 0.0023, Val Loss: 0.0019\n",
      "2024-09-22 22:30:44,628 - INFO - Epoch [16/200], Train Loss: 0.0023, Val Loss: 0.0032\n",
      "2024-09-22 22:31:01,042 - INFO - Epoch [17/200], Train Loss: 0.0023, Val Loss: 0.0028\n",
      "2024-09-22 22:31:17,470 - INFO - Epoch [18/200], Train Loss: 0.0023, Val Loss: 0.0016\n",
      "2024-09-22 22:31:33,897 - INFO - Epoch [19/200], Train Loss: 0.0023, Val Loss: 0.0055\n",
      "2024-09-22 22:31:50,306 - INFO - Epoch [20/200], Train Loss: 0.0023, Val Loss: 0.0023\n",
      "2024-09-22 22:32:06,753 - INFO - Epoch [21/200], Train Loss: 0.0023, Val Loss: 0.0025\n",
      "2024-09-22 22:32:23,256 - INFO - Epoch [22/200], Train Loss: 0.0023, Val Loss: 0.0019\n",
      "2024-09-22 22:32:39,684 - INFO - Epoch [23/200], Train Loss: 0.0023, Val Loss: 0.0018\n",
      "2024-09-22 22:32:56,110 - INFO - Epoch [24/200], Train Loss: 0.0022, Val Loss: 0.0031\n",
      "2024-09-22 22:33:12,489 - INFO - Epoch [25/200], Train Loss: 0.0023, Val Loss: 0.0019\n",
      "2024-09-22 22:33:28,871 - INFO - Epoch [26/200], Train Loss: 0.0023, Val Loss: 0.0020\n",
      "2024-09-22 22:33:45,275 - INFO - Epoch [27/200], Train Loss: 0.0023, Val Loss: 0.0023\n",
      "2024-09-22 22:34:01,659 - INFO - Epoch [28/200], Train Loss: 0.0023, Val Loss: 0.0025\n",
      "2024-09-22 22:34:18,028 - INFO - Epoch [29/200], Train Loss: 0.0022, Val Loss: 0.0028\n",
      "2024-09-22 22:34:34,425 - INFO - Epoch [30/200], Train Loss: 0.0022, Val Loss: 0.0019\n",
      "2024-09-22 22:34:50,802 - INFO - Epoch [31/200], Train Loss: 0.0022, Val Loss: 0.0018\n",
      "2024-09-22 22:35:07,180 - INFO - Epoch [32/200], Train Loss: 0.0022, Val Loss: 0.0024\n",
      "2024-09-22 22:35:23,558 - INFO - Epoch [33/200], Train Loss: 0.0022, Val Loss: 0.0019\n",
      "2024-09-22 22:35:39,921 - INFO - Epoch [34/200], Train Loss: 0.0022, Val Loss: 0.0015\n",
      "2024-09-22 22:35:56,290 - INFO - Epoch [35/200], Train Loss: 0.0023, Val Loss: 0.0022\n",
      "2024-09-22 22:36:12,652 - INFO - Epoch [36/200], Train Loss: 0.0022, Val Loss: 0.0030\n",
      "2024-09-22 22:36:31,004 - INFO - Epoch [37/200], Train Loss: 0.0022, Val Loss: 0.0019\n",
      "2024-09-22 22:36:47,365 - INFO - Epoch [38/200], Train Loss: 0.0022, Val Loss: 0.0023\n",
      "2024-09-22 22:37:03,734 - INFO - Epoch [39/200], Train Loss: 0.0022, Val Loss: 0.0020\n",
      "2024-09-22 22:37:20,062 - INFO - Epoch [40/200], Train Loss: 0.0022, Val Loss: 0.0019\n",
      "2024-09-22 22:37:36,773 - INFO - Epoch [41/200], Train Loss: 0.0023, Val Loss: 0.0017\n",
      "2024-09-22 22:39:36,712 - INFO - Epoch [42/200], Train Loss: 0.0021, Val Loss: 0.0023\n",
      "2024-09-22 22:39:53,471 - INFO - Epoch [43/200], Train Loss: 0.0022, Val Loss: 0.0024\n",
      "2024-09-22 22:40:09,661 - INFO - Epoch [44/200], Train Loss: 0.0022, Val Loss: 0.0023\n",
      "2024-09-22 22:40:26,054 - INFO - Epoch [45/200], Train Loss: 0.0022, Val Loss: 0.0019\n",
      "2024-09-22 22:40:42,560 - INFO - Epoch [46/200], Train Loss: 0.0022, Val Loss: 0.0016\n",
      "2024-09-22 22:40:59,194 - INFO - Epoch [47/200], Train Loss: 0.0021, Val Loss: 0.0017\n",
      "2024-09-22 22:41:16,675 - INFO - Epoch [48/200], Train Loss: 0.0021, Val Loss: 0.0038\n",
      "2024-09-22 22:41:33,232 - INFO - Epoch [49/200], Train Loss: 0.0021, Val Loss: 0.0013\n",
      "2024-09-22 22:41:49,653 - INFO - Epoch [50/200], Train Loss: 0.0022, Val Loss: 0.0019\n",
      "2024-09-22 22:42:06,089 - INFO - Epoch [51/200], Train Loss: 0.0022, Val Loss: 0.0016\n",
      "2024-09-22 22:42:22,548 - INFO - Epoch [52/200], Train Loss: 0.0021, Val Loss: 0.0022\n",
      "2024-09-22 22:42:39,004 - INFO - Epoch [53/200], Train Loss: 0.0021, Val Loss: 0.0090\n",
      "2024-09-22 22:42:55,630 - INFO - Epoch [54/200], Train Loss: 0.0021, Val Loss: 0.0033\n",
      "2024-09-22 22:43:12,048 - INFO - Epoch [55/200], Train Loss: 0.0021, Val Loss: 0.0018\n",
      "2024-09-22 22:43:28,506 - INFO - Epoch [56/200], Train Loss: 0.0021, Val Loss: 0.0014\n",
      "2024-09-22 22:43:45,031 - INFO - Epoch [57/200], Train Loss: 0.0021, Val Loss: 0.0013\n",
      "2024-09-22 22:44:01,434 - INFO - Epoch [58/200], Train Loss: 0.0021, Val Loss: 0.0013\n",
      "2024-09-22 22:44:18,011 - INFO - Epoch [59/200], Train Loss: 0.0021, Val Loss: 0.0018\n",
      "2024-09-22 22:44:34,934 - INFO - Epoch [60/200], Train Loss: 0.0021, Val Loss: 0.0018\n",
      "2024-09-22 22:44:51,793 - INFO - Epoch [61/200], Train Loss: 0.0021, Val Loss: 0.0019\n",
      "2024-09-22 22:45:08,397 - INFO - Epoch [62/200], Train Loss: 0.0021, Val Loss: 0.0020\n",
      "2024-09-22 22:45:25,197 - INFO - Epoch [63/200], Train Loss: 0.0021, Val Loss: 0.0020\n",
      "2024-09-22 22:45:41,791 - INFO - Epoch [64/200], Train Loss: 0.0021, Val Loss: 0.0020\n",
      "2024-09-22 22:45:58,454 - INFO - Epoch [65/200], Train Loss: 0.0021, Val Loss: 0.0018\n",
      "2024-09-22 22:46:15,053 - INFO - Epoch [66/200], Train Loss: 0.0021, Val Loss: 0.0087\n",
      "2024-09-22 22:46:31,500 - INFO - Epoch [67/200], Train Loss: 0.0021, Val Loss: 0.0020\n",
      "2024-09-22 22:46:47,974 - INFO - Epoch [68/200], Train Loss: 0.0021, Val Loss: 0.0019\n",
      "2024-09-22 22:47:04,455 - INFO - Epoch [69/200], Train Loss: 0.0021, Val Loss: 0.0022\n",
      "2024-09-22 22:47:04,456 - INFO - Early stopping triggered at epoch 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.889\n",
      "Model:                            OLS   Adj. R-squared:                  0.889\n",
      "Method:                 Least Squares   F-statistic:                 2.090e+04\n",
      "Date:                Sun, 22 Sep 2024   Prob (F-statistic):               0.00\n",
      "Time:                        22:47:04   Log-Likelihood:             1.3863e+05\n",
      "No. Observations:               78590   AIC:                        -2.772e+05\n",
      "Df Residuals:                   78559   BIC:                        -2.769e+05\n",
      "Df Model:                          30                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4419      0.000   2852.992      0.000       0.442       0.442\n",
      "x1          6945.8426   1088.510      6.381      0.000    4812.369    9079.316\n",
      "x2         -2393.1778   1009.800     -2.370      0.018   -4372.379    -413.977\n",
      "x3          1270.4944    969.385      1.311      0.190    -629.494    3170.483\n",
      "x4          4359.3313   1128.936      3.861      0.000    2146.623    6572.039\n",
      "x5          1770.4276    935.071      1.893      0.058     -62.305    3603.160\n",
      "x6          3802.1011   1019.252      3.730      0.000    1804.373    5799.829\n",
      "x7         -7114.2680    937.404     -7.589      0.000   -8951.575   -5276.961\n",
      "x8           484.1765   1076.328      0.450      0.653   -1625.421    2593.774\n",
      "x9         -2730.0279   1079.404     -2.529      0.011   -4845.653    -614.403\n",
      "x10         1648.2268    938.604      1.756      0.079    -191.432    3487.886\n",
      "x11         2739.8221    934.530      2.932      0.003     908.148    4571.496\n",
      "x12        -3941.4964   1029.268     -3.829      0.000   -5958.856   -1924.137\n",
      "x13         6012.8374   1046.980      5.743      0.000    3960.762    8064.913\n",
      "x14         -486.4544   1046.221     -0.465      0.642   -2537.040    1564.132\n",
      "x15         1.192e+04    943.226     12.642      0.000    1.01e+04    1.38e+04\n",
      "x16         5141.0823    928.688      5.536      0.000    3320.859    6961.306\n",
      "x17         5262.7356    929.881      5.660      0.000    3440.174    7085.297\n",
      "x18         7157.1375   1122.959      6.373      0.000    4956.144    9358.131\n",
      "x19        -4830.2150   1091.746     -4.424      0.000   -6970.031   -2690.399\n",
      "x20           66.1134   1089.786      0.061      0.952   -2069.860    2202.087\n",
      "x21         3748.4030   1126.904      3.326      0.001    1539.678    5957.128\n",
      "x22         7258.6763   1022.382      7.100      0.000    5254.813    9262.540\n",
      "x23         -891.5573    936.761     -0.952      0.341   -2727.604     944.490\n",
      "x24         1641.7819   1056.445      1.554      0.120    -428.844    3712.407\n",
      "x25        -2792.5373    986.575     -2.831      0.005   -4726.218    -858.857\n",
      "x26         3779.5111    933.682      4.048      0.000    1949.500    5609.522\n",
      "x27         1392.9532    947.598      1.470      0.142    -464.334    3250.240\n",
      "x28         4727.7179   1063.846      4.444      0.000    2642.585    6812.851\n",
      "x29         2811.0763    934.728      3.007      0.003     979.014    4643.138\n",
      "x30         2524.6188   1023.562      2.467      0.014     518.444    4530.794\n",
      "==============================================================================\n",
      "Omnibus:                   115788.516   Durbin-Watson:                   2.003\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        192721184.829\n",
      "Skew:                          -8.431   Prob(JB):                         0.00\n",
      "Kurtosis:                     245.011   Cond. No.                     4.28e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.28e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "共同因子数据已保存到 'common_factors_and_returns.csv'\n",
      "调整后的R方: 0.8886\n",
      "损失图表已保存为 'loss_plot.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 22:47:13,525 - INFO - Epoch [1/200], Train Loss: 5.6849, Val Loss: 0.0293\n",
      "2024-09-22 22:47:20,566 - INFO - Epoch [2/200], Train Loss: 0.0202, Val Loss: 0.0374\n",
      "2024-09-22 22:47:27,610 - INFO - Epoch [3/200], Train Loss: 0.0278, Val Loss: 0.0381\n",
      "2024-09-22 22:47:34,547 - INFO - Epoch [4/200], Train Loss: 0.0312, Val Loss: 0.2875\n",
      "2024-09-22 22:47:41,577 - INFO - Epoch [5/200], Train Loss: 0.0504, Val Loss: 0.2587\n",
      "2024-09-22 22:47:48,534 - INFO - Epoch [6/200], Train Loss: 0.0563, Val Loss: 0.2333\n",
      "2024-09-22 22:47:55,484 - INFO - Epoch [7/200], Train Loss: 0.0626, Val Loss: 0.2097\n",
      "2024-09-22 22:48:02,742 - INFO - Epoch [8/200], Train Loss: 0.0615, Val Loss: 0.2796\n",
      "2024-09-22 22:48:09,704 - INFO - Epoch [9/200], Train Loss: 0.0427, Val Loss: 0.1693\n",
      "2024-09-22 22:48:16,786 - INFO - Epoch [10/200], Train Loss: 0.0322, Val Loss: 0.0796\n",
      "2024-09-22 22:48:23,896 - INFO - Epoch [11/200], Train Loss: 0.0311, Val Loss: 0.5285\n",
      "2024-09-22 22:48:30,951 - INFO - Epoch [12/200], Train Loss: 0.0224, Val Loss: 0.0407\n",
      "2024-09-22 22:48:37,944 - INFO - Epoch [13/200], Train Loss: 0.0191, Val Loss: 0.0420\n",
      "2024-09-22 22:48:44,989 - INFO - Epoch [14/200], Train Loss: 0.0174, Val Loss: 0.0111\n",
      "2024-09-22 22:48:51,992 - INFO - Epoch [15/200], Train Loss: 0.0161, Val Loss: 0.0566\n",
      "2024-09-22 22:48:59,251 - INFO - Epoch [16/200], Train Loss: 0.0149, Val Loss: 0.0220\n",
      "2024-09-22 22:49:06,422 - INFO - Epoch [17/200], Train Loss: 0.0148, Val Loss: 0.0159\n",
      "2024-09-22 22:49:13,437 - INFO - Epoch [18/200], Train Loss: 0.0145, Val Loss: 0.0102\n",
      "2024-09-22 22:49:20,446 - INFO - Epoch [19/200], Train Loss: 0.0145, Val Loss: 0.0091\n",
      "2024-09-22 22:49:27,412 - INFO - Epoch [20/200], Train Loss: 0.0145, Val Loss: 0.0120\n",
      "2024-09-22 22:49:34,385 - INFO - Epoch [21/200], Train Loss: 0.0146, Val Loss: 0.0092\n",
      "2024-09-22 22:49:41,374 - INFO - Epoch [22/200], Train Loss: 0.0146, Val Loss: 0.0082\n",
      "2024-09-22 22:49:48,550 - INFO - Epoch [23/200], Train Loss: 0.0147, Val Loss: 0.0076\n",
      "2024-09-22 22:49:55,925 - INFO - Epoch [24/200], Train Loss: 0.0147, Val Loss: 0.0062\n",
      "2024-09-22 22:50:03,091 - INFO - Epoch [25/200], Train Loss: 0.0147, Val Loss: 0.0061\n",
      "2024-09-22 22:50:10,332 - INFO - Epoch [26/200], Train Loss: 0.0147, Val Loss: 0.0083\n",
      "2024-09-22 22:50:17,456 - INFO - Epoch [27/200], Train Loss: 0.0146, Val Loss: 0.0098\n",
      "2024-09-22 22:50:24,604 - INFO - Epoch [28/200], Train Loss: 0.0147, Val Loss: 0.0085\n",
      "2024-09-22 22:50:32,373 - INFO - Epoch [29/200], Train Loss: 0.0147, Val Loss: 0.0065\n",
      "2024-09-22 22:50:39,515 - INFO - Epoch [30/200], Train Loss: 0.0147, Val Loss: 0.0072\n",
      "2024-09-22 22:50:46,620 - INFO - Epoch [31/200], Train Loss: 0.0147, Val Loss: 0.0077\n",
      "2024-09-22 22:50:53,710 - INFO - Epoch [32/200], Train Loss: 0.0146, Val Loss: 0.0051\n",
      "2024-09-22 22:51:00,773 - INFO - Epoch [33/200], Train Loss: 0.0145, Val Loss: 0.0049\n",
      "2024-09-22 22:51:07,821 - INFO - Epoch [34/200], Train Loss: 0.0145, Val Loss: 0.0173\n",
      "2024-09-22 22:51:14,910 - INFO - Epoch [35/200], Train Loss: 0.0146, Val Loss: 0.0061\n",
      "2024-09-22 22:51:21,951 - INFO - Epoch [36/200], Train Loss: 0.0145, Val Loss: 0.0065\n",
      "2024-09-22 22:51:29,067 - INFO - Epoch [37/200], Train Loss: 0.0146, Val Loss: 0.0079\n",
      "2024-09-22 22:51:36,299 - INFO - Epoch [38/200], Train Loss: 0.0145, Val Loss: 0.0049\n",
      "2024-09-22 22:51:43,388 - INFO - Epoch [39/200], Train Loss: 0.0145, Val Loss: 0.0084\n",
      "2024-09-22 22:51:50,456 - INFO - Epoch [40/200], Train Loss: 0.0145, Val Loss: 0.0061\n",
      "2024-09-22 22:51:57,549 - INFO - Epoch [41/200], Train Loss: 0.0146, Val Loss: 0.0065\n",
      "2024-09-22 22:52:04,574 - INFO - Epoch [42/200], Train Loss: 0.0144, Val Loss: 0.0050\n",
      "2024-09-22 22:52:11,646 - INFO - Epoch [43/200], Train Loss: 0.0144, Val Loss: 0.0054\n",
      "2024-09-22 22:52:18,742 - INFO - Epoch [44/200], Train Loss: 0.0145, Val Loss: 0.0079\n",
      "2024-09-22 22:52:25,803 - INFO - Epoch [45/200], Train Loss: 0.0144, Val Loss: 0.0073\n",
      "2024-09-22 22:52:32,986 - INFO - Epoch [46/200], Train Loss: 0.0145, Val Loss: 0.0040\n",
      "2024-09-22 22:52:39,988 - INFO - Epoch [47/200], Train Loss: 0.0144, Val Loss: 0.0080\n",
      "2024-09-22 22:52:47,030 - INFO - Epoch [48/200], Train Loss: 0.0144, Val Loss: 0.0052\n",
      "2024-09-22 22:52:54,038 - INFO - Epoch [49/200], Train Loss: 0.0144, Val Loss: 0.0058\n",
      "2024-09-22 22:53:01,065 - INFO - Epoch [50/200], Train Loss: 0.0144, Val Loss: 0.0065\n",
      "2024-09-22 22:53:08,087 - INFO - Epoch [51/200], Train Loss: 0.0144, Val Loss: 0.0050\n",
      "2024-09-22 22:53:15,102 - INFO - Epoch [52/200], Train Loss: 0.0143, Val Loss: 0.0062\n",
      "2024-09-22 22:53:22,130 - INFO - Epoch [53/200], Train Loss: 0.0144, Val Loss: 0.0053\n",
      "2024-09-22 22:53:29,151 - INFO - Epoch [54/200], Train Loss: 0.0143, Val Loss: 0.0069\n",
      "2024-09-22 22:53:36,215 - INFO - Epoch [55/200], Train Loss: 0.0143, Val Loss: 0.0061\n",
      "2024-09-22 22:53:43,293 - INFO - Epoch [56/200], Train Loss: 0.0144, Val Loss: 0.0106\n",
      "2024-09-22 22:53:50,369 - INFO - Epoch [57/200], Train Loss: 0.0143, Val Loss: 0.0052\n",
      "2024-09-22 22:53:57,419 - INFO - Epoch [58/200], Train Loss: 0.0143, Val Loss: 0.0069\n",
      "2024-09-22 22:54:04,501 - INFO - Epoch [59/200], Train Loss: 0.0143, Val Loss: 0.0050\n",
      "2024-09-22 22:54:11,521 - INFO - Epoch [60/200], Train Loss: 0.0142, Val Loss: 0.0052\n",
      "2024-09-22 22:54:18,549 - INFO - Epoch [61/200], Train Loss: 0.0143, Val Loss: 0.0050\n",
      "2024-09-22 22:54:25,590 - INFO - Epoch [62/200], Train Loss: 0.0141, Val Loss: 0.0047\n",
      "2024-09-22 22:54:32,592 - INFO - Epoch [63/200], Train Loss: 0.0143, Val Loss: 0.0047\n",
      "2024-09-22 22:54:39,663 - INFO - Epoch [64/200], Train Loss: 0.0142, Val Loss: 0.0044\n",
      "2024-09-22 22:54:46,711 - INFO - Epoch [65/200], Train Loss: 0.0142, Val Loss: 0.0041\n",
      "2024-09-22 22:54:53,775 - INFO - Epoch [66/200], Train Loss: 0.0141, Val Loss: 0.0046\n",
      "2024-09-22 22:54:53,775 - INFO - Early stopping triggered at epoch 66\n",
      "2024-09-22 22:55:07,376 - INFO - Epoch [1/200], Train Loss: 2.7159, Val Loss: 0.0209\n",
      "2024-09-22 22:55:20,381 - INFO - Epoch [2/200], Train Loss: 0.0242, Val Loss: 0.0785\n",
      "2024-09-22 22:55:33,329 - INFO - Epoch [3/200], Train Loss: 0.0524, Val Loss: 0.0379\n",
      "2024-09-22 22:55:46,346 - INFO - Epoch [4/200], Train Loss: 0.0516, Val Loss: 0.0360\n",
      "2024-09-22 22:55:59,342 - INFO - Epoch [5/200], Train Loss: 0.0344, Val Loss: 0.0293\n",
      "2024-09-22 22:56:12,336 - INFO - Epoch [6/200], Train Loss: 0.0256, Val Loss: 0.0257\n",
      "2024-09-22 22:56:25,368 - INFO - Epoch [7/200], Train Loss: 0.0158, Val Loss: 0.0123\n",
      "2024-09-22 22:56:38,382 - INFO - Epoch [8/200], Train Loss: 0.0138, Val Loss: 0.0078\n",
      "2024-09-22 22:56:51,371 - INFO - Epoch [9/200], Train Loss: 0.0133, Val Loss: 0.0100\n",
      "2024-09-22 22:57:04,335 - INFO - Epoch [10/200], Train Loss: 0.0134, Val Loss: 0.0077\n",
      "2024-09-22 22:57:17,311 - INFO - Epoch [11/200], Train Loss: 0.0135, Val Loss: 0.0091\n",
      "2024-09-22 22:57:30,313 - INFO - Epoch [12/200], Train Loss: 0.0134, Val Loss: 0.0087\n",
      "2024-09-22 22:57:43,319 - INFO - Epoch [13/200], Train Loss: 0.0134, Val Loss: 0.0078\n",
      "2024-09-22 22:57:56,335 - INFO - Epoch [14/200], Train Loss: 0.0133, Val Loss: 0.0065\n",
      "2024-09-22 22:58:09,324 - INFO - Epoch [15/200], Train Loss: 0.0133, Val Loss: 0.0086\n",
      "2024-09-22 22:58:22,289 - INFO - Epoch [16/200], Train Loss: 0.0133, Val Loss: 0.0086\n",
      "2024-09-22 22:58:35,189 - INFO - Epoch [17/200], Train Loss: 0.0133, Val Loss: 0.0079\n",
      "2024-09-22 22:58:48,162 - INFO - Epoch [18/200], Train Loss: 0.0133, Val Loss: 0.0075\n",
      "2024-09-22 22:59:01,165 - INFO - Epoch [19/200], Train Loss: 0.0133, Val Loss: 0.0073\n",
      "2024-09-22 22:59:14,140 - INFO - Epoch [20/200], Train Loss: 0.0133, Val Loss: 0.0075\n",
      "2024-09-22 22:59:27,150 - INFO - Epoch [21/200], Train Loss: 0.0133, Val Loss: 0.0076\n",
      "2024-09-22 22:59:40,095 - INFO - Epoch [22/200], Train Loss: 0.0133, Val Loss: 0.0077\n",
      "2024-09-22 22:59:53,068 - INFO - Epoch [23/200], Train Loss: 0.0132, Val Loss: 0.0078\n",
      "2024-09-22 23:00:06,043 - INFO - Epoch [24/200], Train Loss: 0.0132, Val Loss: 0.0068\n",
      "2024-09-22 23:00:19,121 - INFO - Epoch [25/200], Train Loss: 0.0132, Val Loss: 0.0071\n",
      "2024-09-22 23:00:32,144 - INFO - Epoch [26/200], Train Loss: 0.0131, Val Loss: 0.0067\n",
      "2024-09-22 23:00:45,169 - INFO - Epoch [27/200], Train Loss: 0.0131, Val Loss: 0.0073\n",
      "2024-09-22 23:00:58,238 - INFO - Epoch [28/200], Train Loss: 0.0132, Val Loss: 0.0067\n",
      "2024-09-22 23:01:11,402 - INFO - Epoch [29/200], Train Loss: 0.0131, Val Loss: 0.0079\n",
      "2024-09-22 23:01:24,397 - INFO - Epoch [30/200], Train Loss: 0.0131, Val Loss: 0.0069\n",
      "2024-09-22 23:01:37,556 - INFO - Epoch [31/200], Train Loss: 0.0131, Val Loss: 0.0075\n",
      "2024-09-22 23:01:50,841 - INFO - Epoch [32/200], Train Loss: 0.0130, Val Loss: 0.0063\n",
      "2024-09-22 23:02:04,343 - INFO - Epoch [33/200], Train Loss: 0.0130, Val Loss: 0.0061\n",
      "2024-09-22 23:02:17,524 - INFO - Epoch [34/200], Train Loss: 0.0130, Val Loss: 0.0071\n",
      "2024-09-22 23:02:30,985 - INFO - Epoch [35/200], Train Loss: 0.0130, Val Loss: 0.0089\n",
      "2024-09-22 23:02:44,547 - INFO - Epoch [36/200], Train Loss: 0.0130, Val Loss: 0.0082\n",
      "2024-09-22 23:02:57,839 - INFO - Epoch [37/200], Train Loss: 0.0130, Val Loss: 0.0081\n",
      "2024-09-22 23:03:11,029 - INFO - Epoch [38/200], Train Loss: 0.0130, Val Loss: 0.0082\n",
      "2024-09-22 23:03:24,151 - INFO - Epoch [39/200], Train Loss: 0.0130, Val Loss: 0.0083\n",
      "2024-09-22 23:03:37,230 - INFO - Epoch [40/200], Train Loss: 0.0130, Val Loss: 0.0067\n",
      "2024-09-22 23:03:50,283 - INFO - Epoch [41/200], Train Loss: 0.0130, Val Loss: 0.0065\n",
      "2024-09-22 23:04:03,289 - INFO - Epoch [42/200], Train Loss: 0.0129, Val Loss: 0.0072\n",
      "2024-09-22 23:04:16,331 - INFO - Epoch [43/200], Train Loss: 0.0129, Val Loss: 0.0072\n",
      "2024-09-22 23:04:29,356 - INFO - Epoch [44/200], Train Loss: 0.0129, Val Loss: 0.0063\n",
      "2024-09-22 23:04:42,365 - INFO - Epoch [45/200], Train Loss: 0.0129, Val Loss: 0.0076\n",
      "2024-09-22 23:04:55,482 - INFO - Epoch [46/200], Train Loss: 0.0129, Val Loss: 0.0078\n",
      "2024-09-22 23:05:08,651 - INFO - Epoch [47/200], Train Loss: 0.0129, Val Loss: 0.0100\n",
      "2024-09-22 23:05:21,836 - INFO - Epoch [48/200], Train Loss: 0.0129, Val Loss: 0.0069\n",
      "2024-09-22 23:05:35,083 - INFO - Epoch [49/200], Train Loss: 0.0129, Val Loss: 0.0073\n",
      "2024-09-22 23:05:48,695 - INFO - Epoch [50/200], Train Loss: 0.0129, Val Loss: 0.0066\n",
      "2024-09-22 23:06:02,043 - INFO - Epoch [51/200], Train Loss: 0.0128, Val Loss: 0.0067\n",
      "2024-09-22 23:06:15,705 - INFO - Epoch [52/200], Train Loss: 0.0128, Val Loss: 0.0059\n",
      "2024-09-22 23:06:29,409 - INFO - Epoch [53/200], Train Loss: 0.0128, Val Loss: 0.0071\n",
      "2024-09-22 23:06:43,042 - INFO - Epoch [54/200], Train Loss: 0.0128, Val Loss: 0.0069\n",
      "2024-09-22 23:06:56,934 - INFO - Epoch [55/200], Train Loss: 0.0128, Val Loss: 0.0073\n",
      "2024-09-22 23:07:10,296 - INFO - Epoch [56/200], Train Loss: 0.0128, Val Loss: 0.0065\n",
      "2024-09-22 23:07:23,648 - INFO - Epoch [57/200], Train Loss: 0.0127, Val Loss: 0.0063\n",
      "2024-09-22 23:07:36,748 - INFO - Epoch [58/200], Train Loss: 0.0128, Val Loss: 0.0047\n",
      "2024-09-22 23:07:50,223 - INFO - Epoch [59/200], Train Loss: 0.0128, Val Loss: 0.0077\n",
      "2024-09-22 23:08:03,849 - INFO - Epoch [60/200], Train Loss: 0.0128, Val Loss: 0.0066\n",
      "2024-09-22 23:08:17,482 - INFO - Epoch [61/200], Train Loss: 0.0128, Val Loss: 0.0079\n",
      "2024-09-22 23:08:31,281 - INFO - Epoch [62/200], Train Loss: 0.0127, Val Loss: 0.0064\n",
      "2024-09-22 23:08:45,090 - INFO - Epoch [63/200], Train Loss: 0.0127, Val Loss: 0.0067\n",
      "2024-09-22 23:08:58,796 - INFO - Epoch [64/200], Train Loss: 0.0127, Val Loss: 0.0059\n",
      "2024-09-22 23:09:11,852 - INFO - Epoch [65/200], Train Loss: 0.0127, Val Loss: 0.0066\n",
      "2024-09-22 23:09:24,860 - INFO - Epoch [66/200], Train Loss: 0.0127, Val Loss: 0.0065\n",
      "2024-09-22 23:09:37,831 - INFO - Epoch [67/200], Train Loss: 0.0127, Val Loss: 0.0067\n",
      "2024-09-22 23:09:50,802 - INFO - Epoch [68/200], Train Loss: 0.0126, Val Loss: 0.0091\n",
      "2024-09-22 23:10:03,805 - INFO - Epoch [69/200], Train Loss: 0.0127, Val Loss: 0.0060\n",
      "2024-09-22 23:10:16,888 - INFO - Epoch [70/200], Train Loss: 0.0127, Val Loss: 0.0082\n",
      "2024-09-22 23:10:29,931 - INFO - Epoch [71/200], Train Loss: 0.0127, Val Loss: 0.0065\n",
      "2024-09-22 23:10:43,090 - INFO - Epoch [72/200], Train Loss: 0.0126, Val Loss: 0.0072\n",
      "2024-09-22 23:10:56,194 - INFO - Epoch [73/200], Train Loss: 0.0126, Val Loss: 0.0050\n",
      "2024-09-22 23:11:09,499 - INFO - Epoch [74/200], Train Loss: 0.0126, Val Loss: 0.0082\n",
      "2024-09-22 23:11:23,318 - INFO - Epoch [75/200], Train Loss: 0.0126, Val Loss: 0.0054\n",
      "2024-09-22 23:11:37,142 - INFO - Epoch [76/200], Train Loss: 0.0126, Val Loss: 0.0081\n",
      "2024-09-22 23:11:51,017 - INFO - Epoch [77/200], Train Loss: 0.0126, Val Loss: 0.0067\n",
      "2024-09-22 23:12:04,926 - INFO - Epoch [78/200], Train Loss: 0.0126, Val Loss: 0.0049\n",
      "2024-09-22 23:12:04,927 - INFO - Early stopping triggered at epoch 78\n",
      "2024-09-22 23:12:26,697 - INFO - Epoch [1/200], Train Loss: 2.5866, Val Loss: 0.0871\n",
      "2024-09-22 23:12:46,470 - INFO - Epoch [2/200], Train Loss: 0.0439, Val Loss: 0.1415\n",
      "2024-09-22 23:13:06,887 - INFO - Epoch [3/200], Train Loss: 0.0764, Val Loss: 1.0475\n",
      "2024-09-22 23:13:27,794 - INFO - Epoch [4/200], Train Loss: 0.0330, Val Loss: 0.4249\n",
      "2024-09-22 23:13:48,388 - INFO - Epoch [5/200], Train Loss: 0.0182, Val Loss: 0.0918\n",
      "2024-09-22 23:14:08,959 - INFO - Epoch [6/200], Train Loss: 0.0139, Val Loss: 0.0123\n",
      "2024-09-22 23:14:29,175 - INFO - Epoch [7/200], Train Loss: 0.0133, Val Loss: 0.0147\n",
      "2024-09-22 23:14:48,732 - INFO - Epoch [8/200], Train Loss: 0.0134, Val Loss: 0.0111\n",
      "2024-09-22 23:15:08,332 - INFO - Epoch [9/200], Train Loss: 0.0135, Val Loss: 0.0135\n",
      "2024-09-22 23:15:27,887 - INFO - Epoch [10/200], Train Loss: 0.0135, Val Loss: 0.0134\n",
      "2024-09-22 23:15:47,823 - INFO - Epoch [11/200], Train Loss: 0.0134, Val Loss: 0.0124\n",
      "2024-09-22 23:16:08,482 - INFO - Epoch [12/200], Train Loss: 0.0133, Val Loss: 0.0075\n",
      "2024-09-22 23:16:29,254 - INFO - Epoch [13/200], Train Loss: 0.0133, Val Loss: 0.0115\n",
      "2024-09-22 23:16:49,665 - INFO - Epoch [14/200], Train Loss: 0.0133, Val Loss: 0.0100\n",
      "2024-09-22 23:17:09,167 - INFO - Epoch [15/200], Train Loss: 0.0133, Val Loss: 0.0106\n",
      "2024-09-22 23:17:28,694 - INFO - Epoch [16/200], Train Loss: 0.0133, Val Loss: 0.0117\n",
      "2024-09-22 23:17:48,159 - INFO - Epoch [17/200], Train Loss: 0.0133, Val Loss: 0.0096\n",
      "2024-09-22 23:18:07,659 - INFO - Epoch [18/200], Train Loss: 0.0133, Val Loss: 0.0103\n",
      "2024-09-22 23:18:27,761 - INFO - Epoch [19/200], Train Loss: 0.0133, Val Loss: 0.0097\n",
      "2024-09-22 23:18:47,628 - INFO - Epoch [20/200], Train Loss: 0.0133, Val Loss: 0.0100\n",
      "2024-09-22 23:19:07,229 - INFO - Epoch [21/200], Train Loss: 0.0132, Val Loss: 0.0115\n",
      "2024-09-22 23:19:26,986 - INFO - Epoch [22/200], Train Loss: 0.0132, Val Loss: 0.0093\n",
      "2024-09-22 23:19:46,506 - INFO - Epoch [23/200], Train Loss: 0.0132, Val Loss: 0.0102\n",
      "2024-09-22 23:20:06,612 - INFO - Epoch [24/200], Train Loss: 0.0132, Val Loss: 0.0100\n",
      "2024-09-22 23:20:26,832 - INFO - Epoch [25/200], Train Loss: 0.0131, Val Loss: 0.0096\n",
      "2024-09-22 23:20:46,682 - INFO - Epoch [26/200], Train Loss: 0.0131, Val Loss: 0.0096\n",
      "2024-09-22 23:21:06,780 - INFO - Epoch [27/200], Train Loss: 0.0131, Val Loss: 0.0081\n",
      "2024-09-22 23:21:26,710 - INFO - Epoch [28/200], Train Loss: 0.0130, Val Loss: 0.0091\n",
      "2024-09-22 23:21:46,710 - INFO - Epoch [29/200], Train Loss: 0.0130, Val Loss: 0.0077\n",
      "2024-09-22 23:22:06,868 - INFO - Epoch [30/200], Train Loss: 0.0130, Val Loss: 0.0092\n",
      "2024-09-22 23:22:26,987 - INFO - Epoch [31/200], Train Loss: 0.0130, Val Loss: 0.0093\n",
      "2024-09-22 23:22:47,085 - INFO - Epoch [32/200], Train Loss: 0.0130, Val Loss: 0.0079\n",
      "2024-09-22 23:22:47,086 - INFO - Early stopping triggered at epoch 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉验证 R² 分数: 0.7049 (±0.0276)\n",
      "交叉验证结果图表已保存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "2024-09-22 23:23:03,876 - INFO - Epoch [1/200], Train Loss: 4.4180, Val Loss: 0.0177\n",
      "2024-09-22 23:23:20,159 - INFO - Epoch [2/200], Train Loss: 0.0243, Val Loss: 0.0674\n",
      "2024-09-22 23:23:36,059 - INFO - Epoch [3/200], Train Loss: 0.0768, Val Loss: 0.0673\n",
      "2024-09-22 23:23:52,128 - INFO - Epoch [4/200], Train Loss: 0.0723, Val Loss: 0.1014\n",
      "2024-09-22 23:24:08,134 - INFO - Epoch [5/200], Train Loss: 0.0389, Val Loss: 0.0731\n",
      "2024-09-22 23:24:23,980 - INFO - Epoch [6/200], Train Loss: 0.0237, Val Loss: 0.0079\n",
      "2024-09-22 23:24:39,782 - INFO - Epoch [7/200], Train Loss: 0.0153, Val Loss: 0.0145\n",
      "2024-09-22 23:24:55,659 - INFO - Epoch [8/200], Train Loss: 0.0136, Val Loss: 0.0098\n",
      "2024-09-22 23:25:11,355 - INFO - Epoch [9/200], Train Loss: 0.0133, Val Loss: 0.0078\n",
      "2024-09-22 23:25:27,076 - INFO - Epoch [10/200], Train Loss: 0.0134, Val Loss: 0.0085\n",
      "2024-09-22 23:25:42,715 - INFO - Epoch [11/200], Train Loss: 0.0134, Val Loss: 0.0085\n",
      "2024-09-22 23:25:58,423 - INFO - Epoch [12/200], Train Loss: 0.0133, Val Loss: 0.0095\n",
      "2024-09-22 23:26:14,002 - INFO - Epoch [13/200], Train Loss: 0.0134, Val Loss: 0.0083\n",
      "2024-09-22 23:26:29,539 - INFO - Epoch [14/200], Train Loss: 0.0134, Val Loss: 0.0080\n",
      "2024-09-22 23:26:45,158 - INFO - Epoch [15/200], Train Loss: 0.0133, Val Loss: 0.0086\n",
      "2024-09-22 23:27:00,864 - INFO - Epoch [16/200], Train Loss: 0.0133, Val Loss: 0.0081\n",
      "2024-09-22 23:27:16,745 - INFO - Epoch [17/200], Train Loss: 0.0133, Val Loss: 0.0085\n",
      "2024-09-22 23:27:32,593 - INFO - Epoch [18/200], Train Loss: 0.0133, Val Loss: 0.0075\n",
      "2024-09-22 23:27:48,529 - INFO - Epoch [19/200], Train Loss: 0.0133, Val Loss: 0.0075\n",
      "2024-09-22 23:28:04,286 - INFO - Epoch [20/200], Train Loss: 0.0133, Val Loss: 0.0075\n",
      "2024-09-22 23:28:20,149 - INFO - Epoch [21/200], Train Loss: 0.0132, Val Loss: 0.0082\n",
      "2024-09-22 23:28:35,881 - INFO - Epoch [22/200], Train Loss: 0.0131, Val Loss: 0.0080\n",
      "2024-09-22 23:28:51,628 - INFO - Epoch [23/200], Train Loss: 0.0132, Val Loss: 0.0077\n",
      "2024-09-22 23:29:07,462 - INFO - Epoch [24/200], Train Loss: 0.0132, Val Loss: 0.0082\n",
      "2024-09-22 23:29:23,244 - INFO - Epoch [25/200], Train Loss: 0.0132, Val Loss: 0.0082\n",
      "2024-09-22 23:29:39,184 - INFO - Epoch [26/200], Train Loss: 0.0131, Val Loss: 0.0079\n",
      "2024-09-22 23:29:55,068 - INFO - Epoch [27/200], Train Loss: 0.0131, Val Loss: 0.0085\n",
      "2024-09-22 23:30:11,117 - INFO - Epoch [28/200], Train Loss: 0.0131, Val Loss: 0.0084\n",
      "2024-09-22 23:30:26,887 - INFO - Epoch [29/200], Train Loss: 0.0130, Val Loss: 0.0081\n",
      "2024-09-22 23:30:42,465 - INFO - Epoch [30/200], Train Loss: 0.0130, Val Loss: 0.0077\n",
      "2024-09-22 23:30:58,161 - INFO - Epoch [31/200], Train Loss: 0.0131, Val Loss: 0.0081\n",
      "2024-09-22 23:31:13,782 - INFO - Epoch [32/200], Train Loss: 0.0130, Val Loss: 0.0069\n",
      "2024-09-22 23:31:29,753 - INFO - Epoch [33/200], Train Loss: 0.0130, Val Loss: 0.0096\n",
      "2024-09-22 23:31:45,465 - INFO - Epoch [34/200], Train Loss: 0.0130, Val Loss: 0.0080\n",
      "2024-09-22 23:32:01,250 - INFO - Epoch [35/200], Train Loss: 0.0129, Val Loss: 0.0066\n",
      "2024-09-22 23:32:17,065 - INFO - Epoch [36/200], Train Loss: 0.0129, Val Loss: 0.0066\n",
      "2024-09-22 23:32:32,834 - INFO - Epoch [37/200], Train Loss: 0.0130, Val Loss: 0.0065\n",
      "2024-09-22 23:32:48,837 - INFO - Epoch [38/200], Train Loss: 0.0129, Val Loss: 0.0067\n",
      "2024-09-22 23:33:04,657 - INFO - Epoch [39/200], Train Loss: 0.0129, Val Loss: 0.0088\n",
      "2024-09-22 23:33:20,400 - INFO - Epoch [40/200], Train Loss: 0.0129, Val Loss: 0.0065\n",
      "2024-09-22 23:33:36,135 - INFO - Epoch [41/200], Train Loss: 0.0129, Val Loss: 0.0068\n",
      "2024-09-22 23:33:51,951 - INFO - Epoch [42/200], Train Loss: 0.0128, Val Loss: 0.0061\n",
      "2024-09-22 23:34:08,126 - INFO - Epoch [43/200], Train Loss: 0.0128, Val Loss: 0.0076\n",
      "2024-09-22 23:34:23,736 - INFO - Epoch [44/200], Train Loss: 0.0128, Val Loss: 0.0086\n",
      "2024-09-22 23:34:39,321 - INFO - Epoch [45/200], Train Loss: 0.0128, Val Loss: 0.0072\n",
      "2024-09-22 23:34:54,901 - INFO - Epoch [46/200], Train Loss: 0.0128, Val Loss: 0.0086\n",
      "2024-09-22 23:35:10,508 - INFO - Epoch [47/200], Train Loss: 0.0128, Val Loss: 0.0073\n",
      "2024-09-22 23:35:26,127 - INFO - Epoch [48/200], Train Loss: 0.0128, Val Loss: 0.0070\n",
      "2024-09-22 23:35:41,789 - INFO - Epoch [49/200], Train Loss: 0.0128, Val Loss: 0.0084\n",
      "2024-09-22 23:35:57,421 - INFO - Epoch [50/200], Train Loss: 0.0128, Val Loss: 0.0075\n",
      "2024-09-22 23:36:13,036 - INFO - Epoch [51/200], Train Loss: 0.0128, Val Loss: 0.0062\n",
      "2024-09-22 23:36:28,614 - INFO - Epoch [52/200], Train Loss: 0.0127, Val Loss: 0.0065\n",
      "2024-09-22 23:36:44,196 - INFO - Epoch [53/200], Train Loss: 0.0127, Val Loss: 0.0069\n",
      "2024-09-22 23:36:59,796 - INFO - Epoch [54/200], Train Loss: 0.0127, Val Loss: 0.0070\n",
      "2024-09-22 23:37:15,434 - INFO - Epoch [55/200], Train Loss: 0.0127, Val Loss: 0.0075\n",
      "2024-09-22 23:37:31,056 - INFO - Epoch [56/200], Train Loss: 0.0127, Val Loss: 0.0060\n",
      "2024-09-22 23:37:46,635 - INFO - Epoch [57/200], Train Loss: 0.0127, Val Loss: 0.0057\n",
      "2024-09-22 23:38:02,251 - INFO - Epoch [58/200], Train Loss: 0.0127, Val Loss: 0.0093\n",
      "2024-09-22 23:38:17,900 - INFO - Epoch [59/200], Train Loss: 0.0127, Val Loss: 0.0077\n",
      "2024-09-22 23:38:33,533 - INFO - Epoch [60/200], Train Loss: 0.0127, Val Loss: 0.0058\n",
      "2024-09-22 23:38:49,158 - INFO - Epoch [61/200], Train Loss: 0.0127, Val Loss: 0.0059\n",
      "2024-09-22 23:39:04,741 - INFO - Epoch [62/200], Train Loss: 0.0126, Val Loss: 0.0064\n",
      "2024-09-22 23:39:20,374 - INFO - Epoch [63/200], Train Loss: 0.0126, Val Loss: 0.0048\n",
      "2024-09-22 23:39:35,978 - INFO - Epoch [64/200], Train Loss: 0.0126, Val Loss: 0.0058\n",
      "2024-09-22 23:39:51,602 - INFO - Epoch [65/200], Train Loss: 0.0126, Val Loss: 0.0067\n",
      "2024-09-22 23:40:07,175 - INFO - Epoch [66/200], Train Loss: 0.0126, Val Loss: 0.0064\n",
      "2024-09-22 23:40:22,783 - INFO - Epoch [67/200], Train Loss: 0.0126, Val Loss: 0.0058\n",
      "2024-09-22 23:40:38,536 - INFO - Epoch [68/200], Train Loss: 0.0126, Val Loss: 0.0076\n",
      "2024-09-22 23:40:54,286 - INFO - Epoch [69/200], Train Loss: 0.0126, Val Loss: 0.0050\n",
      "2024-09-22 23:41:10,269 - INFO - Epoch [70/200], Train Loss: 0.0126, Val Loss: 0.0063\n",
      "2024-09-22 23:41:26,048 - INFO - Epoch [71/200], Train Loss: 0.0126, Val Loss: 0.0081\n",
      "2024-09-22 23:41:41,781 - INFO - Epoch [72/200], Train Loss: 0.0126, Val Loss: 0.0072\n",
      "2024-09-22 23:41:57,495 - INFO - Epoch [73/200], Train Loss: 0.0126, Val Loss: 0.0076\n",
      "2024-09-22 23:42:13,449 - INFO - Epoch [74/200], Train Loss: 0.0126, Val Loss: 0.0071\n",
      "2024-09-22 23:42:29,114 - INFO - Epoch [75/200], Train Loss: 0.0125, Val Loss: 0.0066\n",
      "2024-09-22 23:42:44,869 - INFO - Epoch [76/200], Train Loss: 0.0125, Val Loss: 0.0066\n",
      "2024-09-22 23:43:00,495 - INFO - Epoch [77/200], Train Loss: 0.0125, Val Loss: 0.0059\n",
      "2024-09-22 23:43:16,096 - INFO - Epoch [78/200], Train Loss: 0.0125, Val Loss: 0.0064\n",
      "2024-09-22 23:43:31,710 - INFO - Epoch [79/200], Train Loss: 0.0126, Val Loss: 0.0070\n",
      "2024-09-22 23:43:47,361 - INFO - Epoch [80/200], Train Loss: 0.0125, Val Loss: 0.0056\n",
      "2024-09-22 23:44:03,013 - INFO - Epoch [81/200], Train Loss: 0.0126, Val Loss: 0.0056\n",
      "2024-09-22 23:44:18,606 - INFO - Epoch [82/200], Train Loss: 0.0125, Val Loss: 0.0050\n",
      "2024-09-22 23:44:34,150 - INFO - Epoch [83/200], Train Loss: 0.0125, Val Loss: 0.0042\n",
      "2024-09-22 23:44:49,909 - INFO - Epoch [84/200], Train Loss: 0.0125, Val Loss: 0.0065\n",
      "2024-09-22 23:45:05,519 - INFO - Epoch [85/200], Train Loss: 0.0125, Val Loss: 0.0075\n",
      "2024-09-22 23:45:21,162 - INFO - Epoch [86/200], Train Loss: 0.0125, Val Loss: 0.0061\n",
      "2024-09-22 23:45:36,755 - INFO - Epoch [87/200], Train Loss: 0.0125, Val Loss: 0.0076\n",
      "2024-09-22 23:45:52,306 - INFO - Epoch [88/200], Train Loss: 0.0125, Val Loss: 0.0063\n",
      "2024-09-22 23:46:07,873 - INFO - Epoch [89/200], Train Loss: 0.0124, Val Loss: 0.0045\n",
      "2024-09-22 23:46:23,518 - INFO - Epoch [90/200], Train Loss: 0.0124, Val Loss: 0.0081\n",
      "2024-09-22 23:46:39,233 - INFO - Epoch [91/200], Train Loss: 0.0125, Val Loss: 0.0053\n",
      "2024-09-22 23:46:54,890 - INFO - Epoch [92/200], Train Loss: 0.0124, Val Loss: 0.0048\n",
      "2024-09-22 23:47:10,518 - INFO - Epoch [93/200], Train Loss: 0.0124, Val Loss: 0.0051\n",
      "2024-09-22 23:47:26,198 - INFO - Epoch [94/200], Train Loss: 0.0124, Val Loss: 0.0058\n",
      "2024-09-22 23:47:42,109 - INFO - Epoch [95/200], Train Loss: 0.0124, Val Loss: 0.0057\n",
      "2024-09-22 23:47:58,128 - INFO - Epoch [96/200], Train Loss: 0.0124, Val Loss: 0.0049\n",
      "2024-09-22 23:48:13,912 - INFO - Epoch [97/200], Train Loss: 0.0124, Val Loss: 0.0039\n",
      "2024-09-22 23:48:29,602 - INFO - Epoch [98/200], Train Loss: 0.0124, Val Loss: 0.0039\n",
      "2024-09-22 23:48:45,128 - INFO - Epoch [99/200], Train Loss: 0.0124, Val Loss: 0.0058\n",
      "2024-09-22 23:49:00,694 - INFO - Epoch [100/200], Train Loss: 0.0124, Val Loss: 0.0043\n",
      "2024-09-22 23:49:16,354 - INFO - Epoch [101/200], Train Loss: 0.0124, Val Loss: 0.0043\n",
      "2024-09-22 23:49:31,947 - INFO - Epoch [102/200], Train Loss: 0.0123, Val Loss: 0.0060\n",
      "2024-09-22 23:49:47,583 - INFO - Epoch [103/200], Train Loss: 0.0124, Val Loss: 0.0043\n",
      "2024-09-22 23:50:03,236 - INFO - Epoch [104/200], Train Loss: 0.0124, Val Loss: 0.0036\n",
      "2024-09-22 23:50:18,831 - INFO - Epoch [105/200], Train Loss: 0.0124, Val Loss: 0.0058\n",
      "2024-09-22 23:50:34,456 - INFO - Epoch [106/200], Train Loss: 0.0123, Val Loss: 0.0049\n",
      "2024-09-22 23:50:50,087 - INFO - Epoch [107/200], Train Loss: 0.0123, Val Loss: 0.0052\n",
      "2024-09-22 23:51:05,663 - INFO - Epoch [108/200], Train Loss: 0.0124, Val Loss: 0.0044\n",
      "2024-09-22 23:51:21,211 - INFO - Epoch [109/200], Train Loss: 0.0124, Val Loss: 0.0046\n",
      "2024-09-22 23:51:36,796 - INFO - Epoch [110/200], Train Loss: 0.0123, Val Loss: 0.0059\n",
      "2024-09-22 23:51:52,418 - INFO - Epoch [111/200], Train Loss: 0.0124, Val Loss: 0.0046\n",
      "2024-09-22 23:52:08,178 - INFO - Epoch [112/200], Train Loss: 0.0123, Val Loss: 0.0074\n",
      "2024-09-22 23:52:23,845 - INFO - Epoch [113/200], Train Loss: 0.0123, Val Loss: 0.0051\n",
      "2024-09-22 23:52:39,494 - INFO - Epoch [114/200], Train Loss: 0.0123, Val Loss: 0.0036\n",
      "2024-09-22 23:52:55,102 - INFO - Epoch [115/200], Train Loss: 0.0122, Val Loss: 0.0057\n",
      "2024-09-22 23:53:10,726 - INFO - Epoch [116/200], Train Loss: 0.0123, Val Loss: 0.0050\n",
      "2024-09-22 23:53:26,371 - INFO - Epoch [117/200], Train Loss: 0.0124, Val Loss: 0.0045\n",
      "2024-09-22 23:53:41,990 - INFO - Epoch [118/200], Train Loss: 0.0123, Val Loss: 0.0045\n",
      "2024-09-22 23:53:57,554 - INFO - Epoch [119/200], Train Loss: 0.0123, Val Loss: 0.0046\n",
      "2024-09-22 23:54:13,327 - INFO - Epoch [120/200], Train Loss: 0.0124, Val Loss: 0.0040\n",
      "2024-09-22 23:54:28,959 - INFO - Epoch [121/200], Train Loss: 0.0123, Val Loss: 0.0034\n",
      "2024-09-22 23:54:44,618 - INFO - Epoch [122/200], Train Loss: 0.0123, Val Loss: 0.0039\n",
      "2024-09-22 23:55:00,321 - INFO - Epoch [123/200], Train Loss: 0.0123, Val Loss: 0.0053\n",
      "2024-09-22 23:55:16,073 - INFO - Epoch [124/200], Train Loss: 0.0123, Val Loss: 0.0042\n",
      "2024-09-22 23:55:31,716 - INFO - Epoch [125/200], Train Loss: 0.0122, Val Loss: 0.0040\n",
      "2024-09-22 23:55:47,292 - INFO - Epoch [126/200], Train Loss: 0.0122, Val Loss: 0.0040\n",
      "2024-09-22 23:56:02,749 - INFO - Epoch [127/200], Train Loss: 0.0123, Val Loss: 0.0042\n",
      "2024-09-22 23:56:18,221 - INFO - Epoch [128/200], Train Loss: 0.0123, Val Loss: 0.0060\n",
      "2024-09-22 23:56:33,660 - INFO - Epoch [129/200], Train Loss: 0.0123, Val Loss: 0.0054\n",
      "2024-09-22 23:56:49,133 - INFO - Epoch [130/200], Train Loss: 0.0123, Val Loss: 0.0055\n",
      "2024-09-22 23:57:04,597 - INFO - Epoch [131/200], Train Loss: 0.0123, Val Loss: 0.0041\n",
      "2024-09-22 23:57:20,031 - INFO - Epoch [132/200], Train Loss: 0.0122, Val Loss: 0.0060\n",
      "2024-09-22 23:57:35,584 - INFO - Epoch [133/200], Train Loss: 0.0122, Val Loss: 0.0044\n",
      "2024-09-22 23:57:51,074 - INFO - Epoch [134/200], Train Loss: 0.0122, Val Loss: 0.0058\n",
      "2024-09-22 23:58:06,566 - INFO - Epoch [135/200], Train Loss: 0.0122, Val Loss: 0.0052\n",
      "2024-09-22 23:58:22,043 - INFO - Epoch [136/200], Train Loss: 0.0122, Val Loss: 0.0046\n",
      "2024-09-22 23:58:37,611 - INFO - Epoch [137/200], Train Loss: 0.0122, Val Loss: 0.0046\n",
      "2024-09-22 23:58:53,078 - INFO - Epoch [138/200], Train Loss: 0.0121, Val Loss: 0.0046\n",
      "2024-09-22 23:59:08,526 - INFO - Epoch [139/200], Train Loss: 0.0122, Val Loss: 0.0045\n",
      "2024-09-22 23:59:24,111 - INFO - Epoch [140/200], Train Loss: 0.0122, Val Loss: 0.0040\n",
      "2024-09-22 23:59:39,722 - INFO - Epoch [141/200], Train Loss: 0.0122, Val Loss: 0.0041\n",
      "2024-09-22 23:59:39,722 - INFO - Early stopping triggered at epoch 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本外测试 R²: 0.6911\n",
      "样本外测试调整后 R²: 0.6910\n",
      "\n",
      "样本外测试回归结果:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.691\n",
      "Model:                            OLS   Adj. R-squared:                  0.691\n",
      "Method:                 Least Squares   F-statistic:                     5858.\n",
      "Date:                Sun, 22 Sep 2024   Prob (F-statistic):               0.00\n",
      "Time:                        23:59:40   Log-Likelihood:                 92054.\n",
      "No. Observations:               78590   AIC:                        -1.840e+05\n",
      "Df Residuals:                   78559   BIC:                        -1.838e+05\n",
      "Df Model:                          30                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4712      0.000   1708.417      0.000       0.471       0.472\n",
      "x1          1.955e+04   2437.250      8.023      0.000    1.48e+04    2.43e+04\n",
      "x2         -1.152e+04   2393.455     -4.812      0.000   -1.62e+04   -6826.457\n",
      "x3          2.504e+04   2245.167     11.152      0.000    2.06e+04    2.94e+04\n",
      "x4         -1.125e+04   2283.824     -4.927      0.000   -1.57e+04   -6775.769\n",
      "x5          3730.2369   2352.162      1.586      0.113    -879.987    8340.460\n",
      "x6          5330.7312   2426.296      2.197      0.028     575.204    1.01e+04\n",
      "x7         -1.082e+04   2335.818     -4.630      0.000   -1.54e+04   -6237.485\n",
      "x8         -1.463e+04   2329.105     -6.281      0.000   -1.92e+04   -1.01e+04\n",
      "x9          -1.87e+04   2384.244     -7.842      0.000   -2.34e+04    -1.4e+04\n",
      "x10         3.194e+04   2266.566     14.093      0.000    2.75e+04    3.64e+04\n",
      "x11         2.649e+04   2356.920     11.237      0.000    2.19e+04    3.11e+04\n",
      "x12        -2614.0250   2491.425     -1.049      0.294   -7497.204    2269.154\n",
      "x13        -2.247e+04   2402.431     -9.353      0.000   -2.72e+04   -1.78e+04\n",
      "x14        -2.226e+04   2533.569     -8.787      0.000   -2.72e+04   -1.73e+04\n",
      "x15          -83.4924   2264.114     -0.037      0.971   -4521.142    4354.158\n",
      "x16         1.316e+04   2485.937      5.293      0.000    8284.765     1.8e+04\n",
      "x17         -5.18e+04   2263.108    -22.888      0.000   -5.62e+04   -4.74e+04\n",
      "x18         9617.8590   2494.020      3.856      0.000    4729.594    1.45e+04\n",
      "x19        -1195.0494   2253.549     -0.530      0.596   -5611.993    3221.894\n",
      "x20        -3975.3069   2343.608     -1.696      0.090   -8568.765     618.151\n",
      "x21         2769.1551   2496.345      1.109      0.267   -2123.666    7661.976\n",
      "x22        -1.543e+04   2341.199     -6.589      0.000      -2e+04   -1.08e+04\n",
      "x23         4373.3207   2448.017      1.786      0.074    -424.778    9171.420\n",
      "x24         1.722e+04   2355.218      7.311      0.000    1.26e+04    2.18e+04\n",
      "x25        -8116.1013   2292.121     -3.541      0.000   -1.26e+04   -3623.558\n",
      "x26        -2.939e+04   2434.646    -12.071      0.000   -3.42e+04   -2.46e+04\n",
      "x27         1.653e+04   2358.683      7.009      0.000    1.19e+04    2.12e+04\n",
      "x28        -1.247e+04   2235.610     -5.578      0.000   -1.69e+04   -8089.090\n",
      "x29        -3532.1311   2455.987     -1.438      0.150   -8345.851    1281.589\n",
      "x30         3.091e+04   2311.897     13.370      0.000    2.64e+04    3.54e+04\n",
      "==============================================================================\n",
      "Omnibus:                     9261.494   Durbin-Watson:                   0.059\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            81735.763\n",
      "Skew:                           0.232   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.974   Cond. No.                     5.35e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.35e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "eval_env: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/statsmodels/graphics/regressionplots.py:429: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fig = abline_plot(0, fitted_line.params[0], color='k', ax=ax)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本外测试结果图表已保存\n",
      "交叉验证结果已保存到 'cross_validation_results.csv'\n",
      "样本外测试结果已保存到 'out_of_sample_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 1. 数据准备\n",
    "def load_and_preprocess_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logging.info(f\"Successfully loaded data from {file_path}\")\n",
    "        \n",
    "        Z = data.iloc[:, 4:36].values  \n",
    "        # 特征扩展：两两交互\n",
    "        expanded_features = []\n",
    "        for i, j in combinations(range(Z.shape[1]), 2):\n",
    "            expanded_features.append(Z[:, i] * Z[:, j])\n",
    "        \n",
    "        Z_expanded = np.column_stack([Z] + expanded_features)\n",
    "        logging.info(f\"Expanded features from {Z.shape[1]} to {Z_expanded.shape[1]}\")\n",
    "\n",
    "        r = data.iloc[:, 3].values  # 收益率，对应r_t\n",
    "\n",
    "        # 使用StandardScaler来标准化特征\n",
    "        scaler = StandardScaler()\n",
    "        Z_scaled = scaler.fit_transform(Z_expanded)\n",
    "\n",
    "        Z = Z_scaled.astype(np.float32)\n",
    "        r = r.astype(np.float32)\n",
    "\n",
    "        # 检查并移除无限值或NaN\n",
    "        mask = np.isfinite(Z).all(axis=1) & np.isfinite(r)\n",
    "        Z = Z[mask]\n",
    "        r = r[mask]\n",
    "\n",
    "        return Z, r\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading or preprocessing data: {e}\")\n",
    "        raise\n",
    "\n",
    "# 2. 模型设计\n",
    "class ConditionalAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout_rate=0.2):\n",
    "        super(ConditionalAutoencoder, self).__init__()\n",
    "        \n",
    "        # 深度学习自解码因子荷载神经网络 B(Z_{t-1})\n",
    "        self.beta_net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(hidden_dim, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(hidden_dim // 2, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(latent_dim, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        ])\n",
    "        \n",
    "        # 深度学习自解码的因子提取网络 f_t\n",
    "        self.factor_net = tf.keras.layers.Dense(latent_dim, use_bias=False, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        Z = inputs\n",
    "        beta = self.beta_net(Z, training=training)  # B(Z_{t-1})\n",
    "        f = self.factor_net(tf.ones((tf.shape(Z)[0], 1)))  # f_t\n",
    "        return tf.reduce_sum(beta * f, axis=1)  # r_t = B(Z_{t-1})f_t\n",
    "\n",
    "# 3. 实验设计\n",
    "def prepare_data(Z, r, test_size=0.2, val_size=0.2):\n",
    "    Z_train_val, Z_test, r_train_val, r_test = train_test_split(Z, r, test_size=test_size, random_state=42)\n",
    "    Z_train, Z_val, r_train, r_val = train_test_split(Z_train_val, r_train_val, test_size=val_size, random_state=42)\n",
    "    return (Z_train, r_train), (Z_val, r_val), (Z_test, r_test)\n",
    "\n",
    "def train_model(model, train_data, val_data, num_epochs=200, batch_size=128, patience=20):\n",
    "    Z_train, r_train = train_data\n",
    "    Z_val, r_val = val_data\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = tf.keras.metrics.Mean()\n",
    "        for i in range(0, len(Z_train), batch_size):\n",
    "            batch_Z = Z_train[i:i+batch_size]\n",
    "            batch_r = r_train[i:i+batch_size]\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_Z, training=True)\n",
    "                loss = tf.reduce_mean(tf.square(batch_r - predictions))\n",
    "                # 添加L2正则化损失\n",
    "                l2_loss = sum(tf.nn.l2_loss(v) for v in model.trainable_variables if 'kernel' in v.name)\n",
    "                total_loss = loss + 0.01 * l2_loss\n",
    "            gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            train_loss.update_state(loss)\n",
    "\n",
    "        # Validation\n",
    "        val_predictions = model(Z_val, training=False)\n",
    "        val_loss = tf.reduce_mean(tf.square(r_val - val_predictions))\n",
    "\n",
    "        train_losses.append(train_loss.result().numpy())\n",
    "        val_losses.append(val_loss.numpy())\n",
    "\n",
    "        logging.info(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.result():.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # 早停\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                logging.info(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # 学习率调整\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            optimizer.learning_rate = optimizer.learning_rate * 0.9\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 提取共同因子的函数\n",
    "def extract_common_factors(model, Z):\n",
    "    beta = model.beta_net(Z).numpy()\n",
    "    factor_weights = model.factor_net.weights[0].numpy()\n",
    "    common_factors = np.einsum('ij,kj->ik', beta, factor_weights.T)\n",
    "    return common_factors\n",
    "\n",
    "# 时间序列交叉验证\n",
    "def time_series_cv(Z, r, n_splits=3):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_scores = []\n",
    "    cv_results = []\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(Z)):\n",
    "        Z_train, Z_test = Z[train_index], Z[test_index]\n",
    "        r_train, r_test = r[train_index], r[test_index]\n",
    "        \n",
    "        model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=30)\n",
    "        train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_test, r_test))\n",
    "        \n",
    "        # 评估模型\n",
    "        common_factors = extract_common_factors(model, Z_test)\n",
    "        scaler = StandardScaler()\n",
    "        common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "        X = sm.add_constant(common_factors_scaled)\n",
    "        y = r_test\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        results = ols_model.fit()\n",
    "        \n",
    "        cv_scores.append(results.rsquared_adj)\n",
    "        cv_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'r_squared': results.rsquared,\n",
    "            'adj_r_squared': results.rsquared_adj,\n",
    "            'aic': results.aic,\n",
    "            'bic': results.bic\n",
    "        })\n",
    "    \n",
    "    return cv_scores, cv_results\n",
    "\n",
    "# 样本外测试\n",
    "def out_of_sample_test(Z, r, train_ratio=0.6, val_ratio=0.2):\n",
    "    n = len(Z)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    Z_train, r_train = Z[:train_end], r[:train_end]\n",
    "    Z_val, r_val = Z[train_end:val_end], r[train_end:val_end]\n",
    "    Z_test, r_test = Z[val_end:], r[val_end:]\n",
    "    \n",
    "    model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=30)\n",
    "    train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_val, r_val))\n",
    "    \n",
    "    # 在样本外数据上评估模型\n",
    "    common_factors = extract_common_factors(model, Z_test)\n",
    "    scaler = StandardScaler()\n",
    "    common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "    X = sm.add_constant(common_factors_scaled)\n",
    "    y = r_test\n",
    "    ols_model = sm.OLS(y, X)\n",
    "    results = ols_model.fit()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 可视化函数\n",
    "def plot_cv_results(cv_results):\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(data=df[['r_squared', 'adj_r_squared']])\n",
    "    plt.title('Cross-Validation Results: R-squared and Adjusted R-squared')\n",
    "    plt.savefig('cv_results_boxplot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data=df, x='fold', y='adj_r_squared')\n",
    "    plt.title('Adjusted R-squared across CV Folds')\n",
    "    plt.savefig('cv_results_scatter.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_oos_results(oos_results):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(oos_results.fittedvalues, oos_results.resid)\n",
    "    plt.xlabel('Fitted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Out-of-Sample Test: Residuals vs Fitted')\n",
    "    plt.savefig('oos_residuals_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sm.graphics.plot_regress_exog(oos_results, 'x1', fig=plt.gcf())\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('oos_regress_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    try:\n",
    "        # 文件路径与数据导入\n",
    "        file_path = '/Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/DL_Data7.csv'\n",
    "        Z, r = load_and_preprocess_data(file_path)\n",
    "\n",
    "        # 原始模型训练和评估\n",
    "        train_data, val_data, test_data = prepare_data(Z, r)\n",
    "        Z_train, r_train = train_data\n",
    "        Z_val, r_val = val_data\n",
    "        Z_test, r_test = test_data\n",
    "\n",
    "        model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=30)\n",
    "        train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_val, r_val))\n",
    "\n",
    "        # 提取测试集的共同因子\n",
    "        common_factors = extract_common_factors(model, Z_test)\n",
    "        scaler = StandardScaler()\n",
    "        common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "\n",
    "        # 准备回归数据\n",
    "        X = sm.add_constant(common_factors_scaled)\n",
    "        y = r_test\n",
    "\n",
    "        # 进行OLS回归\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        results = ols_model.fit()\n",
    "\n",
    "        # 输出回归结果\n",
    "        print(results.summary())\n",
    "\n",
    "        # 保存共同因子数据\n",
    "        factor_df = pd.DataFrame(common_factors_scaled, columns=[f'Factor_{i+1}' for i in range(common_factors_scaled.shape[1])])\n",
    "        factor_df['Return'] = r_test\n",
    "        factor_df.to_csv('common_factors_and_returns.csv', index=False)\n",
    "\n",
    "        print(\"共同因子数据已保存到 'common_factors_and_returns.csv'\")\n",
    "\n",
    "        # 计算调整后的R方\n",
    "        adjusted_r_squared = results.rsquared_adj\n",
    "        print(f\"调整后的R方: {adjusted_r_squared:.4f}\")\n",
    "\n",
    "        # 绘制训练和验证损失\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('loss_plot.png')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"损失图表已保存为 'loss_plot.png'\")\n",
    "\n",
    "        # 添加时间序列交叉验证\n",
    "        cv_scores, cv_results = time_series_cv(Z, r)\n",
    "        print(f\"交叉验证 R² 分数: {np.mean(cv_scores):.4f} (±{np.std(cv_scores):.4f})\")\n",
    "        \n",
    "        # 可视化交叉验证结果\n",
    "        plot_cv_results(cv_results)\n",
    "        print(\"交叉验证结果图表已保存\")\n",
    "\n",
    "\n",
    "        # 添加样本外测试\n",
    "        oos_results = out_of_sample_test(Z, r)\n",
    "        print(f\"样本外测试 R²: {oos_results.rsquared:.4f}\")\n",
    "        print(f\"样本外测试调整后 R²: {oos_results.rsquared_adj:.4f}\")\n",
    "        \n",
    "        # 输出样本外测试的完整回归结果\n",
    "        print(\"\\n样本外测试回归结果:\")\n",
    "        print(oos_results.summary())\n",
    "\n",
    "        # 可视化样本外测试结果\n",
    "        plot_oos_results(oos_results)\n",
    "        print(\"样本外测试结果图表已保存\")\n",
    "\n",
    "        # 保存交叉验证和样本外测试结果\n",
    "        cv_df = pd.DataFrame(cv_results)\n",
    "        cv_df.to_csv('cross_validation_results.csv', index=False)\n",
    "        print(\"交叉验证结果已保存到 'cross_validation_results.csv'\")\n",
    "\n",
    "        oos_df = pd.DataFrame({\n",
    "            'Actual': oos_results.model.endog,\n",
    "            'Predicted': oos_results.fittedvalues,\n",
    "            'Residuals': oos_results.resid\n",
    "        })\n",
    "        oos_df.to_csv('out_of_sample_results.csv', index=False)\n",
    "        print(\"样本外测试结果已保存到 'out_of_sample_results.csv'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e34c3a-1ec6-44cb-8d7f-c4514cff1a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
