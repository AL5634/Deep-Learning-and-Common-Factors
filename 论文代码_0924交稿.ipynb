{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200073e6-e6af-44d9-aea8-3037e04a44c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 13:08:32.459884: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 13:08:36,891 - INFO - Successfully loaded data from /Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/DL_Data7.csv\n",
      "2024-09-24 13:08:41,076 - INFO - Expanded features from 32 to 528\n",
      "2024-09-24 13:08:49,717 - INFO - Filled NaN values with mean\n",
      "2024-09-24 13:08:58.796856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 13:09:37,020 - INFO - Epoch [1/200], Train Loss: 0.5125, Val Loss: 0.0058\n",
      "2024-09-24 13:10:15,669 - INFO - Epoch [2/200], Train Loss: 0.0069, Val Loss: 0.0093\n",
      "2024-09-24 13:10:54,180 - INFO - Epoch [3/200], Train Loss: 0.0031, Val Loss: 0.0147\n",
      "2024-09-24 13:11:32,412 - INFO - Epoch [4/200], Train Loss: 0.0034, Val Loss: 0.0067\n",
      "2024-09-24 13:12:10,569 - INFO - Epoch [5/200], Train Loss: 0.0034, Val Loss: 0.0046\n",
      "2024-09-24 13:12:48,615 - INFO - Epoch [6/200], Train Loss: 0.0035, Val Loss: 0.0055\n",
      "2024-09-24 13:13:26,616 - INFO - Epoch [7/200], Train Loss: 0.0035, Val Loss: 0.0041\n",
      "2024-09-24 13:14:04,580 - INFO - Epoch [8/200], Train Loss: 0.0035, Val Loss: 0.0293\n",
      "2024-09-24 13:14:42,554 - INFO - Epoch [9/200], Train Loss: 0.0035, Val Loss: 0.0047\n",
      "2024-09-24 13:15:21,312 - INFO - Epoch [10/200], Train Loss: 0.0035, Val Loss: 0.0066\n",
      "2024-09-24 13:15:59,582 - INFO - Epoch [11/200], Train Loss: 0.0035, Val Loss: 0.0077\n",
      "2024-09-24 13:16:39,907 - INFO - Epoch [12/200], Train Loss: 0.0034, Val Loss: 0.0070\n",
      "2024-09-24 13:17:19,203 - INFO - Epoch [13/200], Train Loss: 0.0034, Val Loss: 0.0043\n",
      "2024-09-24 13:17:58,096 - INFO - Epoch [14/200], Train Loss: 0.0034, Val Loss: 0.0058\n",
      "2024-09-24 13:18:43,719 - INFO - Epoch [15/200], Train Loss: 0.0033, Val Loss: 0.0271\n",
      "2024-09-24 13:19:24,133 - INFO - Epoch [16/200], Train Loss: 0.0034, Val Loss: 0.0068\n",
      "2024-09-24 13:20:03,408 - INFO - Epoch [17/200], Train Loss: 0.0034, Val Loss: 0.0043\n",
      "2024-09-24 13:20:42,310 - INFO - Epoch [18/200], Train Loss: 0.0033, Val Loss: 0.0056\n",
      "2024-09-24 13:21:21,169 - INFO - Epoch [19/200], Train Loss: 0.0034, Val Loss: 0.0044\n",
      "2024-09-24 13:22:00,446 - INFO - Epoch [20/200], Train Loss: 0.0035, Val Loss: 0.0059\n",
      "2024-09-24 13:22:41,391 - INFO - Epoch [21/200], Train Loss: 0.0034, Val Loss: 0.0764\n",
      "2024-09-24 13:23:22,583 - INFO - Epoch [22/200], Train Loss: 0.0033, Val Loss: 0.0046\n",
      "2024-09-24 13:24:01,575 - INFO - Epoch [23/200], Train Loss: 0.0033, Val Loss: 0.0046\n",
      "2024-09-24 13:24:40,087 - INFO - Epoch [24/200], Train Loss: 0.0033, Val Loss: 0.0051\n",
      "2024-09-24 13:25:19,482 - INFO - Epoch [25/200], Train Loss: 0.0034, Val Loss: 0.0046\n",
      "2024-09-24 13:26:03,104 - INFO - Epoch [26/200], Train Loss: 0.0033, Val Loss: 0.0051\n",
      "2024-09-24 13:26:43,941 - INFO - Epoch [27/200], Train Loss: 0.0033, Val Loss: 0.0045\n",
      "2024-09-24 13:26:43,942 - INFO - Early stopping triggered at epoch 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.846\n",
      "Model:                            OLS   Adj. R-squared:                  0.846\n",
      "Method:                 Least Squares   F-statistic:                 3.309e+04\n",
      "Date:                Tue, 24 Sep 2024   Prob (F-statistic):               0.00\n",
      "Time:                        13:26:44   Log-Likelihood:             2.8214e+05\n",
      "No. Observations:              180354   AIC:                        -5.642e+05\n",
      "Df Residuals:                  180323   BIC:                        -5.639e+05\n",
      "Df Model:                          30                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4533      0.000   3698.453      0.000       0.453       0.454\n",
      "x1          -1.17e+04    760.321    -15.392      0.000   -1.32e+04   -1.02e+04\n",
      "x2         -4599.7864    701.359     -6.558      0.000   -5974.435   -3225.138\n",
      "x3          6276.7966    787.490      7.971      0.000    4733.334    7820.259\n",
      "x4          2537.0029    672.769      3.771      0.000    1218.391    3855.615\n",
      "x5          6042.8308    782.201      7.725      0.000    4509.735    7575.926\n",
      "x6         -2105.3209    688.978     -3.056      0.002   -3455.702    -754.940\n",
      "x7          2088.3810    726.223      2.876      0.004     665.000    3511.762\n",
      "x8         -5001.8840    701.042     -7.135      0.000   -6375.909   -3627.859\n",
      "x9          -247.5752    681.173     -0.363      0.716   -1582.659    1087.509\n",
      "x10         8111.4919    753.545     10.764      0.000    6634.561    9588.423\n",
      "x11         6990.6512    678.990     10.296      0.000    5659.847    8321.456\n",
      "x12         5708.5366    687.030      8.309      0.000    4361.973    7055.101\n",
      "x13          -94.1621    735.574     -0.128      0.898   -1535.870    1347.546\n",
      "x14         4260.3896    693.547      6.143      0.000    2901.053    5619.727\n",
      "x15         1791.4440    693.292      2.584      0.010     432.607    3150.281\n",
      "x16        -3658.1111    677.130     -5.402      0.000   -4985.271   -2330.952\n",
      "x17        -5317.8543    732.467     -7.260      0.000   -6753.472   -3882.237\n",
      "x18         1138.8964    777.825      1.464      0.143    -385.624    2663.417\n",
      "x19        -1136.1637    790.518     -1.437      0.151   -2685.561     413.233\n",
      "x20        -7681.1292    768.928     -9.989      0.000   -9188.211   -6174.047\n",
      "x21        -3992.8593    693.006     -5.762      0.000   -5351.135   -2634.584\n",
      "x22        -8694.4325    783.970    -11.090      0.000   -1.02e+04   -7157.869\n",
      "x23        -6962.6883    770.924     -9.032      0.000   -8473.681   -5451.696\n",
      "x24         3692.6906    794.907      4.645      0.000    2134.691    5250.691\n",
      "x25        -2327.9683    778.339     -2.991      0.003   -3853.494    -802.442\n",
      "x26         -275.1704    782.949     -0.351      0.725   -1809.732    1259.391\n",
      "x27        -4288.7768    751.437     -5.707      0.000   -5761.575   -2815.978\n",
      "x28         2302.6120    778.944      2.956      0.003     775.900    3829.324\n",
      "x29        -7612.7947    761.454     -9.998      0.000   -9105.228   -6120.361\n",
      "x30           -5.2463    690.419     -0.008      0.994   -1358.452    1347.959\n",
      "==============================================================================\n",
      "Omnibus:                   173279.803   Durbin-Watson:                   2.001\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        155259626.348\n",
      "Skew:                           3.687   Prob(JB):                         0.00\n",
      "Kurtosis:                     146.549   Cond. No.                     3.77e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.77e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "共同因子数据已保存到 'common_factors_and_returns.csv'\n",
      "调整后的R方: 0.8463\n",
      "损失图表已保存为 'loss_plot.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 13:27:06,477 - INFO - Epoch [1/200], Train Loss: 2.1506, Val Loss: 0.0513\n",
      "2024-09-24 13:27:24,115 - INFO - Epoch [2/200], Train Loss: 0.0345, Val Loss: 0.5829\n",
      "2024-09-24 13:27:41,822 - INFO - Epoch [3/200], Train Loss: 0.0713, Val Loss: 0.1660\n",
      "2024-09-24 13:27:59,838 - INFO - Epoch [4/200], Train Loss: 0.0563, Val Loss: 0.1339\n",
      "2024-09-24 13:28:17,845 - INFO - Epoch [5/200], Train Loss: 0.0365, Val Loss: 0.0558\n",
      "2024-09-24 13:28:35,598 - INFO - Epoch [6/200], Train Loss: 0.0251, Val Loss: 0.0602\n",
      "2024-09-24 13:28:53,232 - INFO - Epoch [7/200], Train Loss: 0.0200, Val Loss: 0.0252\n",
      "2024-09-24 13:29:10,633 - INFO - Epoch [8/200], Train Loss: 0.0187, Val Loss: 0.0173\n",
      "2024-09-24 13:29:28,152 - INFO - Epoch [9/200], Train Loss: 0.0187, Val Loss: 0.0107\n",
      "2024-09-24 13:29:45,498 - INFO - Epoch [10/200], Train Loss: 0.0186, Val Loss: 0.0125\n",
      "2024-09-24 13:30:02,903 - INFO - Epoch [11/200], Train Loss: 0.0187, Val Loss: 0.0132\n",
      "2024-09-24 13:30:20,494 - INFO - Epoch [12/200], Train Loss: 0.0185, Val Loss: 0.0117\n",
      "2024-09-24 13:30:37,854 - INFO - Epoch [13/200], Train Loss: 0.0186, Val Loss: 0.0117\n",
      "2024-09-24 13:30:55,345 - INFO - Epoch [14/200], Train Loss: 0.0186, Val Loss: 0.0128\n",
      "2024-09-24 13:31:12,228 - INFO - Epoch [15/200], Train Loss: 0.0186, Val Loss: 0.0123\n",
      "2024-09-24 13:31:28,887 - INFO - Epoch [16/200], Train Loss: 0.0186, Val Loss: 0.0120\n",
      "2024-09-24 13:31:45,437 - INFO - Epoch [17/200], Train Loss: 0.0186, Val Loss: 0.0119\n",
      "2024-09-24 13:32:01,946 - INFO - Epoch [18/200], Train Loss: 0.0185, Val Loss: 0.0114\n",
      "2024-09-24 13:32:18,381 - INFO - Epoch [19/200], Train Loss: 0.0186, Val Loss: 0.0122\n",
      "2024-09-24 13:32:34,913 - INFO - Epoch [20/200], Train Loss: 0.0186, Val Loss: 0.0115\n",
      "2024-09-24 13:32:51,818 - INFO - Epoch [21/200], Train Loss: 0.0185, Val Loss: 0.0123\n",
      "2024-09-24 13:33:08,444 - INFO - Epoch [22/200], Train Loss: 0.0184, Val Loss: 0.0121\n",
      "2024-09-24 13:33:25,569 - INFO - Epoch [23/200], Train Loss: 0.0185, Val Loss: 0.0109\n",
      "2024-09-24 13:33:43,001 - INFO - Epoch [24/200], Train Loss: 0.0184, Val Loss: 0.0108\n",
      "2024-09-24 13:34:00,625 - INFO - Epoch [25/200], Train Loss: 0.0184, Val Loss: 0.0108\n",
      "2024-09-24 13:34:18,195 - INFO - Epoch [26/200], Train Loss: 0.0184, Val Loss: 0.0115\n",
      "2024-09-24 13:34:34,863 - INFO - Epoch [27/200], Train Loss: 0.0184, Val Loss: 0.0136\n",
      "2024-09-24 13:34:52,188 - INFO - Epoch [28/200], Train Loss: 0.0184, Val Loss: 0.0113\n",
      "2024-09-24 13:35:09,398 - INFO - Epoch [29/200], Train Loss: 0.0184, Val Loss: 0.0120\n",
      "2024-09-24 13:35:09,399 - INFO - Early stopping triggered at epoch 29\n",
      "2024-09-24 13:35:43,095 - INFO - Epoch [1/200], Train Loss: 1.6874, Val Loss: 0.1036\n",
      "2024-09-24 13:36:14,694 - INFO - Epoch [2/200], Train Loss: 0.0687, Val Loss: 0.1034\n",
      "2024-09-24 13:36:45,875 - INFO - Epoch [3/200], Train Loss: 0.0327, Val Loss: 0.0147\n",
      "2024-09-24 13:37:17,931 - INFO - Epoch [4/200], Train Loss: 0.0173, Val Loss: 0.0134\n",
      "2024-09-24 13:37:48,442 - INFO - Epoch [5/200], Train Loss: 0.0160, Val Loss: 0.0104\n",
      "2024-09-24 13:38:19,193 - INFO - Epoch [6/200], Train Loss: 0.0160, Val Loss: 0.0124\n",
      "2024-09-24 13:38:49,860 - INFO - Epoch [7/200], Train Loss: 0.0160, Val Loss: 0.0106\n",
      "2024-09-24 13:39:21,055 - INFO - Epoch [8/200], Train Loss: 0.0160, Val Loss: 0.0118\n",
      "2024-09-24 13:39:52,078 - INFO - Epoch [9/200], Train Loss: 0.0160, Val Loss: 0.0107\n",
      "2024-09-24 13:40:23,268 - INFO - Epoch [10/200], Train Loss: 0.0160, Val Loss: 0.0116\n",
      "2024-09-24 13:40:54,090 - INFO - Epoch [11/200], Train Loss: 0.0160, Val Loss: 0.0121\n",
      "2024-09-24 13:41:24,415 - INFO - Epoch [12/200], Train Loss: 0.0159, Val Loss: 0.0108\n",
      "2024-09-24 13:41:55,159 - INFO - Epoch [13/200], Train Loss: 0.0159, Val Loss: 0.0106\n",
      "2024-09-24 13:42:26,324 - INFO - Epoch [14/200], Train Loss: 0.0159, Val Loss: 0.0109\n",
      "2024-09-24 13:42:57,447 - INFO - Epoch [15/200], Train Loss: 0.0158, Val Loss: 0.0129\n",
      "2024-09-24 13:43:28,371 - INFO - Epoch [16/200], Train Loss: 0.0159, Val Loss: 0.0120\n",
      "2024-09-24 13:43:59,562 - INFO - Epoch [17/200], Train Loss: 0.0159, Val Loss: 0.0113\n",
      "2024-09-24 13:44:30,822 - INFO - Epoch [18/200], Train Loss: 0.0158, Val Loss: 0.0107\n",
      "2024-09-24 13:45:01,831 - INFO - Epoch [19/200], Train Loss: 0.0158, Val Loss: 0.0115\n",
      "2024-09-24 13:45:32,643 - INFO - Epoch [20/200], Train Loss: 0.0158, Val Loss: 0.0105\n",
      "2024-09-24 13:46:03,217 - INFO - Epoch [21/200], Train Loss: 0.0158, Val Loss: 0.0103\n",
      "2024-09-24 13:46:34,118 - INFO - Epoch [22/200], Train Loss: 0.0157, Val Loss: 0.0113\n",
      "2024-09-24 13:47:04,940 - INFO - Epoch [23/200], Train Loss: 0.0156, Val Loss: 0.0087\n",
      "2024-09-24 13:47:36,232 - INFO - Epoch [24/200], Train Loss: 0.0156, Val Loss: 0.0104\n",
      "2024-09-24 13:48:06,957 - INFO - Epoch [25/200], Train Loss: 0.0157, Val Loss: 0.0092\n",
      "2024-09-24 13:48:37,775 - INFO - Epoch [26/200], Train Loss: 0.0156, Val Loss: 0.0107\n",
      "2024-09-24 13:49:08,795 - INFO - Epoch [27/200], Train Loss: 0.0156, Val Loss: 0.0088\n",
      "2024-09-24 13:49:39,701 - INFO - Epoch [28/200], Train Loss: 0.0156, Val Loss: 0.0097\n",
      "2024-09-24 13:50:10,610 - INFO - Epoch [29/200], Train Loss: 0.0156, Val Loss: 0.0112\n",
      "2024-09-24 13:50:41,307 - INFO - Epoch [30/200], Train Loss: 0.0156, Val Loss: 0.0093\n",
      "2024-09-24 13:51:11,728 - INFO - Epoch [31/200], Train Loss: 0.0156, Val Loss: 0.0098\n",
      "2024-09-24 13:51:42,322 - INFO - Epoch [32/200], Train Loss: 0.0155, Val Loss: 0.0089\n",
      "2024-09-24 13:52:12,824 - INFO - Epoch [33/200], Train Loss: 0.0155, Val Loss: 0.0108\n",
      "2024-09-24 13:52:43,578 - INFO - Epoch [34/200], Train Loss: 0.0155, Val Loss: 0.0094\n",
      "2024-09-24 13:53:14,499 - INFO - Epoch [35/200], Train Loss: 0.0154, Val Loss: 0.0091\n",
      "2024-09-24 13:53:45,548 - INFO - Epoch [36/200], Train Loss: 0.0154, Val Loss: 0.0082\n",
      "2024-09-24 13:54:16,125 - INFO - Epoch [37/200], Train Loss: 0.0154, Val Loss: 0.0086\n",
      "2024-09-24 13:54:46,490 - INFO - Epoch [38/200], Train Loss: 0.0154, Val Loss: 0.0087\n",
      "2024-09-24 13:55:17,601 - INFO - Epoch [39/200], Train Loss: 0.0154, Val Loss: 0.0095\n",
      "2024-09-24 13:55:48,326 - INFO - Epoch [40/200], Train Loss: 0.0154, Val Loss: 0.0091\n",
      "2024-09-24 13:56:19,128 - INFO - Epoch [41/200], Train Loss: 0.0154, Val Loss: 0.0096\n",
      "2024-09-24 13:56:49,852 - INFO - Epoch [42/200], Train Loss: 0.0153, Val Loss: 0.0095\n",
      "2024-09-24 13:57:21,356 - INFO - Epoch [43/200], Train Loss: 0.0153, Val Loss: 0.0105\n",
      "2024-09-24 13:57:52,610 - INFO - Epoch [44/200], Train Loss: 0.0153, Val Loss: 0.0098\n",
      "2024-09-24 13:58:22,499 - INFO - Epoch [45/200], Train Loss: 0.0153, Val Loss: 0.0094\n",
      "2024-09-24 13:58:52,763 - INFO - Epoch [46/200], Train Loss: 0.0153, Val Loss: 0.0107\n",
      "2024-09-24 13:59:22,910 - INFO - Epoch [47/200], Train Loss: 0.0153, Val Loss: 0.0073\n",
      "2024-09-24 13:59:52,969 - INFO - Epoch [48/200], Train Loss: 0.0153, Val Loss: 0.0094\n",
      "2024-09-24 14:00:28,724 - INFO - Epoch [49/200], Train Loss: 0.0153, Val Loss: 0.0097\n",
      "2024-09-24 14:00:59,046 - INFO - Epoch [50/200], Train Loss: 0.0153, Val Loss: 0.0087\n",
      "2024-09-24 14:01:30,771 - INFO - Epoch [51/200], Train Loss: 0.0153, Val Loss: 0.0099\n",
      "2024-09-24 14:02:01,496 - INFO - Epoch [52/200], Train Loss: 0.0152, Val Loss: 0.0094\n",
      "2024-09-24 14:02:31,975 - INFO - Epoch [53/200], Train Loss: 0.0152, Val Loss: 0.0097\n",
      "2024-09-24 14:03:02,217 - INFO - Epoch [54/200], Train Loss: 0.0153, Val Loss: 0.0105\n",
      "2024-09-24 14:03:33,187 - INFO - Epoch [55/200], Train Loss: 0.0152, Val Loss: 0.0098\n",
      "2024-09-24 14:04:03,209 - INFO - Epoch [56/200], Train Loss: 0.0152, Val Loss: 0.0088\n",
      "2024-09-24 14:04:33,651 - INFO - Epoch [57/200], Train Loss: 0.0152, Val Loss: 0.0081\n",
      "2024-09-24 14:05:03,869 - INFO - Epoch [58/200], Train Loss: 0.0152, Val Loss: 0.0118\n",
      "2024-09-24 14:05:34,388 - INFO - Epoch [59/200], Train Loss: 0.0152, Val Loss: 0.0097\n",
      "2024-09-24 14:06:04,734 - INFO - Epoch [60/200], Train Loss: 0.0152, Val Loss: 0.0074\n",
      "2024-09-24 14:06:35,100 - INFO - Epoch [61/200], Train Loss: 0.0152, Val Loss: 0.0088\n",
      "2024-09-24 14:07:05,921 - INFO - Epoch [62/200], Train Loss: 0.0151, Val Loss: 0.0079\n",
      "2024-09-24 14:07:36,698 - INFO - Epoch [63/200], Train Loss: 0.0151, Val Loss: 0.0081\n",
      "2024-09-24 14:08:06,984 - INFO - Epoch [64/200], Train Loss: 0.0151, Val Loss: 0.0088\n",
      "2024-09-24 14:08:37,786 - INFO - Epoch [65/200], Train Loss: 0.0151, Val Loss: 0.0106\n",
      "2024-09-24 14:09:08,803 - INFO - Epoch [66/200], Train Loss: 0.0152, Val Loss: 0.0095\n",
      "2024-09-24 14:09:39,068 - INFO - Epoch [67/200], Train Loss: 0.0151, Val Loss: 0.0088\n",
      "2024-09-24 14:09:39,068 - INFO - Early stopping triggered at epoch 67\n",
      "2024-09-24 14:10:26,226 - INFO - Epoch [1/200], Train Loss: 1.1480, Val Loss: 0.4073\n",
      "2024-09-24 14:11:11,105 - INFO - Epoch [2/200], Train Loss: 0.0461, Val Loss: 0.0278\n",
      "2024-09-24 14:11:55,860 - INFO - Epoch [3/200], Train Loss: 0.0161, Val Loss: 0.0159\n",
      "2024-09-24 14:12:40,539 - INFO - Epoch [4/200], Train Loss: 0.0151, Val Loss: 0.0195\n",
      "2024-09-24 14:13:25,279 - INFO - Epoch [5/200], Train Loss: 0.0152, Val Loss: 0.0184\n",
      "2024-09-24 14:14:09,856 - INFO - Epoch [6/200], Train Loss: 0.0152, Val Loss: 0.0180\n",
      "2024-09-24 14:14:55,133 - INFO - Epoch [7/200], Train Loss: 0.0152, Val Loss: 0.0157\n",
      "2024-09-24 14:15:40,205 - INFO - Epoch [8/200], Train Loss: 0.0151, Val Loss: 0.0186\n",
      "2024-09-24 14:16:25,146 - INFO - Epoch [9/200], Train Loss: 0.0151, Val Loss: 0.0165\n",
      "2024-09-24 14:17:10,532 - INFO - Epoch [10/200], Train Loss: 0.0151, Val Loss: 0.0138\n",
      "2024-09-24 14:17:57,392 - INFO - Epoch [11/200], Train Loss: 0.0150, Val Loss: 0.0151\n",
      "2024-09-24 14:18:43,132 - INFO - Epoch [12/200], Train Loss: 0.0148, Val Loss: 0.0157\n",
      "2024-09-24 14:19:36,381 - INFO - Epoch [13/200], Train Loss: 0.0147, Val Loss: 0.0159\n",
      "2024-09-24 14:20:22,917 - INFO - Epoch [14/200], Train Loss: 0.0147, Val Loss: 0.0133\n",
      "2024-09-24 14:21:08,345 - INFO - Epoch [15/200], Train Loss: 0.0147, Val Loss: 0.0166\n",
      "2024-09-24 14:21:53,916 - INFO - Epoch [16/200], Train Loss: 0.0147, Val Loss: 0.0146\n",
      "2024-09-24 14:22:41,820 - INFO - Epoch [17/200], Train Loss: 0.0146, Val Loss: 0.0136\n",
      "2024-09-24 14:23:29,045 - INFO - Epoch [18/200], Train Loss: 0.0146, Val Loss: 0.0163\n",
      "2024-09-24 14:24:14,186 - INFO - Epoch [19/200], Train Loss: 0.0146, Val Loss: 0.0134\n",
      "2024-09-24 14:25:00,440 - INFO - Epoch [20/200], Train Loss: 0.0146, Val Loss: 0.0133\n",
      "2024-09-24 14:25:47,028 - INFO - Epoch [21/200], Train Loss: 0.0146, Val Loss: 0.0152\n",
      "2024-09-24 14:26:33,527 - INFO - Epoch [22/200], Train Loss: 0.0145, Val Loss: 0.0138\n",
      "2024-09-24 14:27:19,955 - INFO - Epoch [23/200], Train Loss: 0.0145, Val Loss: 0.0135\n",
      "2024-09-24 14:28:06,281 - INFO - Epoch [24/200], Train Loss: 0.0145, Val Loss: 0.0119\n",
      "2024-09-24 14:28:52,368 - INFO - Epoch [25/200], Train Loss: 0.0144, Val Loss: 0.0135\n",
      "2024-09-24 14:29:38,119 - INFO - Epoch [26/200], Train Loss: 0.0144, Val Loss: 0.0127\n",
      "2024-09-24 14:30:23,029 - INFO - Epoch [27/200], Train Loss: 0.0145, Val Loss: 0.0130\n",
      "2024-09-24 14:31:13,477 - INFO - Epoch [28/200], Train Loss: 0.0145, Val Loss: 0.0130\n",
      "2024-09-24 14:32:02,909 - INFO - Epoch [29/200], Train Loss: 0.0144, Val Loss: 0.0129\n",
      "2024-09-24 14:32:51,627 - INFO - Epoch [30/200], Train Loss: 0.0144, Val Loss: 0.0132\n",
      "2024-09-24 14:33:40,148 - INFO - Epoch [31/200], Train Loss: 0.0144, Val Loss: 0.0125\n",
      "2024-09-24 14:34:28,433 - INFO - Epoch [32/200], Train Loss: 0.0143, Val Loss: 0.0133\n",
      "2024-09-24 14:35:16,816 - INFO - Epoch [33/200], Train Loss: 0.0143, Val Loss: 0.0145\n",
      "2024-09-24 14:36:04,314 - INFO - Epoch [34/200], Train Loss: 0.0143, Val Loss: 0.0124\n",
      "2024-09-24 14:36:53,394 - INFO - Epoch [35/200], Train Loss: 0.0143, Val Loss: 0.0129\n",
      "2024-09-24 14:37:41,254 - INFO - Epoch [36/200], Train Loss: 0.0143, Val Loss: 0.0118\n",
      "2024-09-24 14:38:29,493 - INFO - Epoch [37/200], Train Loss: 0.0143, Val Loss: 0.0111\n",
      "2024-09-24 14:39:18,922 - INFO - Epoch [38/200], Train Loss: 0.0143, Val Loss: 0.0119\n",
      "2024-09-24 14:40:06,757 - INFO - Epoch [39/200], Train Loss: 0.0143, Val Loss: 0.0134\n",
      "2024-09-24 14:40:55,502 - INFO - Epoch [40/200], Train Loss: 0.0143, Val Loss: 0.0118\n",
      "2024-09-24 14:41:43,614 - INFO - Epoch [41/200], Train Loss: 0.0143, Val Loss: 0.0139\n",
      "2024-09-24 14:42:39,197 - INFO - Epoch [42/200], Train Loss: 0.0143, Val Loss: 0.0138\n",
      "2024-09-24 14:43:28,038 - INFO - Epoch [43/200], Train Loss: 0.0142, Val Loss: 0.0129\n",
      "2024-09-24 14:44:16,899 - INFO - Epoch [44/200], Train Loss: 0.0142, Val Loss: 0.0142\n",
      "2024-09-24 14:45:05,479 - INFO - Epoch [45/200], Train Loss: 0.0142, Val Loss: 0.0132\n",
      "2024-09-24 14:45:53,630 - INFO - Epoch [46/200], Train Loss: 0.0142, Val Loss: 0.0119\n",
      "2024-09-24 14:46:42,005 - INFO - Epoch [47/200], Train Loss: 0.0142, Val Loss: 0.0130\n",
      "2024-09-24 14:47:32,624 - INFO - Epoch [48/200], Train Loss: 0.0142, Val Loss: 0.0128\n",
      "2024-09-24 14:48:23,102 - INFO - Epoch [49/200], Train Loss: 0.0142, Val Loss: 0.0133\n",
      "2024-09-24 14:49:18,504 - INFO - Epoch [50/200], Train Loss: 0.0142, Val Loss: 0.0122\n",
      "2024-09-24 14:50:07,513 - INFO - Epoch [51/200], Train Loss: 0.0142, Val Loss: 0.0130\n",
      "2024-09-24 14:50:55,319 - INFO - Epoch [52/200], Train Loss: 0.0142, Val Loss: 0.0115\n",
      "2024-09-24 14:51:45,247 - INFO - Epoch [53/200], Train Loss: 0.0141, Val Loss: 0.0113\n",
      "2024-09-24 14:52:34,230 - INFO - Epoch [54/200], Train Loss: 0.0142, Val Loss: 0.0118\n",
      "2024-09-24 14:53:23,392 - INFO - Epoch [55/200], Train Loss: 0.0142, Val Loss: 0.0122\n",
      "2024-09-24 14:54:11,488 - INFO - Epoch [56/200], Train Loss: 0.0142, Val Loss: 0.0085\n",
      "2024-09-24 14:55:08,844 - INFO - Epoch [57/200], Train Loss: 0.0142, Val Loss: 0.0098\n",
      "2024-09-24 14:55:57,068 - INFO - Epoch [58/200], Train Loss: 0.0141, Val Loss: 0.0109\n",
      "2024-09-24 14:56:46,011 - INFO - Epoch [59/200], Train Loss: 0.0141, Val Loss: 0.0129\n",
      "2024-09-24 14:57:34,362 - INFO - Epoch [60/200], Train Loss: 0.0141, Val Loss: 0.0106\n",
      "2024-09-24 14:58:23,177 - INFO - Epoch [61/200], Train Loss: 0.0141, Val Loss: 0.0110\n",
      "2024-09-24 14:59:11,112 - INFO - Epoch [62/200], Train Loss: 0.0141, Val Loss: 0.0102\n",
      "2024-09-24 14:59:59,945 - INFO - Epoch [63/200], Train Loss: 0.0141, Val Loss: 0.0083\n",
      "2024-09-24 15:00:48,158 - INFO - Epoch [64/200], Train Loss: 0.0141, Val Loss: 0.0103\n",
      "2024-09-24 15:01:36,477 - INFO - Epoch [65/200], Train Loss: 0.0141, Val Loss: 0.0097\n",
      "2024-09-24 15:02:25,694 - INFO - Epoch [66/200], Train Loss: 0.0141, Val Loss: 0.0108\n",
      "2024-09-24 15:03:15,584 - INFO - Epoch [67/200], Train Loss: 0.0141, Val Loss: 0.0190\n",
      "2024-09-24 15:04:04,327 - INFO - Epoch [68/200], Train Loss: 0.0141, Val Loss: 0.0107\n",
      "2024-09-24 15:04:53,380 - INFO - Epoch [69/200], Train Loss: 0.0141, Val Loss: 0.0116\n",
      "2024-09-24 15:05:42,503 - INFO - Epoch [70/200], Train Loss: 0.0141, Val Loss: 0.0118\n",
      "2024-09-24 15:06:31,539 - INFO - Epoch [71/200], Train Loss: 0.0141, Val Loss: 0.0094\n",
      "2024-09-24 15:07:19,380 - INFO - Epoch [72/200], Train Loss: 0.0141, Val Loss: 0.0104\n",
      "2024-09-24 15:08:07,466 - INFO - Epoch [73/200], Train Loss: 0.0141, Val Loss: 0.0111\n",
      "2024-09-24 15:08:55,667 - INFO - Epoch [74/200], Train Loss: 0.0141, Val Loss: 0.0459\n",
      "2024-09-24 15:09:43,666 - INFO - Epoch [75/200], Train Loss: 0.0141, Val Loss: 0.0118\n",
      "2024-09-24 15:10:32,254 - INFO - Epoch [76/200], Train Loss: 0.0141, Val Loss: 0.0091\n",
      "2024-09-24 15:11:20,963 - INFO - Epoch [77/200], Train Loss: 0.0141, Val Loss: 0.0095\n",
      "2024-09-24 15:12:09,858 - INFO - Epoch [78/200], Train Loss: 0.0141, Val Loss: 0.0111\n",
      "2024-09-24 15:12:58,067 - INFO - Epoch [79/200], Train Loss: 0.0141, Val Loss: 0.0092\n",
      "2024-09-24 15:13:46,295 - INFO - Epoch [80/200], Train Loss: 0.0141, Val Loss: 0.0107\n",
      "2024-09-24 15:14:34,513 - INFO - Epoch [81/200], Train Loss: 0.0141, Val Loss: 0.0095\n",
      "2024-09-24 15:15:23,685 - INFO - Epoch [82/200], Train Loss: 0.0140, Val Loss: 0.0100\n",
      "2024-09-24 15:16:12,861 - INFO - Epoch [83/200], Train Loss: 0.0140, Val Loss: 0.0154\n",
      "2024-09-24 15:16:12,862 - INFO - Early stopping triggered at epoch 83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉验证 R² 分数: 0.4435 (±0.0892)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉验证结果图表已保存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 15:16:54,341 - INFO - Epoch [1/200], Train Loss: 1.5816, Val Loss: 0.1055\n",
      "2024-09-24 15:17:33,768 - INFO - Epoch [2/200], Train Loss: 0.0619, Val Loss: 0.1193\n",
      "2024-09-24 15:18:12,931 - INFO - Epoch [3/200], Train Loss: 0.0266, Val Loss: 0.0164\n",
      "2024-09-24 15:18:52,402 - INFO - Epoch [4/200], Train Loss: 0.0156, Val Loss: 0.0174\n",
      "2024-09-24 15:19:31,614 - INFO - Epoch [5/200], Train Loss: 0.0153, Val Loss: 0.0211\n",
      "2024-09-24 15:20:11,104 - INFO - Epoch [6/200], Train Loss: 0.0155, Val Loss: 0.0205\n",
      "2024-09-24 15:20:50,393 - INFO - Epoch [7/200], Train Loss: 0.0154, Val Loss: 0.0205\n",
      "2024-09-24 15:21:30,425 - INFO - Epoch [8/200], Train Loss: 0.0154, Val Loss: 0.0199\n",
      "2024-09-24 15:22:10,484 - INFO - Epoch [9/200], Train Loss: 0.0155, Val Loss: 0.0205\n",
      "2024-09-24 15:22:50,613 - INFO - Epoch [10/200], Train Loss: 0.0154, Val Loss: 0.0200\n",
      "2024-09-24 15:23:29,900 - INFO - Epoch [11/200], Train Loss: 0.0154, Val Loss: 0.0184\n",
      "2024-09-24 15:24:09,374 - INFO - Epoch [12/200], Train Loss: 0.0153, Val Loss: 0.0188\n",
      "2024-09-24 15:24:48,258 - INFO - Epoch [13/200], Train Loss: 0.0153, Val Loss: 0.0175\n",
      "2024-09-24 15:25:27,500 - INFO - Epoch [14/200], Train Loss: 0.0152, Val Loss: 0.0178\n",
      "2024-09-24 15:26:06,540 - INFO - Epoch [15/200], Train Loss: 0.0152, Val Loss: 0.0204\n",
      "2024-09-24 15:26:45,766 - INFO - Epoch [16/200], Train Loss: 0.0152, Val Loss: 0.0197\n",
      "2024-09-24 15:27:24,802 - INFO - Epoch [17/200], Train Loss: 0.0152, Val Loss: 0.0165\n",
      "2024-09-24 15:28:03,741 - INFO - Epoch [18/200], Train Loss: 0.0151, Val Loss: 0.0190\n",
      "2024-09-24 15:28:43,021 - INFO - Epoch [19/200], Train Loss: 0.0151, Val Loss: 0.0184\n",
      "2024-09-24 15:29:22,119 - INFO - Epoch [20/200], Train Loss: 0.0151, Val Loss: 0.0194\n",
      "2024-09-24 15:30:01,348 - INFO - Epoch [21/200], Train Loss: 0.0151, Val Loss: 0.0185\n",
      "2024-09-24 15:30:41,103 - INFO - Epoch [22/200], Train Loss: 0.0149, Val Loss: 0.0180\n",
      "2024-09-24 15:31:20,118 - INFO - Epoch [23/200], Train Loss: 0.0149, Val Loss: 0.0136\n",
      "2024-09-24 15:31:58,931 - INFO - Epoch [24/200], Train Loss: 0.0149, Val Loss: 0.0163\n",
      "2024-09-24 15:32:37,655 - INFO - Epoch [25/200], Train Loss: 0.0149, Val Loss: 0.0140\n",
      "2024-09-24 15:33:16,374 - INFO - Epoch [26/200], Train Loss: 0.0149, Val Loss: 0.0155\n",
      "2024-09-24 15:33:55,273 - INFO - Epoch [27/200], Train Loss: 0.0149, Val Loss: 0.0165\n",
      "2024-09-24 15:34:34,033 - INFO - Epoch [28/200], Train Loss: 0.0149, Val Loss: 0.0139\n",
      "2024-09-24 15:35:13,005 - INFO - Epoch [29/200], Train Loss: 0.0149, Val Loss: 0.0156\n",
      "2024-09-24 15:35:52,071 - INFO - Epoch [30/200], Train Loss: 0.0149, Val Loss: 0.0148\n",
      "2024-09-24 15:36:31,245 - INFO - Epoch [31/200], Train Loss: 0.0149, Val Loss: 0.0135\n",
      "2024-09-24 15:37:10,480 - INFO - Epoch [32/200], Train Loss: 0.0148, Val Loss: 0.0135\n",
      "2024-09-24 15:37:49,800 - INFO - Epoch [33/200], Train Loss: 0.0148, Val Loss: 0.0150\n",
      "2024-09-24 15:38:29,318 - INFO - Epoch [34/200], Train Loss: 0.0148, Val Loss: 0.0126\n",
      "2024-09-24 15:39:08,558 - INFO - Epoch [35/200], Train Loss: 0.0148, Val Loss: 0.0141\n",
      "2024-09-24 15:39:48,023 - INFO - Epoch [36/200], Train Loss: 0.0147, Val Loss: 0.0132\n",
      "2024-09-24 15:40:27,357 - INFO - Epoch [37/200], Train Loss: 0.0148, Val Loss: 0.0147\n",
      "2024-09-24 15:41:06,051 - INFO - Epoch [38/200], Train Loss: 0.0148, Val Loss: 0.0151\n",
      "2024-09-24 15:41:45,198 - INFO - Epoch [39/200], Train Loss: 0.0148, Val Loss: 0.0124\n",
      "2024-09-24 15:42:24,397 - INFO - Epoch [40/200], Train Loss: 0.0148, Val Loss: 0.0113\n",
      "2024-09-24 15:43:03,729 - INFO - Epoch [41/200], Train Loss: 0.0148, Val Loss: 0.0140\n",
      "2024-09-24 15:43:59,407 - INFO - Epoch [42/200], Train Loss: 0.0147, Val Loss: 0.0146\n",
      "2024-09-24 15:44:48,295 - INFO - Epoch [43/200], Train Loss: 0.0147, Val Loss: 0.0146\n",
      "2024-09-24 15:45:27,421 - INFO - Epoch [44/200], Train Loss: 0.0147, Val Loss: 0.0159\n",
      "2024-09-24 15:46:06,417 - INFO - Epoch [45/200], Train Loss: 0.0147, Val Loss: 0.0114\n",
      "2024-09-24 15:46:46,035 - INFO - Epoch [46/200], Train Loss: 0.0147, Val Loss: 0.0139\n",
      "2024-09-24 15:47:25,347 - INFO - Epoch [47/200], Train Loss: 0.0147, Val Loss: 0.0107\n",
      "2024-09-24 15:48:04,726 - INFO - Epoch [48/200], Train Loss: 0.0147, Val Loss: 0.0123\n",
      "2024-09-24 15:48:44,199 - INFO - Epoch [49/200], Train Loss: 0.0147, Val Loss: 0.0145\n",
      "2024-09-24 15:49:23,262 - INFO - Epoch [50/200], Train Loss: 0.0147, Val Loss: 0.0120\n",
      "2024-09-24 15:50:02,904 - INFO - Epoch [51/200], Train Loss: 0.0146, Val Loss: 0.0121\n",
      "2024-09-24 15:50:42,258 - INFO - Epoch [52/200], Train Loss: 0.0146, Val Loss: 0.0111\n",
      "2024-09-24 15:51:21,819 - INFO - Epoch [53/200], Train Loss: 0.0146, Val Loss: 0.0114\n",
      "2024-09-24 15:52:01,375 - INFO - Epoch [54/200], Train Loss: 0.0146, Val Loss: 0.0111\n",
      "2024-09-24 15:52:40,197 - INFO - Epoch [55/200], Train Loss: 0.0146, Val Loss: 0.0139\n",
      "2024-09-24 15:53:18,982 - INFO - Epoch [56/200], Train Loss: 0.0146, Val Loss: 0.0120\n",
      "2024-09-24 15:53:57,801 - INFO - Epoch [57/200], Train Loss: 0.0146, Val Loss: 0.0123\n",
      "2024-09-24 15:54:36,953 - INFO - Epoch [58/200], Train Loss: 0.0146, Val Loss: 0.0117\n",
      "2024-09-24 15:55:16,051 - INFO - Epoch [59/200], Train Loss: 0.0145, Val Loss: 0.0111\n",
      "2024-09-24 15:55:55,201 - INFO - Epoch [60/200], Train Loss: 0.0146, Val Loss: 0.0097\n",
      "2024-09-24 15:56:33,918 - INFO - Epoch [61/200], Train Loss: 0.0146, Val Loss: 0.0116\n",
      "2024-09-24 15:57:19,131 - INFO - Epoch [62/200], Train Loss: 0.0145, Val Loss: 0.0092\n",
      "2024-09-24 15:57:59,477 - INFO - Epoch [63/200], Train Loss: 0.0145, Val Loss: 0.0112\n",
      "2024-09-24 15:58:39,460 - INFO - Epoch [64/200], Train Loss: 0.0145, Val Loss: 0.0099\n",
      "2024-09-24 15:59:18,169 - INFO - Epoch [65/200], Train Loss: 0.0145, Val Loss: 0.0123\n",
      "2024-09-24 15:59:56,861 - INFO - Epoch [66/200], Train Loss: 0.0145, Val Loss: 0.0117\n",
      "2024-09-24 16:00:35,788 - INFO - Epoch [67/200], Train Loss: 0.0145, Val Loss: 0.0092\n",
      "2024-09-24 16:01:14,637 - INFO - Epoch [68/200], Train Loss: 0.0145, Val Loss: 0.0112\n",
      "2024-09-24 16:01:53,245 - INFO - Epoch [69/200], Train Loss: 0.0145, Val Loss: 0.0113\n",
      "2024-09-24 16:02:31,738 - INFO - Epoch [70/200], Train Loss: 0.0145, Val Loss: 0.0097\n",
      "2024-09-24 16:03:10,320 - INFO - Epoch [71/200], Train Loss: 0.0145, Val Loss: 0.0111\n",
      "2024-09-24 16:03:49,011 - INFO - Epoch [72/200], Train Loss: 0.0144, Val Loss: 0.0095\n",
      "2024-09-24 16:04:27,574 - INFO - Epoch [73/200], Train Loss: 0.0144, Val Loss: 0.0091\n",
      "2024-09-24 16:05:06,064 - INFO - Epoch [74/200], Train Loss: 0.0144, Val Loss: 0.0090\n",
      "2024-09-24 16:05:45,522 - INFO - Epoch [75/200], Train Loss: 0.0144, Val Loss: 0.0098\n",
      "2024-09-24 16:06:24,203 - INFO - Epoch [76/200], Train Loss: 0.0144, Val Loss: 0.0083\n",
      "2024-09-24 16:07:03,118 - INFO - Epoch [77/200], Train Loss: 0.0144, Val Loss: 0.0105\n",
      "2024-09-24 16:07:42,187 - INFO - Epoch [78/200], Train Loss: 0.0144, Val Loss: 0.0090\n",
      "2024-09-24 16:08:21,449 - INFO - Epoch [79/200], Train Loss: 0.0144, Val Loss: 0.0085\n",
      "2024-09-24 16:09:00,164 - INFO - Epoch [80/200], Train Loss: 0.0144, Val Loss: 0.0109\n",
      "2024-09-24 16:09:39,038 - INFO - Epoch [81/200], Train Loss: 0.0144, Val Loss: 0.0109\n",
      "2024-09-24 16:10:17,994 - INFO - Epoch [82/200], Train Loss: 0.0144, Val Loss: 0.0106\n",
      "2024-09-24 16:10:56,884 - INFO - Epoch [83/200], Train Loss: 0.0144, Val Loss: 0.0111\n",
      "2024-09-24 16:11:35,552 - INFO - Epoch [84/200], Train Loss: 0.0144, Val Loss: 0.0078\n",
      "2024-09-24 16:12:14,220 - INFO - Epoch [85/200], Train Loss: 0.0144, Val Loss: 0.0103\n",
      "2024-09-24 16:12:53,071 - INFO - Epoch [86/200], Train Loss: 0.0144, Val Loss: 0.0093\n",
      "2024-09-24 16:13:31,807 - INFO - Epoch [87/200], Train Loss: 0.0143, Val Loss: 0.0099\n",
      "2024-09-24 16:14:11,026 - INFO - Epoch [88/200], Train Loss: 0.0144, Val Loss: 0.0086\n",
      "2024-09-24 16:14:53,789 - INFO - Epoch [89/200], Train Loss: 0.0144, Val Loss: 0.0097\n",
      "2024-09-24 16:15:36,391 - INFO - Epoch [90/200], Train Loss: 0.0144, Val Loss: 0.0092\n",
      "2024-09-24 16:16:15,645 - INFO - Epoch [91/200], Train Loss: 0.0144, Val Loss: 0.0081\n",
      "2024-09-24 16:16:55,124 - INFO - Epoch [92/200], Train Loss: 0.0143, Val Loss: 0.0094\n",
      "2024-09-24 16:17:33,823 - INFO - Epoch [93/200], Train Loss: 0.0143, Val Loss: 0.0097\n",
      "2024-09-24 16:18:12,651 - INFO - Epoch [94/200], Train Loss: 0.0143, Val Loss: 0.0095\n",
      "2024-09-24 16:18:51,075 - INFO - Epoch [95/200], Train Loss: 0.0143, Val Loss: 0.0107\n",
      "2024-09-24 16:19:29,580 - INFO - Epoch [96/200], Train Loss: 0.0143, Val Loss: 0.0106\n",
      "2024-09-24 16:20:08,425 - INFO - Epoch [97/200], Train Loss: 0.0143, Val Loss: 0.0090\n",
      "2024-09-24 16:20:47,526 - INFO - Epoch [98/200], Train Loss: 0.0143, Val Loss: 0.0073\n",
      "2024-09-24 16:21:26,385 - INFO - Epoch [99/200], Train Loss: 0.0143, Val Loss: 0.0098\n",
      "2024-09-24 16:22:05,570 - INFO - Epoch [100/200], Train Loss: 0.0143, Val Loss: 0.0070\n",
      "2024-09-24 16:22:44,968 - INFO - Epoch [101/200], Train Loss: 0.0143, Val Loss: 0.0088\n",
      "2024-09-24 16:23:24,125 - INFO - Epoch [102/200], Train Loss: 0.0143, Val Loss: 0.0085\n",
      "2024-09-24 16:24:16,336 - INFO - Epoch [103/200], Train Loss: 0.0143, Val Loss: 0.0087\n",
      "2024-09-24 16:24:55,686 - INFO - Epoch [104/200], Train Loss: 0.0143, Val Loss: 0.0083\n",
      "2024-09-24 16:25:35,295 - INFO - Epoch [105/200], Train Loss: 0.0143, Val Loss: 0.0087\n",
      "2024-09-24 16:26:15,171 - INFO - Epoch [106/200], Train Loss: 0.0143, Val Loss: 0.0091\n",
      "2024-09-24 16:26:54,063 - INFO - Epoch [107/200], Train Loss: 0.0143, Val Loss: 0.0083\n",
      "2024-09-24 16:27:33,097 - INFO - Epoch [108/200], Train Loss: 0.0143, Val Loss: 0.0099\n",
      "2024-09-24 16:28:12,814 - INFO - Epoch [109/200], Train Loss: 0.0142, Val Loss: 0.0075\n",
      "2024-09-24 16:28:51,859 - INFO - Epoch [110/200], Train Loss: 0.0143, Val Loss: 0.0075\n",
      "2024-09-24 16:29:30,749 - INFO - Epoch [111/200], Train Loss: 0.0143, Val Loss: 0.0111\n",
      "2024-09-24 16:30:09,676 - INFO - Epoch [112/200], Train Loss: 0.0142, Val Loss: 0.0069\n",
      "2024-09-24 16:30:48,553 - INFO - Epoch [113/200], Train Loss: 0.0142, Val Loss: 0.0072\n",
      "2024-09-24 16:31:28,201 - INFO - Epoch [114/200], Train Loss: 0.0142, Val Loss: 0.0073\n",
      "2024-09-24 16:32:07,181 - INFO - Epoch [115/200], Train Loss: 0.0143, Val Loss: 0.0065\n",
      "2024-09-24 16:32:46,050 - INFO - Epoch [116/200], Train Loss: 0.0142, Val Loss: 0.0085\n",
      "2024-09-24 16:33:25,134 - INFO - Epoch [117/200], Train Loss: 0.0142, Val Loss: 0.0079\n",
      "2024-09-24 16:34:04,477 - INFO - Epoch [118/200], Train Loss: 0.0142, Val Loss: 0.0097\n",
      "2024-09-24 16:34:43,488 - INFO - Epoch [119/200], Train Loss: 0.0142, Val Loss: 0.0072\n",
      "2024-09-24 16:35:22,305 - INFO - Epoch [120/200], Train Loss: 0.0142, Val Loss: 0.0084\n",
      "2024-09-24 16:36:01,023 - INFO - Epoch [121/200], Train Loss: 0.0142, Val Loss: 0.0050\n",
      "2024-09-24 16:36:39,790 - INFO - Epoch [122/200], Train Loss: 0.0142, Val Loss: 0.0056\n",
      "2024-09-24 16:37:18,397 - INFO - Epoch [123/200], Train Loss: 0.0142, Val Loss: 0.0061\n",
      "2024-09-24 16:37:57,181 - INFO - Epoch [124/200], Train Loss: 0.0141, Val Loss: 0.0106\n",
      "2024-09-24 16:38:36,250 - INFO - Epoch [125/200], Train Loss: 0.0141, Val Loss: 0.0059\n",
      "2024-09-24 16:39:16,308 - INFO - Epoch [126/200], Train Loss: 0.0142, Val Loss: 0.0091\n",
      "2024-09-24 16:39:55,831 - INFO - Epoch [127/200], Train Loss: 0.0142, Val Loss: 0.0061\n",
      "2024-09-24 16:40:35,042 - INFO - Epoch [128/200], Train Loss: 0.0142, Val Loss: 0.0078\n",
      "2024-09-24 16:41:18,298 - INFO - Epoch [129/200], Train Loss: 0.0142, Val Loss: 0.0078\n",
      "2024-09-24 16:41:59,256 - INFO - Epoch [130/200], Train Loss: 0.0142, Val Loss: 0.0070\n",
      "2024-09-24 16:42:37,651 - INFO - Epoch [131/200], Train Loss: 0.0142, Val Loss: 0.0082\n",
      "2024-09-24 16:43:16,890 - INFO - Epoch [132/200], Train Loss: 0.0141, Val Loss: 0.0123\n",
      "2024-09-24 16:43:56,185 - INFO - Epoch [133/200], Train Loss: 0.0141, Val Loss: 0.0068\n",
      "2024-09-24 16:44:35,791 - INFO - Epoch [134/200], Train Loss: 0.0141, Val Loss: 0.0083\n",
      "2024-09-24 16:45:15,870 - INFO - Epoch [135/200], Train Loss: 0.0141, Val Loss: 0.0075\n",
      "2024-09-24 16:45:55,469 - INFO - Epoch [136/200], Train Loss: 0.0141, Val Loss: 0.0071\n",
      "2024-09-24 16:46:34,814 - INFO - Epoch [137/200], Train Loss: 0.0141, Val Loss: 0.0070\n",
      "2024-09-24 16:47:14,018 - INFO - Epoch [138/200], Train Loss: 0.0141, Val Loss: 0.0086\n",
      "2024-09-24 16:47:53,147 - INFO - Epoch [139/200], Train Loss: 0.0141, Val Loss: 0.0065\n",
      "2024-09-24 16:48:32,132 - INFO - Epoch [140/200], Train Loss: 0.0141, Val Loss: 0.0106\n",
      "2024-09-24 16:49:26,163 - INFO - Epoch [141/200], Train Loss: 0.0141, Val Loss: 0.0078\n",
      "2024-09-24 16:49:26,163 - INFO - Early stopping triggered at epoch 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本外测试 R²: 0.5712\n",
      "样本外测试调整后 R²: 0.5711\n",
      "\n",
      "样本外测试回归结果:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.571\n",
      "Model:                            OLS   Adj. R-squared:                  0.571\n",
      "Method:                 Least Squares   F-statistic:                     8007.\n",
      "Date:                Tue, 24 Sep 2024   Prob (F-statistic):               0.00\n",
      "Time:                        16:49:27   Log-Likelihood:             1.8613e+05\n",
      "No. Observations:              180354   AIC:                        -3.722e+05\n",
      "Df Residuals:                  180323   BIC:                        -3.719e+05\n",
      "Df Model:                          30                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.5013      0.000   2426.372      0.000       0.501       0.502\n",
      "x1           -1.4e+04   1294.216    -10.820      0.000   -1.65e+04   -1.15e+04\n",
      "x2         -5336.7790   1315.301     -4.057      0.000   -7914.738   -2758.820\n",
      "x3         -5105.5047   1320.044     -3.868      0.000   -7692.760   -2518.249\n",
      "x4          9058.7299   1184.068      7.651      0.000    6737.984    1.14e+04\n",
      "x5         -9176.1301   1113.243     -8.243      0.000   -1.14e+04   -6994.200\n",
      "x6         -2954.0389   1299.228     -2.274      0.023   -5500.497    -407.581\n",
      "x7          3636.1307   1356.589      2.680      0.007     977.248    6295.013\n",
      "x8         -2521.8999   1336.537     -1.887      0.059   -5141.483      97.683\n",
      "x9         -6306.0912   1170.279     -5.389      0.000   -8599.812   -4012.371\n",
      "x10         2326.2551   1115.911      2.085      0.037     139.096    4513.414\n",
      "x11         2163.3399   1212.227      1.785      0.074    -212.597    4539.277\n",
      "x12         5691.6980   1230.358      4.626      0.000    3280.225    8103.171\n",
      "x13          221.1029   1309.019      0.169      0.866   -2344.545    2786.751\n",
      "x14          1.07e+04   1128.458      9.479      0.000    8484.470    1.29e+04\n",
      "x15        -5550.7624   1342.537     -4.135      0.000   -8182.103   -2919.421\n",
      "x16         3521.1529   1300.227      2.708      0.007     972.738    6069.568\n",
      "x17          1.33e+04   1323.707     10.047      0.000    1.07e+04    1.59e+04\n",
      "x18        -1.137e+04   1295.532     -8.773      0.000   -1.39e+04   -8825.871\n",
      "x19         4694.6698   1231.882      3.811      0.000    2280.209    7109.131\n",
      "x20         2946.5836   1335.633      2.206      0.027     328.773    5564.395\n",
      "x21         8218.9839   1158.073      7.097      0.000    5949.187    1.05e+04\n",
      "x22        -5771.3126   1322.222     -4.365      0.000   -8362.838   -3179.787\n",
      "x23         1.239e+04   1326.775      9.337      0.000    9787.581     1.5e+04\n",
      "x24          767.2543   1325.626      0.579      0.563   -1830.943    3365.452\n",
      "x25         8128.5009   1254.619      6.479      0.000    5669.476    1.06e+04\n",
      "x26         4940.6300   1261.930      3.915      0.000    2467.276    7413.984\n",
      "x27         2212.3670   1232.095      1.796      0.073    -202.510    4627.244\n",
      "x28         4521.8607   1308.366      3.456      0.001    1957.492    7086.229\n",
      "x29         3325.1674   1325.040      2.509      0.012     728.119    5922.216\n",
      "x30        -8849.6816   1259.335     -7.027      0.000   -1.13e+04   -6381.414\n",
      "==============================================================================\n",
      "Omnibus:                    70863.612   Durbin-Watson:                   0.027\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1139529.729\n",
      "Skew:                           1.465   Prob(JB):                         0.00\n",
      "Kurtosis:                      14.961   Cond. No.                     3.74e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.74e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "eval_env: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoquanliu/anaconda3/lib/python3.10/site-packages/statsmodels/graphics/regressionplots.py:429: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fig = abline_plot(0, fitted_line.params[0], color='k', ax=ax)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本外测试结果图表已保存\n",
      "交叉验证结果已保存到 'cross_validation_results.csv'\n",
      "样本外测试结果已保存到 'out_of_sample_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 1. 数据准备\n",
    "def load_and_preprocess_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logging.info(f\"Successfully loaded data from {file_path}\")\n",
    "        \n",
    "        Z = data.iloc[:, 4:36].values  \n",
    "        # 特征扩展：两两交互\n",
    "        expanded_features = []\n",
    "        for i, j in combinations(range(Z.shape[1]), 2):\n",
    "            expanded_features.append(Z[:, i] * Z[:, j])\n",
    "        \n",
    "        Z_expanded = np.column_stack([Z] + expanded_features)\n",
    "        logging.info(f\"Expanded features from {Z.shape[1]} to {Z_expanded.shape[1]}\")\n",
    "\n",
    "        r = data.iloc[:, 3].values  # 收益率，对应r_t\n",
    "\n",
    "        # 使用均值填充NaN\n",
    "        Z_expanded = np.nan_to_num(Z_expanded, nan=np.nanmean(Z_expanded))\n",
    "        r = np.nan_to_num(r, nan=np.nanmean(r))\n",
    "        logging.info(\"Filled NaN values with mean\")\n",
    "\n",
    "        # 使用StandardScaler来标准化特征\n",
    "        scaler = StandardScaler()\n",
    "        Z_scaled = scaler.fit_transform(Z_expanded)\n",
    "\n",
    "        Z = Z_scaled.astype(np.float32)\n",
    "        r = r.astype(np.float32)\n",
    "\n",
    "        return Z, r\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading or preprocessing data: {e}\")\n",
    "        raise\n",
    "\n",
    "# 2. 模型设计\n",
    "class ConditionalAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout_rate=0.2):\n",
    "        super(ConditionalAutoencoder, self).__init__()\n",
    "        \n",
    "        # 深度学习自解码因子荷载神经网络 B(Z_{t-1})\n",
    "        self.beta_net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(hidden_dim, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(hidden_dim // 2, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(latent_dim, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        ])\n",
    "        \n",
    "        # 深度学习自解码的因子提取网络 f_t\n",
    "        self.factor_net = tf.keras.layers.Dense(latent_dim, use_bias=False, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        Z = inputs\n",
    "        beta = self.beta_net(Z, training=training)  # B(Z_{t-1})\n",
    "        f = self.factor_net(tf.ones((tf.shape(Z)[0], 1)))  # f_t\n",
    "        return tf.reduce_sum(beta * f, axis=1)  # r_t = B(Z_{t-1})f_t\n",
    "\n",
    "# 3. 实验设计\n",
    "def prepare_data(Z, r, test_size=0.2, val_size=0.2):\n",
    "    Z_train_val, Z_test, r_train_val, r_test = train_test_split(Z, r, test_size=test_size, random_state=42)\n",
    "    Z_train, Z_val, r_train, r_val = train_test_split(Z_train_val, r_train_val, test_size=val_size, random_state=42)\n",
    "    return (Z_train, r_train), (Z_val, r_val), (Z_test, r_test)\n",
    "\n",
    "def train_model(model, train_data, val_data, num_epochs=200, batch_size=128, patience=20):\n",
    "    Z_train, r_train = train_data\n",
    "    Z_val, r_val = val_data\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = tf.keras.metrics.Mean()\n",
    "        for i in range(0, len(Z_train), batch_size):\n",
    "            batch_Z = Z_train[i:i+batch_size]\n",
    "            batch_r = r_train[i:i+batch_size]\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_Z, training=True)\n",
    "                loss = tf.reduce_mean(tf.square(batch_r - predictions))\n",
    "                # 添加L2正则化损失\n",
    "                l2_loss = sum(tf.nn.l2_loss(v) for v in model.trainable_variables if 'kernel' in v.name)\n",
    "                total_loss = loss + 0.01 * l2_loss\n",
    "            gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            train_loss.update_state(loss)\n",
    "\n",
    "        # Validation\n",
    "        val_predictions = model(Z_val, training=False)\n",
    "        val_loss = tf.reduce_mean(tf.square(r_val - val_predictions))\n",
    "\n",
    "        train_losses.append(train_loss.result().numpy())\n",
    "        val_losses.append(val_loss.numpy())\n",
    "\n",
    "        logging.info(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.result():.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # 早停\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                logging.info(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # 学习率调整\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            optimizer.learning_rate = optimizer.learning_rate * 0.9\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 提取共同因子的函数\n",
    "def extract_common_factors(model, Z):\n",
    "    beta = model.beta_net(Z).numpy()\n",
    "    factor_weights = model.factor_net.weights[0].numpy()\n",
    "    common_factors = np.einsum('ij,kj->ik', beta, factor_weights.T)\n",
    "    return common_factors\n",
    "\n",
    "# 时间序列交叉验证\n",
    "def time_series_cv(Z, r, n_splits=3):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_scores = []\n",
    "    cv_results = []\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(Z)):\n",
    "        Z_train, Z_test = Z[train_index], Z[test_index]\n",
    "        r_train, r_test = r[train_index], r[test_index]\n",
    "        \n",
    "        model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=30)\n",
    "        train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_test, r_test))\n",
    "        \n",
    "        # 评估模型\n",
    "        common_factors = extract_common_factors(model, Z_test)\n",
    "        scaler = StandardScaler()\n",
    "        common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "        X = sm.add_constant(common_factors_scaled)\n",
    "        y = r_test\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        results = ols_model.fit()\n",
    "        \n",
    "        cv_scores.append(results.rsquared_adj)\n",
    "        cv_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'r_squared': results.rsquared,\n",
    "            'adj_r_squared': results.rsquared_adj,\n",
    "            'aic': results.aic,\n",
    "            'bic': results.bic\n",
    "        })\n",
    "    \n",
    "    return cv_scores, cv_results\n",
    "\n",
    "# 样本外测试\n",
    "def out_of_sample_test(Z, r, train_ratio=0.6, val_ratio=0.2):\n",
    "    n = len(Z)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    Z_train, r_train = Z[:train_end], r[:train_end]\n",
    "    Z_val, r_val = Z[train_end:val_end], r[train_end:val_end]\n",
    "    Z_test, r_test = Z[val_end:], r[val_end:]\n",
    "    \n",
    "    model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=30)\n",
    "    train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_val, r_val))\n",
    "    \n",
    "    # 在样本外数据上评估模型\n",
    "    common_factors = extract_common_factors(model, Z_test)\n",
    "    scaler = StandardScaler()\n",
    "    common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "    X = sm.add_constant(common_factors_scaled)\n",
    "    y = r_test\n",
    "    ols_model = sm.OLS(y, X)\n",
    "    results = ols_model.fit()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 可视化函数\n",
    "def plot_cv_results(cv_results):\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(data=df[['r_squared', 'adj_r_squared']])\n",
    "    plt.title('Cross-Validation Results: R-squared and Adjusted R-squared')\n",
    "    plt.savefig('cv_results_boxplot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data=df, x='fold', y='adj_r_squared')\n",
    "    plt.title('Adjusted R-squared across CV Folds')\n",
    "    plt.savefig('cv_results_scatter.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_oos_results(oos_results):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(oos_results.fittedvalues, oos_results.resid)\n",
    "    plt.xlabel('Fitted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Out-of-Sample Test: Residuals vs Fitted')\n",
    "    plt.savefig('oos_residuals_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sm.graphics.plot_regress_exog(oos_results, 'x1', fig=plt.gcf())\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('oos_regress_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    try:\n",
    "        # 文件路径与数据导入\n",
    "        file_path = '/Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/DL_Data7.csv'\n",
    "        Z, r = load_and_preprocess_data(file_path)\n",
    "\n",
    "        # 原始模型训练和评估\n",
    "        train_data, val_data, test_data = prepare_data(Z, r)\n",
    "        Z_train, r_train = train_data\n",
    "        Z_val, r_val = val_data\n",
    "        Z_test, r_test = test_data\n",
    "\n",
    "        model = ConditionalAutoencoder(input_dim=Z.shape[1], hidden_dim=128, latent_dim=30)\n",
    "        train_losses, val_losses = train_model(model, (Z_train, r_train), (Z_val, r_val))\n",
    "\n",
    "        # 提取测试集的共同因子\n",
    "        common_factors = extract_common_factors(model, Z_test)\n",
    "        scaler = StandardScaler()\n",
    "        common_factors_scaled = scaler.fit_transform(common_factors)\n",
    "\n",
    "        # 准备回归数据\n",
    "        X = sm.add_constant(common_factors_scaled)\n",
    "        y = r_test\n",
    "\n",
    "        # 进行OLS回归\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        results = ols_model.fit()\n",
    "\n",
    "        # 输出回归结果\n",
    "        print(results.summary())\n",
    "\n",
    "        # 保存共同因子数据\n",
    "        factor_df = pd.DataFrame(common_factors_scaled, columns=[f'Factor_{i+1}' for i in range(common_factors_scaled.shape[1])])\n",
    "        factor_df['Return'] = r_test\n",
    "        factor_df.to_csv('common_factors_and_returns.csv', index=False)\n",
    "\n",
    "        print(\"共同因子数据已保存到 'common_factors_and_returns.csv'\")\n",
    "\n",
    "        # 计算调整后的R方\n",
    "        adjusted_r_squared = results.rsquared_adj\n",
    "        print(f\"调整后的R方: {adjusted_r_squared:.4f}\")\n",
    "\n",
    "        # 绘制训练和验证损失\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('loss_plot.png')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"损失图表已保存为 'loss_plot.png'\")\n",
    "\n",
    "        # 添加时间序列交叉验证\n",
    "        cv_scores, cv_results = time_series_cv(Z, r)\n",
    "        print(f\"交叉验证 R² 分数: {np.mean(cv_scores):.4f} (±{np.std(cv_scores):.4f})\")\n",
    "        \n",
    "        # 可视化交叉验证结果\n",
    "        plot_cv_results(cv_results)\n",
    "        print(\"交叉验证结果图表已保存\")\n",
    "\n",
    "        # 样本外测试\n",
    "        oos_results = out_of_sample_test(Z, r)\n",
    "        print(f\"样本外测试 R²: {oos_results.rsquared:.4f}\")\n",
    "        print(f\"样本外测试调整后 R²: {oos_results.rsquared_adj:.4f}\")\n",
    "        \n",
    "        # 输出样本外测试的完整回归结果\n",
    "        print(\"\\n样本外测试回归结果:\")\n",
    "        print(oos_results.summary())\n",
    "\n",
    "        # 可视化样本外测试结果\n",
    "        plot_oos_results(oos_results)\n",
    "        print(\"样本外测试结果图表已保存\")\n",
    "\n",
    "        # 保存交叉验证和样本外测试结果\n",
    "        cv_df = pd.DataFrame(cv_results)\n",
    "        cv_df.to_csv('cross_validation_results.csv', index=False)\n",
    "        print(\"交叉验证结果已保存到 'cross_validation_results.csv'\")\n",
    "\n",
    "        oos_df = pd.DataFrame({\n",
    "            'Actual': oos_results.model.endog,\n",
    "            'Predicted': oos_results.fittedvalues,\n",
    "            'Residuals': oos_results.resid\n",
    "        })\n",
    "        oos_df.to_csv('out_of_sample_results.csv', index=False)\n",
    "        print(\"样本外测试结果已保存到 'out_of_sample_results.csv'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b02fc-66f6-486a-93d8-5923bccd5b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
